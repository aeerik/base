{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (0.16.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.2 in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from torchtext) (2.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from torchtext) (1.25.2)\n",
      "Requirement already satisfied: torchdata==0.7.1 in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from torchtext) (0.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from torch==2.1.2->torchtext) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\erika\\appdata\\roaming\\python\\python39\\site-packages (from torch==2.1.2->torchtext) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from torch==2.1.2->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from torch==2.1.2->torchtext) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from torch==2.1.2->torchtext) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from torch==2.1.2->torchtext) (2023.6.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from requests->torchtext) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\erika\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from jinja2->torch==2.1.2->torchtext) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages (from sympy->torch==2.1.2->torchtext) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import typing\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 491161/491161 [00:10<00:00, 47381.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [02:20<00:00, 356.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          masked_sentence  \\\n",
      "0       [[CLS], one, of, the, other, reviewers, has, m...   \n",
      "1       [[CLS], a, very, comical, but, down, [MASK], e...   \n",
      "2       [[CLS], [MASK], are, glisten, [MASK], rudi, th...   \n",
      "3       [[CLS], it, doesn, ', t, [MASK], that, bava, h...   \n",
      "4       [[CLS], trust, me, ,, this, is, not, a, show, ...   \n",
      "...                                                   ...   \n",
      "882317  [[CLS], [MASK], me, sick, ., [PAD], [PAD], [PA...   \n",
      "882318  [[CLS], no, one, expects, the, star, trek, mov...   \n",
      "882319  [[CLS], the, scenes, lifeforms, the, set, feel...   \n",
      "882320  [[CLS], unfortunately, ,, this, movie, had, a,...   \n",
      "882321  [[CLS], i, like, all, the, character, ishibash...   \n",
      "\n",
      "                                           masked_indices  \\\n",
      "0       [0, 5, 6, 7, 8, 9, 10, 11, 12, 2, 14, 2, 16, 2...   \n",
      "1       [0, 56, 204, 4088, 153, 221, 2, 2656, 448, 179...   \n",
      "2       [0, 2, 25, 31547, 2, 61340, 29, 30, 31, 32, 2,...   \n",
      "3       [0, 73, 144, 20, 134, 2, 12, 20636, 537, 745, ...   \n",
      "4       [0, 54, 35, 27, 29, 30, 55, 56, 57, 58, 7, 2, ...   \n",
      "...                                                   ...   \n",
      "882317  [0, 2, 35, 3388, 36, 1, 1, 1, 1, 1, 1, 1, 1, 1...   \n",
      "882318  [0, 64, 5, 8571, 7, 756, 5005, 574, 67, 22, 10...   \n",
      "882319  [0, 7, 46, 28512, 7, 49, 987, 384, 68695, 28, ...   \n",
      "882320  [0, 1381, 27, 29, 364, 537, 56, 10578, 27, 2, ...   \n",
      "882321  [0, 124, 383, 92, 7, 988, 49719, 2, 2619, 55, ...   \n",
      "\n",
      "                                                 sentence  \\\n",
      "0       [[CLS], one, of, the, other, reviewers, has, m...   \n",
      "1       [[CLS], a, very, comical, but, down, to, earth...   \n",
      "2       [[CLS], they, are, right, ,, as, this, is, exa...   \n",
      "3       [[CLS], it, doesn, ', t, appear, that, bava, h...   \n",
      "4       [[CLS], trust, me, ,, this, is, not, a, show, ...   \n",
      "...                                                   ...   \n",
      "882317  [[CLS], makes, me, sick, ., [PAD], [PAD], [PAD...   \n",
      "882318  [[CLS], no, one, expects, the, star, trek, mov...   \n",
      "882319  [[CLS], the, scenes, on, the, set, feel, real,...   \n",
      "882320  [[CLS], unfortunately, ,, this, movie, had, a,...   \n",
      "882321  [[CLS], i, like, all, the, character, in, my, ...   \n",
      "\n",
      "                                                  indices  \\\n",
      "0       [0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...   \n",
      "1       [0, 56, 204, 4088, 153, 221, 67, 2656, 448, 17...   \n",
      "2       [0, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34...   \n",
      "3       [0, 73, 144, 20, 134, 1242, 12, 20636, 537, 74...   \n",
      "4       [0, 54, 35, 27, 29, 30, 55, 56, 57, 58, 7, 59,...   \n",
      "...                                                   ...   \n",
      "882317  [0, 528, 35, 3388, 36, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
      "882318  [0, 64, 5, 8571, 7, 756, 5005, 574, 67, 22, 10...   \n",
      "882319  [0, 7, 46, 84, 7, 49, 987, 384, 27, 28, 5234, ...   \n",
      "882320  [0, 1381, 27, 29, 364, 537, 56, 10578, 27, 262...   \n",
      "882321  [0, 124, 383, 92, 7, 988, 50, 492, 2619, 55, 5...   \n",
      "\n",
      "                                               token_mask  is_next  \n",
      "0       [True, True, True, True, True, True, True, Tru...        1  \n",
      "1       [True, True, True, True, True, False, True, Tr...        0  \n",
      "2       [False, True, False, False, False, True, True,...        1  \n",
      "3       [True, True, True, True, False, True, True, Tr...        0  \n",
      "4       [True, True, True, True, True, True, True, Tru...        1  \n",
      "...                                                   ...      ...  \n",
      "882317  [False, True, True, True, True, True, True, Tr...        0  \n",
      "882318  [True, True, True, True, True, True, True, Tru...        1  \n",
      "882319  [True, True, False, True, True, True, True, Fa...        0  \n",
      "882320  [True, True, True, True, True, True, True, Tru...        1  \n",
      "882321  [True, True, True, True, True, False, False, T...        0  \n",
      "\n",
      "[882322 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class IMDBBertDataset(Dataset):\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    SEP = '[SEP]'\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    MASK_PERCENTAGE = 0.15\n",
    "\n",
    "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
    "    TARGET_COLUMN = 'indices'\n",
    "    NSP_TARGET_COLUMN = 'is_next'\n",
    "    TOKEN_MASK_COLUMN = 'token_mask'\n",
    "\n",
    "    OPTIMAL_LENGTH_PERCENTILE = 70\n",
    "\n",
    "    def __init__(self, path, ds_from=None, ds_to=None, should_include_text=False):\n",
    "        self.ds: pd.Series = pd.read_csv(path)['review']\n",
    "\n",
    "        if ds_from is not None or ds_to is not None:\n",
    "            self.ds = self.ds[ds_from:ds_to]\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.counter = Counter()\n",
    "        self.vocab = None\n",
    "\n",
    "        self.optimal_sentence_length = None\n",
    "        self.should_include_text = should_include_text\n",
    "\n",
    "        if should_include_text:\n",
    "            self.columns = ['masked_sentence', self.MASKED_INDICES_COLUMN, 'sentence', self.TARGET_COLUMN,\n",
    "                            self.TOKEN_MASK_COLUMN,\n",
    "                            self.NSP_TARGET_COLUMN]\n",
    "        else:\n",
    "            self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN, self.TOKEN_MASK_COLUMN,\n",
    "                            self.NSP_TARGET_COLUMN]\n",
    "\n",
    "        self.df = self.prepare_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "\n",
    "        inp = torch.Tensor(item[self.MASKED_INDICES_COLUMN]).long()\n",
    "        token_mask = torch.Tensor(item[self.TOKEN_MASK_COLUMN]).bool()\n",
    "\n",
    "        mask_target = torch.Tensor(item[self.TARGET_COLUMN]).long()\n",
    "        mask_target = mask_target.masked_fill_(token_mask, 0)\n",
    "\n",
    "        attention_mask = (inp == self.vocab[self.PAD]).unsqueeze(0)\n",
    "\n",
    "        if item[self.NSP_TARGET_COLUMN] == 0:\n",
    "            t = [1, 0]\n",
    "        else:\n",
    "            t = [0, 1]\n",
    "\n",
    "        nsp_target = torch.Tensor(t)\n",
    "\n",
    "        return (\n",
    "            inp.to(device),\n",
    "            attention_mask.to(device),\n",
    "            token_mask.to(device),\n",
    "            mask_target.to(device),\n",
    "            nsp_target.to(device)\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self) -> pd.DataFrame:\n",
    "        sentences = []\n",
    "        nsp = []\n",
    "        sentence_lens = []\n",
    "\n",
    "        # Split dataset on sentences\n",
    "        for review in self.ds:\n",
    "            review_sentences = review.split('. ')\n",
    "            sentences += review_sentences\n",
    "            self._update_length(review_sentences, sentence_lens)\n",
    "        self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
    "\n",
    "        print(\"Create vocabulary\")\n",
    "        for sentence in tqdm(sentences):\n",
    "            s = self.tokenizer(sentence)\n",
    "            self.counter.update(s)\n",
    "\n",
    "        self._fill_vocab()\n",
    "\n",
    "        print(\"Preprocessing dataset\")\n",
    "        for review in tqdm(self.ds):\n",
    "            review_sentences = review.split('. ')\n",
    "            if len(review_sentences) > 1:\n",
    "                for i in range(len(review_sentences) - 1):\n",
    "                    # True NSP item\n",
    "                    first, second = self.tokenizer(review_sentences[i]), self.tokenizer(review_sentences[i + 1])\n",
    "                    nsp.append(self._create_item(first, second, 1))\n",
    "\n",
    "                    # False NSP item\n",
    "                    first, second = self._select_false_nsp_sentences(sentences)\n",
    "                    first, second = self.tokenizer(first), self.tokenizer(second)\n",
    "                    nsp.append(self._create_item(first, second, 0))\n",
    "        df = pd.DataFrame(nsp, columns=self.columns)\n",
    "        return df\n",
    "\n",
    "    def _update_length(self, sentences: typing.List[str], lengths: typing.List[int]):\n",
    "        for v in sentences:\n",
    "            l = len(v.split())\n",
    "            lengths.append(l)\n",
    "        return lengths\n",
    "\n",
    "    def _find_optimal_sentence_length(self, lengths: typing.List[int]):\n",
    "        arr = np.array(lengths)\n",
    "        return int(np.percentile(arr, self.OPTIMAL_LENGTH_PERCENTILE))\n",
    "\n",
    "    def _fill_vocab(self):\n",
    " \n",
    "        self.vocab = vocab(self.counter, min_freq=2)\n",
    "\n",
    "        self.vocab.insert_token(self.CLS, 0)\n",
    "        self.vocab.insert_token(self.PAD, 1)\n",
    "        self.vocab.insert_token(self.MASK, 2)\n",
    "        self.vocab.insert_token(self.SEP, 3)\n",
    "        self.vocab.insert_token(self.UNK, 4)\n",
    "        self.vocab.set_default_index(4)\n",
    "\n",
    "    def _create_item(self, first: typing.List[str], second: typing.List[str], target: int = 1):\n",
    "\n",
    "        updated_first, first_mask = self._preprocess_sentence(first.copy())\n",
    "        updated_second, second_mask = self._preprocess_sentence(second.copy())\n",
    "\n",
    "        nsp_sentence = updated_first + [self.SEP] + updated_second\n",
    "        nsp_indices = self.vocab.lookup_indices(nsp_sentence)\n",
    "        inverse_token_mask = first_mask + [True] + second_mask\n",
    "\n",
    "        first, _ = self._preprocess_sentence(first.copy(), should_mask=False)\n",
    "        second, _ = self._preprocess_sentence(second.copy(), should_mask=False)\n",
    "        original_nsp_sentence = first + [self.SEP] + second\n",
    "        original_nsp_indices = self.vocab.lookup_indices(original_nsp_sentence)\n",
    "\n",
    "        if self.should_include_text:\n",
    "            return (\n",
    "                nsp_sentence,\n",
    "                nsp_indices,\n",
    "                original_nsp_sentence,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                nsp_indices,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "\n",
    "    def _select_false_nsp_sentences(self, sentences: typing.List[str]):\n",
    "        sentences_len = len(sentences)\n",
    "        sentence_index = random.randint(0, sentences_len - 1)\n",
    "        next_sentence_index = random.randint(0, sentences_len - 1)\n",
    "\n",
    "        # To be sure that it's not real next sentence\n",
    "        while next_sentence_index == sentence_index + 1:\n",
    "            next_sentence_index = random.randint(0, sentences_len - 1)\n",
    "\n",
    "        return sentences[sentence_index], sentences[next_sentence_index]\n",
    "\n",
    "    def _preprocess_sentence(self, sentence: typing.List[str], should_mask: bool = True):\n",
    "        inverse_token_mask = None\n",
    "        if should_mask:\n",
    "            sentence, inverse_token_mask = self._mask_sentence(sentence)\n",
    "        sentence, inverse_token_mask = self._pad_sentence([self.CLS] + sentence, inverse_token_mask)\n",
    "\n",
    "        return sentence, inverse_token_mask\n",
    "\n",
    "    def _mask_sentence(self, sentence: typing.List[str]):\n",
    "        len_s = len(sentence)\n",
    "        inverse_token_mask = [True for _ in range(max(len_s, self.optimal_sentence_length))]\n",
    "\n",
    "        mask_amount = round(len_s * self.MASK_PERCENTAGE)\n",
    "        for _ in range(mask_amount):\n",
    "            i = random.randint(0, len_s - 1)\n",
    "\n",
    "            if random.random() < 0.8:\n",
    "                sentence[i] = self.MASK\n",
    "            else:\n",
    "                j = random.randint(5, len(self.vocab) - 1)\n",
    "                sentence[i] = self.vocab.lookup_token(j)\n",
    "            inverse_token_mask[i] = False\n",
    "        return sentence, inverse_token_mask\n",
    "\n",
    "    def _pad_sentence(self, sentence: typing.List[str], inverse_token_mask: typing.List[bool] = None):\n",
    "        len_s = len(sentence)\n",
    "\n",
    "        if len_s >= self.optimal_sentence_length:\n",
    "            s = sentence[:self.optimal_sentence_length]\n",
    "        else:\n",
    "            s = sentence + [self.PAD] * (self.optimal_sentence_length - len_s)\n",
    "\n",
    "        # inverse token mask should be padded as well\n",
    "        if inverse_token_mask:\n",
    "            len_m = len(inverse_token_mask)\n",
    "            if len_m >= self.optimal_sentence_length:\n",
    "                inverse_token_mask = inverse_token_mask[:self.optimal_sentence_length]\n",
    "            else:\n",
    "                inverse_token_mask = inverse_token_mask + [True] * (self.optimal_sentence_length - len_m)\n",
    "        return s, inverse_token_mask\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BASE_DIR = Path('c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\kod\\\\').resolve().parent.parent\n",
    "\n",
    "    ds = IMDBBertDataset(BASE_DIR.joinpath('data/imdb.csv'), ds_from=0, ds_to=50000,\n",
    "                         should_include_text=True)\n",
    "    print(ds.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class JointEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, size):\n",
    "        super(JointEmbedding, self).__init__()\n",
    "\n",
    "        self.size = size\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, size)\n",
    "        self.segment_emb = nn.Embedding(vocab_size, size)\n",
    "\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        sentence_size = input_tensor.size(-1)\n",
    "        pos_tensor = self.attention_position(self.size, input_tensor)\n",
    "\n",
    "        segment_tensor = torch.zeros_like(input_tensor).to(device)\n",
    "        segment_tensor[:, sentence_size // 2 + 1:] = 1\n",
    "\n",
    "        output = self.token_emb(input_tensor) + self.segment_emb(segment_tensor) + pos_tensor\n",
    "        return self.norm(output)\n",
    "\n",
    "    def attention_position(self, dim, input_tensor):\n",
    "        batch_size = input_tensor.size(0)\n",
    "        sentence_size = input_tensor.size(-1)\n",
    "\n",
    "        pos = torch.arange(sentence_size, dtype=torch.long).to(device)\n",
    "        d = torch.arange(dim, dtype=torch.long).to(device)\n",
    "        d = (2 * d / dim)\n",
    "\n",
    "        pos = pos.unsqueeze(1)\n",
    "        pos = pos / (1e4 ** d)\n",
    "\n",
    "        pos[:, ::2] = torch.sin(pos[:, ::2])\n",
    "        pos[:, 1::2] = torch.cos(pos[:, 1::2])\n",
    "\n",
    "        return pos.expand(batch_size, *pos.size())\n",
    "\n",
    "    def numeric_position(self, dim, input_tensor):\n",
    "        pos_tensor = torch.arange(dim, dtype=torch.long).to(device)\n",
    "        return pos_tensor.expand_as(input_tensor)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out):\n",
    "        super(AttentionHead, self).__init__()\n",
    "\n",
    "        self.dim_inp = dim_inp\n",
    "\n",
    "        self.q = nn.Linear(dim_inp, dim_out)\n",
    "        self.k = nn.Linear(dim_inp, dim_out)\n",
    "        self.v = nn.Linear(dim_inp, dim_out)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)\n",
    "\n",
    "        scale = query.size(1) ** 0.5\n",
    "        scores = torch.bmm(query, key.transpose(1, 2)) / scale\n",
    "\n",
    "        scores = scores.masked_fill_(attention_mask, -1e9)\n",
    "        attn = f.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(attn, value)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, dim_inp, dim_out):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(dim_inp, dim_out) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(dim_out * num_heads, dim_inp)\n",
    "        self.norm = nn.LayerNorm(dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        s = [head(input_tensor, attention_mask) for head in self.heads]\n",
    "        scores = torch.cat(s, dim=-1)\n",
    "        scores = self.linear(scores)\n",
    "        return self.norm(scores)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out, attention_heads=4, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(attention_heads, dim_inp, dim_out)  # batch_size x sentence size x dim_inp\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim_inp, dim_out),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim_out, dim_inp),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        context = self.attention(input_tensor, attention_mask)\n",
    "        res = self.feed_forward(context)\n",
    "        return self.norm(res)\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dim_inp, dim_out, attention_heads=4):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        self.embedding = JointEmbedding(vocab_size, dim_inp)\n",
    "        self.encoder = Encoder(dim_inp, dim_out, attention_heads)\n",
    "\n",
    "        self.token_prediction_layer = nn.Linear(dim_inp, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.classification_layer = nn.Linear(dim_inp, 2)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        embedded = self.embedding(input_tensor)\n",
    "        encoded = self.encoder(embedded, attention_mask)\n",
    "\n",
    "        token_predictions = self.token_prediction_layer(encoded)\n",
    "\n",
    "        first_word = encoded[:, 0, :]\n",
    "        return self.softmax(token_predictions), self.classification_layer(first_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def percentage(batch_size: int, max_index: int, current_index: int):\n",
    "\n",
    "    batched_max = max_index // batch_size\n",
    "    return round(current_index / batched_max * 100, 2)\n",
    "\n",
    "\n",
    "def nsp_accuracy(result: torch.Tensor, target: torch.Tensor):\n",
    "\n",
    "    s = (result.argmax(1) == target.argmax(1)).sum()\n",
    "    return round(float(s / result.size(0)), 2)\n",
    "\n",
    "\n",
    "def token_accuracy(result: torch.Tensor, target: torch.Tensor, inverse_token_mask: torch.Tensor):\n",
    " \n",
    "    r = result.argmax(-1).masked_select(~inverse_token_mask)\n",
    "    t = target.masked_select(~inverse_token_mask)\n",
    "    s = (r == t).sum()\n",
    "    return round(float(s / (result.size(0) * result.size(1))), 2)\n",
    "\n",
    "\n",
    "class BertTrainer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: BERT,\n",
    "                 dataset: IMDBBertDataset,\n",
    "                 log_dir: Path,\n",
    "                 checkpoint_dir: Path = None,\n",
    "                 print_progress_every: int = 10,\n",
    "                 print_accuracy_every: int = 50,\n",
    "                 batch_size: int = 24,\n",
    "                 learning_rate: float = 0.005,\n",
    "                 epochs: int = 5,\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.writer = SummaryWriter(str(log_dir))\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        self.ml_criterion = nn.NLLLoss(ignore_index=0).to(device)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.015)\n",
    "\n",
    "        self._splitter_size = 35\n",
    "\n",
    "        self._ds_len = len(self.dataset)\n",
    "        self._batched_len = self._ds_len // self.batch_size\n",
    "\n",
    "        self._print_every = print_progress_every\n",
    "        self._accuracy_every = print_accuracy_every\n",
    "\n",
    "    def print_summary(self):\n",
    "        ds_len = len(self.dataset)\n",
    "\n",
    "        print(\"Model Summary\\n\")\n",
    "        print('=' * self._splitter_size)\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"Training dataset len: {ds_len}\")\n",
    "        print(f\"Max / Optimal sentence len: {self.dataset.optimal_sentence_length}\")\n",
    "        print(f\"Vocab size: {len(self.dataset.vocab)}\")\n",
    "        print(f\"Batch size: {self.batch_size}\")\n",
    "        print(f\"Batched dataset len: {self._batched_len}\")\n",
    "        print('=' * self._splitter_size)\n",
    "        print()\n",
    "\n",
    "    def __call__(self):\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            loss = self.train(self.current_epoch)\n",
    "            self.save_checkpoint(self.current_epoch, step=-1, loss=loss)\n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Begin epoch {epoch}\")\n",
    "\n",
    "        prev = time.time()\n",
    "        average_nsp_loss = 0\n",
    "        average_mlm_loss = 0\n",
    "        for i, value in enumerate(self.loader):\n",
    "            index = i + 1\n",
    "            inp, mask, inverse_token_mask, token_target, nsp_target = value\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            token, nsp = self.model(inp, mask)\n",
    "\n",
    "            tm = inverse_token_mask.unsqueeze(-1).expand_as(token)\n",
    "            token = token.masked_fill(tm, 0)\n",
    "\n",
    "            loss_token = self.ml_criterion(token.transpose(1, 2), token_target)  \n",
    "            loss_nsp = self.criterion(nsp, nsp_target)\n",
    "\n",
    "            loss = loss_token + loss_nsp\n",
    "            average_nsp_loss += loss_nsp\n",
    "            average_mlm_loss += loss_token\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if index % self._print_every == 0:\n",
    "                elapsed = time.gmtime(time.time() - prev)\n",
    "                s = self.training_summary(elapsed, index, average_nsp_loss, average_mlm_loss)\n",
    "\n",
    "                if index % self._accuracy_every == 0:\n",
    "                    s += self.accuracy_summary(index, token, nsp, token_target, nsp_target)\n",
    "\n",
    "                print(s)\n",
    "\n",
    "                average_nsp_loss = 0\n",
    "                average_mlm_loss = 0\n",
    "        return loss\n",
    "\n",
    "    def training_summary(self, elapsed, index, average_nsp_loss, average_mlm_loss):\n",
    "        passed = percentage(self.batch_size, self._ds_len, index)\n",
    "        global_step = self.current_epoch * len(self.loader) + index\n",
    "\n",
    "        print_nsp_loss = average_nsp_loss / self._print_every\n",
    "        print_mlm_loss = average_mlm_loss / self._print_every\n",
    "\n",
    "        s = f\"{time.strftime('%H:%M:%S', elapsed)}\"\n",
    "        s += f\" | Epoch {self.current_epoch + 1} | {index} / {self._batched_len} ({passed}%) | \" \\\n",
    "             f\"NSP loss {print_nsp_loss:6.2f} | MLM loss {print_mlm_loss:6.2f}\"\n",
    "\n",
    "        self.writer.add_scalar(\"NSP loss\", print_nsp_loss, global_step=global_step)\n",
    "        self.writer.add_scalar(\"MLM loss\", print_mlm_loss, global_step=global_step)\n",
    "        return s\n",
    "\n",
    "    def accuracy_summary(self, index, token, nsp, token_target, nsp_target, inverse_token_mask):\n",
    "        global_step = self.current_epoch * len(self.loader) + index\n",
    "        nsp_acc = nsp_accuracy(nsp, nsp_target)\n",
    "        token_acc = token_accuracy(token, token_target, inverse_token_mask)\n",
    "\n",
    "        self.writer.add_scalar(\"NSP train accuracy\", nsp_acc, global_step=global_step)\n",
    "        self.writer.add_scalar(\"Token train accuracy\", token_acc, global_step=global_step)\n",
    "\n",
    "        return f\" | NSP accuracy {nsp_acc} | Token accuracy {token_acc}\"\n",
    "\n",
    "    def save_checkpoint(self, epoch, step, loss):\n",
    "        if not self.checkpoint_dir:\n",
    "            return\n",
    "\n",
    "        prev = time.time()\n",
    "        name = f\"bert_epoch{epoch}_step{step}_{datetime.utcnow().timestamp():.0f}.pt\"\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, self.checkpoint_dir.joinpath(name))\n",
    "\n",
    "        print()\n",
    "        print('=' * self._splitter_size)\n",
    "        print(f\"Model saved as '{name}' for {time.time() - prev:.2f}s\")\n",
    "        print('=' * self._splitter_size)\n",
    "        print()\n",
    "\n",
    "    def load_checkpoint(self, path: Path):\n",
    "        print('=' * self._splitter_size)\n",
    "        print(f\"Restoring model {path}\")\n",
    "        checkpoint = torch.load(path)\n",
    "        self.current_epoch = checkpoint['epoch']\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"Model is restored.\")\n",
    "        print('=' * self._splitter_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare dataset\n",
      "Create vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9561/9561 [00:00<00:00, 35202.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 631.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "\n",
      "===================================\n",
      "Device: cpu\n",
      "Training dataset len: 17122\n",
      "Max / Optimal sentence len: 27\n",
      "Vocab size: 9626\n",
      "Batch size: 12\n",
      "Batched dataset len: 1426\n",
      "===================================\n",
      "\n",
      "Begin epoch 0\n",
      "00:00:03 | Epoch 1 | 20 / 1426 (1.4%) | NSP loss   0.72 | MLM loss   9.32\n",
      "00:00:06 | Epoch 1 | 40 / 1426 (2.81%) | NSP loss   0.71 | MLM loss   9.18\n",
      "00:00:09 | Epoch 1 | 60 / 1426 (4.21%) | NSP loss   0.71 | MLM loss   9.05\n",
      "00:00:12 | Epoch 1 | 80 / 1426 (5.61%) | NSP loss   0.70 | MLM loss   9.02\n",
      "00:00:14 | Epoch 1 | 100 / 1426 (7.01%) | NSP loss   0.70 | MLM loss   8.97\n",
      "00:00:17 | Epoch 1 | 120 / 1426 (8.42%) | NSP loss   0.70 | MLM loss   8.93\n",
      "00:00:20 | Epoch 1 | 140 / 1426 (9.82%) | NSP loss   0.68 | MLM loss   8.86\n",
      "00:00:23 | Epoch 1 | 160 / 1426 (11.22%) | NSP loss   0.71 | MLM loss   8.83\n",
      "00:00:26 | Epoch 1 | 180 / 1426 (12.62%) | NSP loss   0.69 | MLM loss   8.81\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "BASE_DIR = Path(('c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\')).resolve().parent\n",
    "\n",
    "EMB_SIZE = 64\n",
    "HIDDEN_SIZE = 36\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 12\n",
    "NUM_HEADS = 4\n",
    "\n",
    "CHECKPOINT_DIR = BASE_DIR.joinpath('data/bert_checkpoints')\n",
    "\n",
    "timestamp = datetime.datetime.utcnow().timestamp()\n",
    "LOG_DIR = BASE_DIR.joinpath(f'data/logs/bert_experiment_{timestamp}')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Prepare dataset\")\n",
    "    ds = IMDBBertDataset(BASE_DIR.joinpath('data/imdb.csv'), ds_from=0, ds_to=1000)\n",
    "\n",
    "    bert = BERT(len(ds.vocab), EMB_SIZE, HIDDEN_SIZE, NUM_HEADS).to(device)\n",
    "    trainer = BertTrainer(\n",
    "        model=bert,\n",
    "        dataset=ds,\n",
    "        log_dir=LOG_DIR,\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        print_progress_every=20,\n",
    "        print_accuracy_every=200,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=0.00007,\n",
    "        epochs=15\n",
    "    )\n",
    "\n",
    "    trainer.print_summary()\n",
    "    trainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
