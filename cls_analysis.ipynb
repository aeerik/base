{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\n",
      "\n",
      " Retrieving data from: c:\\Users\\erika\\Desktop\\Exjobb\\data\n",
      "Loading data...\n",
      "Data correctly loaded, 6485 samples found\n",
      "Creating vocabulary...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#load model\n",
    "#load data\n",
    "# create new dataset which has more information (country, number of genes etc)\n",
    "# run a 1 epoch no batching training\n",
    "# extract the CLS \n",
    "#PCA --> Plotting \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "from data_preprocessing import data_loader, data_original\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from bert_builder import BERT\n",
    "from misc import get_paths\n",
    "from misc import model_loader\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "############################\n",
    "model_name = '2024-03-14modelEnc3Emb256Mask0.15ModeTrue.pt'\n",
    "hyperparameters = re.findall('[A-Z][^A-Z]*', model_name)\n",
    "numbers = []\n",
    "for i in range (len(hyperparameters)):\n",
    "    numbers.append(re.findall('\\d+', hyperparameters[i]))\n",
    "\n",
    "parameters = [int(num) for sublist in numbers for num in sublist if num]\n",
    "\n",
    "\n",
    "num_enc = parameters[0]\n",
    "dim_emb = parameters[1]\n",
    "dim_hidden = parameters[1]\n",
    "mask_prob = 0\n",
    "\n",
    "########################\n",
    "threshold_year = 1970\n",
    "max_length = [51,44]\n",
    "mask_prob = 0.15\n",
    "drop_prob = 0.2\n",
    "reduced_samples = 1000 \n",
    "\n",
    "attention_heads = 4 \n",
    "\n",
    "include_pheno = True   \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#############################\n",
    "\n",
    "base_dir = Path(os.path.abspath(''))\n",
    "os.chdir(base_dir)\n",
    "data_dir, ab_dir, save_directory = get_paths()\n",
    "\n",
    "print(f\"\\n Retrieving data from: {data_dir}\")\n",
    "print(\"Loading data...\")\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_dir,ab_dir)\n",
    "NCBI_geno_only = data_original(threshold_year,data_dir, ab_dir)\n",
    "print(f\"Data correctly loaded, {len(NCBI)} samples found\")\n",
    "print(\"Creating vocabulary...\")\n",
    "vocabulary_geno = vocab_geno(NCBI_geno_only)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "class CLSDataset(Dataset):\n",
    "\n",
    "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
    "    TARGET_COLUMN = 'indices'\n",
    "    TOKEN_MASK_COLUMN = 'token_mask'\n",
    "    AB_INDEX = 'ab_index'\n",
    "    SR_CLASS = 'sr_class'\n",
    "    NUM_GENES = 'num_genes'\n",
    "    NUM_ABS = 'num_abs'\n",
    "    LOCATIONS = 'locations'\n",
    "\n",
    "    def __init__(self,\n",
    "                 data: pd.DataFrame,\n",
    "                 vocab_geno: vocab,\n",
    "                 vocab_pheno: vocab,\n",
    "                 max_seq_len: list,\n",
    "                 mask_prob: float,\n",
    "                 include_pheno:bool,\n",
    "                 random_state: int = 23,\n",
    "                 ):\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        CLS = '[CLS]'\n",
    "        PAD = '[PAD]'\n",
    "        MASK = '[MASK]'\n",
    "        UNK = '[UNK]'\n",
    "\n",
    "        self.include_pheno = include_pheno\n",
    "        self.data = data.reset_index(drop=True) \n",
    "        self.num_samples = self.data.shape[0]\n",
    "        self.vocab_geno = vocab_geno\n",
    "        self.vocab_pheno = vocab_pheno\n",
    "        self.vocab_size_geno = len(self.vocab_geno)\n",
    "        self.CLS = CLS \n",
    "        self.PAD = PAD\n",
    "        self.MASK = MASK\n",
    "        self.UNK = UNK\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "        self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN, self.AB_INDEX, self.SR_CLASS, self.NUM_GENES, self.NUM_ABS, self.LOCATIONS]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        input = torch.tensor(item[self.MASKED_INDICES_COLUMN],device=device).long()\n",
    "        token_mask  = torch.tensor(item[self.TARGET_COLUMN], device=device).long()\n",
    "        attention_mask = (input == self.vocab_geno[self.PAD]).unsqueeze(0)\n",
    "        ab_idx  = torch.tensor(item[self.AB_INDEX], device=device).long()\n",
    "        sr_class = torch.tensor(item[self.SR_CLASS], device=device).long()\n",
    "        num_genes = torch.tensor(item[self.NUM_GENES], device=device).long()\n",
    "        num_abs = torch.tensor(item[self.NUM_ABS], device=device).long()\n",
    "        locations = torch.tensor(item[self.LOCATIONS], device=device).long()\n",
    "\n",
    "        return input, token_mask , attention_mask, ab_idx, sr_class, num_genes, num_abs, locations\n",
    "\n",
    "\n",
    "    def _construct_masking(self):\n",
    "        sequences = deepcopy(self.data['genes'].tolist())\n",
    "        masked_sequences = []\n",
    "        target_indices_list = []\n",
    "        seq_starts = [[self.CLS, self.data['year'].iloc[i], self.data['location'].iloc[i]] for i in range(self.data.shape[0])]\n",
    "\n",
    "        for i, geno_seq in enumerate(sequences):\n",
    "            seq_len = len(geno_seq)\n",
    "            masking_index = np.random.rand(seq_len) < self.mask_prob   \n",
    "            target_indices = np.array([-1]*seq_len)\n",
    "            indices = masking_index.nonzero()[0]\n",
    "            target_indices[indices] = self.vocab_geno.lookup_indices([geno_seq[i] for i in indices])\n",
    "            for i in indices:\n",
    "                r = np.random.rand()\n",
    "                if r < 0.8:\n",
    "                    geno_seq[i] = self.MASK\n",
    "                elif r > 0.9:\n",
    "                    geno_seq[i] = self.vocab_geno.lookup_token(np.random.randint(self.vocab_size_geno))\n",
    "            geno_seq = seq_starts[i] + geno_seq\n",
    "            target_indices = [-1]*3 + target_indices.tolist() \n",
    "            masked_sequences.append(geno_seq)\n",
    "            target_indices_list.append(target_indices)\n",
    "        masked_sequences = [seq + [self.PAD]*(self.max_seq_len[0] - len(seq)) for seq in masked_sequences]\n",
    "        for i in range(len(target_indices_list)):\n",
    "            indices = target_indices_list[i]\n",
    "            padding = [-1] * (self.max_seq_len[0] - len(indices))\n",
    "            target_indices_list[i] = indices + padding\n",
    "        return masked_sequences, target_indices_list \n",
    "    \n",
    "    def _Ab_SR_indexing(self):\n",
    "        sequences = deepcopy(self.data['AST_phenotypes'].tolist())\n",
    "        list_idx = []\n",
    "        list_SR = []\n",
    "        for i in range(len(sequences)):\n",
    "            current_seq = sequences[i]\n",
    "            current_idxs = []\n",
    "            current_SRs = []\n",
    "            for j in range(len(current_seq)):\n",
    "                item = current_seq[j].split('=')\n",
    "                abs = item[0]   \n",
    "                sr = item[1]\n",
    "                current_idxs.append(self.vocab_pheno.lookup_indices([abs]))\n",
    "                for k in range(len(sr)):\n",
    "                    if sr == 'R':\n",
    "                        current_SRs.append(1)\n",
    "                    else:\n",
    "                        current_SRs.append(0)\n",
    "            current_idxs = [int(item[0]) for item in current_idxs]\n",
    "            for i in range(0,self.max_seq_len[1] - len(current_idxs)):\n",
    "                current_idxs.append(-1)\n",
    "            for i in range(0,self.max_seq_len[1] - len(current_SRs)):\n",
    "                current_SRs.append(-1)\n",
    "            list_idx.append(current_idxs)\n",
    "            list_SR.append(current_SRs)\n",
    "        return list_idx, list_SR\n",
    "    \n",
    "    def _num_tested(self):\n",
    "        ab_sequences = deepcopy(self.data['AST_phenotypes'].tolist())\n",
    "        gene_sequences = deepcopy(self.data['genes'].tolist())\n",
    "        num_genes = []\n",
    "        num_abs = []\n",
    "        for i in range(len(gene_sequences)):\n",
    "            current_gene_seq = gene_sequences[i]\n",
    "            current_ab_seq = ab_sequences[i]\n",
    "\n",
    "            num_genes.append(len(current_gene_seq))\n",
    "            num_abs.append(len(current_ab_seq))\n",
    "        \n",
    "        return num_genes, num_abs\n",
    "    \n",
    "    def _location(self):\n",
    "        location = deepcopy(self.data['location'].tolist())\n",
    "        locations = []\n",
    "        for i in range(len(location)):\n",
    "            current_location = location[i]\n",
    "\n",
    "            locations.append(self.vocab_geno.lookup_indices([current_location]))\n",
    "        \n",
    "        return locations\n",
    "    \n",
    "    \n",
    "    def prepare_dataset(self):\n",
    "        masked_sequences, target_indices = self._construct_masking()\n",
    "        indices_masked = [self.vocab_geno.lookup_indices(masked_seq) for masked_seq in masked_sequences]\n",
    "        list_idx, list_SR = self._Ab_SR_indexing()\n",
    "        num_genes, num_abs = self._num_tested()\n",
    "        locations = self._location()    \n",
    "\n",
    "        rows = zip(indices_masked, target_indices, list_idx, list_SR,num_genes,num_abs, locations)\n",
    "        self.df = pd.DataFrame(rows, columns=self.columns)\n",
    "\n",
    "dataset = CLSDataset(NCBI, vocabulary_geno, vocabulary_pheno, max_length, mask_prob, include_pheno)\n",
    "dataset.prepare_dataset()\n",
    "\n",
    "class getCLS:\n",
    "    def __init__(self, model, dataset, epochs, batch_size,device):\n",
    "        \n",
    "        random_seed = 41\n",
    "        np.random.seed(random_seed)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.epochs = epochs    \n",
    "        self.batch_size = batch_size\n",
    "        self.current_epoch  = 0\n",
    "\n",
    "\n",
    "        self.device = device\n",
    "    def __call__(self):    \n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
    "            self.CLS_out = self.train(self.current_epoch)\n",
    "\n",
    "            results = {\n",
    "            \"CLS_tokens\": self.CLS_out, \n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _init_result_lists(self):\n",
    "        self.CLS_out = []\n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        cls_tokens = []\n",
    "        for i, batch in enumerate(self.data_loader):\n",
    "            input, token_target, attn_mask, AB_idx, SR_class, filler1, filler2, filler3 = batch\n",
    "\n",
    "            token_predictions, resistance_predictions, cls_s = self.model(input, attn_mask) \n",
    "            cls_tokens.append(cls_s)\n",
    "\n",
    "        cls_tokens = torch.cat(cls_tokens, dim=0)\n",
    "            \n",
    "        return cls_tokens\n",
    "\n",
    "model = BERT(vocab_size=len(vocabulary_geno), dim_embedding = dim_emb, dim_hidden=dim_hidden, attention_heads=8, num_encoders=num_enc, dropout_prob=drop_prob, num_ab=len(vocabulary_pheno), device=device).to(device)\n",
    "trainer = getCLS(model, dataset, 1, len(NCBI), device)\n",
    "\n",
    "results = trainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exjobb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
