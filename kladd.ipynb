{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from itertools import chain \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lokalt\n",
    "data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\repo\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station√§r\n",
    "data_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saga\n",
    "data_dir = \"/home/aeerik/data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget config file\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "max_length = [51,81]\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 32\n",
    "drop_prob = 0.2\n",
    "\n",
    "#Encoder\n",
    "dim_emb = 128\n",
    "dim_hidden = 128\n",
    "attention_heads = 8 \n",
    "\n",
    "#BERT\n",
    "num_encoders = 2\n",
    "\n",
    "#trainer\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "stop_patience = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "include_pheno = False\n",
    "vocabulary = vocab_geno(NCBI, include_pheno)\n",
    "vocab = vocab_pheno(ab_df)\n",
    "print(len(vocabulary))\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from data_preprocessing import data_loader\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "\n",
    "max_length = [51,81]\n",
    "mask_prob = 0.25\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = deepcopy(NCBI['AST_phenotypes'].tolist())\n",
    "max_seq_len = [max_length[0],max_length[1]]\n",
    "list_idx = []\n",
    "list_SR = []\n",
    "for i in range(len(sequences)):\n",
    "    current_seq = sequences[i]\n",
    "    current_idxs = []\n",
    "    current_SRs = []\n",
    "    for j in range(len(current_seq)):\n",
    "        item = current_seq[j].split('=')\n",
    "        abs = item[0]   \n",
    "        sr = item[1]\n",
    "        current_idxs.append(vocabulary_pheno.lookup_indices([abs]))\n",
    "        for k in range(len(sr)):\n",
    "            if sr == 'R':\n",
    "                current_SRs.append(1)\n",
    "            else:\n",
    "                current_SRs.append(0)\n",
    "\n",
    "    if len(current_idxs) != len(current_SRs):\n",
    "        print(\"current sequence:\",current_seq, \"\\n\", \"with length:\", len(current_seq))\n",
    "        print(\"indexes:\",current_idxs, \"with length:\", len(current_idxs))\n",
    "        print(\"suceptability\",current_SRs, \"with length:\", len(current_SRs))\n",
    "        print('error at', j)\n",
    "        print(\"--------------------\")\n",
    "    current_idxs = [int(item[0]) for item in current_idxs]\n",
    "    #for i in range(0,max_length[1] - len(current_idxs)):\n",
    "    #    current_idxs.append(-1)\n",
    "    #for i in range(0,max_length[1] - len(current_SRs)):\n",
    "    #    current_SRs.append(-1)\n",
    "    list_idx.append(current_idxs)\n",
    "    list_SR.append(current_SRs)\n",
    "for i in range(len(list_idx)):\n",
    "    if len(list_idx[i]) != len(list_SR[i]):\n",
    "        print('error at', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_loss(logits, targets, pad_index=-1):\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "    \n",
    "    mask = (targets != pad_index).float()\n",
    "    masked_loss = loss * mask\n",
    "    \n",
    "    average_loss = masked_loss.sum() / mask.sum()\n",
    "    \n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4494, grad_fn=<DivBackward0>)\n",
      "tensor(0.6192, grad_fn=<DivBackward0>)\n",
      "tensor(0.4631, grad_fn=<DivBackward0>)\n",
      "tensor(0.8214, grad_fn=<DivBackward0>)\n",
      "tensor(0.8458, grad_fn=<DivBackward0>)\n",
      "tensor(0.6789, grad_fn=<DivBackward0>)\n",
      "tensor(0.7272, grad_fn=<DivBackward0>)\n",
      "tensor(0.6520, grad_fn=<DivBackward0>)\n",
      "tensor(0.7455, grad_fn=<DivBackward0>)\n",
      "tensor(0.6448, grad_fn=<DivBackward0>)\n",
      "tensor(0.7252, grad_fn=<DivBackward0>)\n",
      "tensor(0.7435, grad_fn=<DivBackward0>)\n",
      "tensor(0.6430, grad_fn=<DivBackward0>)\n",
      "tensor(0.6515, grad_fn=<DivBackward0>)\n",
      "tensor(0.6933, grad_fn=<DivBackward0>)\n",
      "tensor(0.6552, grad_fn=<DivBackward0>)\n",
      "pheno tensor(10.7591, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.6246, grad_fn=<DivBackward0>)\n",
      "tensor(0.5742, grad_fn=<DivBackward0>)\n",
      "tensor(0.7922, grad_fn=<DivBackward0>)\n",
      "tensor(0.5629, grad_fn=<DivBackward0>)\n",
      "tensor(0.6612, grad_fn=<DivBackward0>)\n",
      "tensor(0.6294, grad_fn=<DivBackward0>)\n",
      "tensor(0.6250, grad_fn=<DivBackward0>)\n",
      "tensor(0.5781, grad_fn=<DivBackward0>)\n",
      "tensor(0.7151, grad_fn=<DivBackward0>)\n",
      "tensor(0.7066, grad_fn=<DivBackward0>)\n",
      "tensor(0.6012, grad_fn=<DivBackward0>)\n",
      "tensor(0.5439, grad_fn=<DivBackward0>)\n",
      "tensor(0.5832, grad_fn=<DivBackward0>)\n",
      "tensor(0.5798, grad_fn=<DivBackward0>)\n",
      "tensor(0.6079, grad_fn=<DivBackward0>)\n",
      "tensor(0.5336, grad_fn=<DivBackward0>)\n",
      "pheno tensor(9.9191, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5629, grad_fn=<DivBackward0>)\n",
      "tensor(0.5458, grad_fn=<DivBackward0>)\n",
      "tensor(0.5672, grad_fn=<DivBackward0>)\n",
      "tensor(0.6077, grad_fn=<DivBackward0>)\n",
      "tensor(0.6073, grad_fn=<DivBackward0>)\n",
      "tensor(0.5151, grad_fn=<DivBackward0>)\n",
      "tensor(0.5960, grad_fn=<DivBackward0>)\n",
      "tensor(0.5461, grad_fn=<DivBackward0>)\n",
      "tensor(0.5753, grad_fn=<DivBackward0>)\n",
      "tensor(0.4369, grad_fn=<DivBackward0>)\n",
      "tensor(0.5634, grad_fn=<DivBackward0>)\n",
      "tensor(0.4985, grad_fn=<DivBackward0>)\n",
      "tensor(0.5628, grad_fn=<DivBackward0>)\n",
      "tensor(0.5415, grad_fn=<DivBackward0>)\n",
      "tensor(0.5369, grad_fn=<DivBackward0>)\n",
      "tensor(0.6386, grad_fn=<DivBackward0>)\n",
      "pheno tensor(8.9020, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5018, grad_fn=<DivBackward0>)\n",
      "tensor(0.6425, grad_fn=<DivBackward0>)\n",
      "tensor(0.3489, grad_fn=<DivBackward0>)\n",
      "tensor(0.4459, grad_fn=<DivBackward0>)\n",
      "tensor(0.5718, grad_fn=<DivBackward0>)\n",
      "tensor(0.3695, grad_fn=<DivBackward0>)\n",
      "tensor(0.4279, grad_fn=<DivBackward0>)\n",
      "tensor(0.5357, grad_fn=<DivBackward0>)\n",
      "tensor(0.4736, grad_fn=<DivBackward0>)\n",
      "tensor(0.4826, grad_fn=<DivBackward0>)\n",
      "tensor(0.7257, grad_fn=<DivBackward0>)\n",
      "tensor(0.5394, grad_fn=<DivBackward0>)\n",
      "tensor(0.4723, grad_fn=<DivBackward0>)\n",
      "tensor(0.6047, grad_fn=<DivBackward0>)\n",
      "tensor(0.5224, grad_fn=<DivBackward0>)\n",
      "tensor(0.5808, grad_fn=<DivBackward0>)\n",
      "pheno tensor(8.2456, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.4660, grad_fn=<DivBackward0>)\n",
      "tensor(0.4852, grad_fn=<DivBackward0>)\n",
      "tensor(0.6586, grad_fn=<DivBackward0>)\n",
      "tensor(0.5002, grad_fn=<DivBackward0>)\n",
      "tensor(0.3570, grad_fn=<DivBackward0>)\n",
      "tensor(0.3346, grad_fn=<DivBackward0>)\n",
      "tensor(0.7593, grad_fn=<DivBackward0>)\n",
      "tensor(0.3917, grad_fn=<DivBackward0>)\n",
      "tensor(0.3500, grad_fn=<DivBackward0>)\n",
      "tensor(0.4950, grad_fn=<DivBackward0>)\n",
      "tensor(0.5639, grad_fn=<DivBackward0>)\n",
      "tensor(0.3917, grad_fn=<DivBackward0>)\n",
      "tensor(0.3394, grad_fn=<DivBackward0>)\n",
      "tensor(0.4299, grad_fn=<DivBackward0>)\n",
      "tensor(0.3297, grad_fn=<DivBackward0>)\n",
      "tensor(0.5289, grad_fn=<DivBackward0>)\n",
      "pheno tensor(7.3811, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5784, grad_fn=<DivBackward0>)\n",
      "tensor(0.7858, grad_fn=<DivBackward0>)\n",
      "tensor(0.7450, grad_fn=<DivBackward0>)\n",
      "tensor(0.8342, grad_fn=<DivBackward0>)\n",
      "tensor(0.8003, grad_fn=<DivBackward0>)\n",
      "tensor(0.7615, grad_fn=<DivBackward0>)\n",
      "tensor(0.7754, grad_fn=<DivBackward0>)\n",
      "tensor(0.8403, grad_fn=<DivBackward0>)\n",
      "tensor(0.7923, grad_fn=<DivBackward0>)\n",
      "tensor(0.7989, grad_fn=<DivBackward0>)\n",
      "tensor(0.9176, grad_fn=<DivBackward0>)\n",
      "tensor(0.7700, grad_fn=<DivBackward0>)\n",
      "tensor(0.7818, grad_fn=<DivBackward0>)\n",
      "tensor(0.8853, grad_fn=<DivBackward0>)\n",
      "tensor(0.8046, grad_fn=<DivBackward0>)\n",
      "tensor(0.8255, grad_fn=<DivBackward0>)\n",
      "pheno tensor(12.6967, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.7293, grad_fn=<DivBackward0>)\n",
      "tensor(0.7857, grad_fn=<DivBackward0>)\n",
      "tensor(0.7156, grad_fn=<DivBackward0>)\n",
      "tensor(0.6079, grad_fn=<DivBackward0>)\n",
      "tensor(0.7837, grad_fn=<DivBackward0>)\n",
      "tensor(0.6396, grad_fn=<DivBackward0>)\n",
      "tensor(0.6884, grad_fn=<DivBackward0>)\n",
      "tensor(0.8099, grad_fn=<DivBackward0>)\n",
      "tensor(0.7502, grad_fn=<DivBackward0>)\n",
      "tensor(0.7205, grad_fn=<DivBackward0>)\n",
      "tensor(0.5605, grad_fn=<DivBackward0>)\n",
      "tensor(0.6232, grad_fn=<DivBackward0>)\n",
      "tensor(0.8054, grad_fn=<DivBackward0>)\n",
      "tensor(0.7512, grad_fn=<DivBackward0>)\n",
      "tensor(0.6784, grad_fn=<DivBackward0>)\n",
      "tensor(0.7003, grad_fn=<DivBackward0>)\n",
      "pheno tensor(11.3499, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5806, grad_fn=<DivBackward0>)\n",
      "tensor(0.5937, grad_fn=<DivBackward0>)\n",
      "tensor(0.5269, grad_fn=<DivBackward0>)\n",
      "tensor(0.6608, grad_fn=<DivBackward0>)\n",
      "tensor(0.5937, grad_fn=<DivBackward0>)\n",
      "tensor(0.6394, grad_fn=<DivBackward0>)\n",
      "tensor(0.5618, grad_fn=<DivBackward0>)\n",
      "tensor(0.5803, grad_fn=<DivBackward0>)\n",
      "tensor(0.7042, grad_fn=<DivBackward0>)\n",
      "tensor(0.6732, grad_fn=<DivBackward0>)\n",
      "tensor(0.6667, grad_fn=<DivBackward0>)\n",
      "tensor(0.5419, grad_fn=<DivBackward0>)\n",
      "tensor(0.5724, grad_fn=<DivBackward0>)\n",
      "tensor(0.5622, grad_fn=<DivBackward0>)\n",
      "tensor(0.5596, grad_fn=<DivBackward0>)\n",
      "tensor(0.5259, grad_fn=<DivBackward0>)\n",
      "pheno tensor(9.5435, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5112, grad_fn=<DivBackward0>)\n",
      "tensor(0.5430, grad_fn=<DivBackward0>)\n",
      "tensor(0.4644, grad_fn=<DivBackward0>)\n",
      "tensor(0.5875, grad_fn=<DivBackward0>)\n",
      "tensor(0.4972, grad_fn=<DivBackward0>)\n",
      "tensor(0.6459, grad_fn=<DivBackward0>)\n",
      "tensor(0.4852, grad_fn=<DivBackward0>)\n",
      "tensor(0.4855, grad_fn=<DivBackward0>)\n",
      "tensor(0.7060, grad_fn=<DivBackward0>)\n",
      "tensor(0.9851, grad_fn=<DivBackward0>)\n",
      "tensor(0.8329, grad_fn=<DivBackward0>)\n",
      "tensor(0.8666, grad_fn=<DivBackward0>)\n",
      "tensor(0.8465, grad_fn=<DivBackward0>)\n",
      "tensor(0.8306, grad_fn=<DivBackward0>)\n",
      "tensor(0.6029, grad_fn=<DivBackward0>)\n",
      "tensor(0.4709, grad_fn=<DivBackward0>)\n",
      "pheno tensor(10.3614, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5169, grad_fn=<DivBackward0>)\n",
      "tensor(0.7717, grad_fn=<DivBackward0>)\n",
      "tensor(0.7607, grad_fn=<DivBackward0>)\n",
      "tensor(0.8253, grad_fn=<DivBackward0>)\n",
      "tensor(0.8822, grad_fn=<DivBackward0>)\n",
      "tensor(0.8139, grad_fn=<DivBackward0>)\n",
      "tensor(0.7908, grad_fn=<DivBackward0>)\n",
      "tensor(0.7070, grad_fn=<DivBackward0>)\n",
      "tensor(0.7512, grad_fn=<DivBackward0>)\n",
      "tensor(0.7885, grad_fn=<DivBackward0>)\n",
      "tensor(0.6836, grad_fn=<DivBackward0>)\n",
      "tensor(0.8017, grad_fn=<DivBackward0>)\n",
      "tensor(0.7629, grad_fn=<DivBackward0>)\n",
      "tensor(0.6187, grad_fn=<DivBackward0>)\n",
      "tensor(0.6952, grad_fn=<DivBackward0>)\n",
      "tensor(0.8308, grad_fn=<DivBackward0>)\n",
      "pheno tensor(12.0012, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "------------------\n",
      "target tensor([ 0, -1,  1,  1,  1, -1,  1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1,\n",
      "        -1,  1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,\n",
      "        -1, -1,  0, -1, -1, -1,  0,  0, -1, -1,  1, -1, -1,  0, -1, -1, -1,  1,\n",
      "        -1, -1, -1,  0, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         1, -1, -1,  0,  0, -1,  1, -1, -1]) shape  torch.Size([81]) type <class 'torch.Tensor'>\n",
      "pred tensor([ 0.4786,  0.4658,  0.2647,  1.0500,  0.5575,  0.1078, -0.0405, -0.3351,\n",
      "        -0.6943, -0.5706, -1.3000, -0.0464,  0.1630,  0.0561,  1.1942, -0.7140,\n",
      "        -0.4864,  0.1857,  0.5022,  0.3904, -1.0160,  0.5669, -0.2253, -0.2588,\n",
      "        -0.5341, -0.0920, -0.8664,  0.3386,  0.3392, -0.5458, -0.3611, -0.9976,\n",
      "         0.6472, -0.2721, -1.0072, -0.2907,  0.6698, -0.0895,  0.9034,  0.2583,\n",
      "        -1.0019, -0.6530,  0.2442,  0.3329, -0.5051, -0.1699,  0.4667,  0.2443,\n",
      "        -0.2963, -0.2937, -0.6594, -0.5588,  0.5937,  1.1591, -0.1682, -0.3226,\n",
      "         0.8355,  0.2033,  0.1068, -0.2998,  0.3313, -0.1574,  0.1974, -1.0022,\n",
      "         0.7160,  0.5348, -0.6237,  0.7474, -0.1159, -0.4257, -0.6888, -0.4665,\n",
      "        -0.5394, -0.5916, -0.3950,  0.0524,  0.4405, -0.3407, -0.5185, -0.5657,\n",
      "        -0.3095]) shape  torch.Size([81]) type <class 'torch.Tensor'>\n",
      "tensor(0.5315)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from bert_builder import BERT_ft\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "bert_test = BERT_ft(len(vocabulary_geno), max_length, dim_emb, dim_hidden, attention_heads, num_encoders, drop_prob, len(vocabulary_pheno), device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(bert_test.parameters(), lr=0.001, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = -1).to(device)\n",
    "\n",
    "\n",
    "loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "for i, batch in enumerate(loader):\n",
    "    if i >= 10:\n",
    "        break \n",
    "    optimizer.zero_grad()\n",
    "    input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "    token_predictions, resistance_predictions = bert_test(input, attn_mask) \n",
    "    result_list = []\n",
    "    for j in range(len(AB_idx)):\n",
    "        result_tensor = torch.full((81,), -1)  # Create tensor filled with -1 values\n",
    "        for idx, value in enumerate(AB_idx[j]):\n",
    "            if value != -1:\n",
    "                result_tensor[value.item()] = SR_class[j][idx]\n",
    "                #print(AB_idx[j])\n",
    "                #print(SR_class[j])\n",
    "                #print(result_tensor)\n",
    "        result_list.append(result_tensor)\n",
    "    ab_loss = 0\n",
    "    pheno_loss = 0\n",
    "    for i, row in enumerate(resistance_predictions):\n",
    "        prediction = row\n",
    "        #print(\"pred\",prediction, \"shape \", prediction.shape,\"type\" ,type(prediction))\n",
    "        target = result_list[i]\n",
    "        #print(\"target\",target, \"shape \", target.shape, \"type\", type(target))\n",
    "        ab_loss = custom_loss(prediction, target.float()) \n",
    "        print(ab_loss)\n",
    "        pheno_loss += ab_loss\n",
    "    print(\"pheno\",pheno_loss)\n",
    "    params_before_optimization = copy.deepcopy(bert_test.state_dict())\n",
    "    pheno_loss.backward()\n",
    "    # Call optimizer.step() to perform optimization\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compare parameter values after optimization\n",
    "    params_after_optimization = bert_test.state_dict()\n",
    "\n",
    "    # Check if parameters were updated\n",
    "    parameters_updated = any((params_before_optimization[key] != params_after_optimization[key]).any() for key in params_before_optimization)\n",
    "\n",
    "    if parameters_updated:\n",
    "        print(\"Parameters were updated after optimization.\")\n",
    "    else:\n",
    "        print(\"Parameters were not updated after optimization.\")\n",
    "    \n",
    "\n",
    "target = torch.tensor([ 0, -1,  1,  1,  1, -1,  1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1,\n",
    "        -1,  1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,\n",
    "        -1, -1,  0, -1, -1, -1,  0,  0, -1, -1,  1, -1, -1,  0, -1, -1, -1,  1,\n",
    "        -1, -1, -1,  0, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "         1, -1, -1,  0,  0, -1,  1, -1, -1])\n",
    "prediction = torch.tensor([ 0.4786,  0.4658,  0.2647,  1.0500,  0.5575,  0.1078, -0.0405, -0.3351,\n",
    "        -0.6943, -0.5706, -1.3000, -0.0464,  0.1630,  0.0561,  1.1942, -0.7140,\n",
    "        -0.4864,  0.1857,  0.5022,  0.3904, -1.0160,  0.5669, -0.2253, -0.2588,\n",
    "        -0.5341, -0.0920, -0.8664,  0.3386,  0.3392, -0.5458, -0.3611, -0.9976,\n",
    "         0.6472, -0.2721, -1.0072, -0.2907,  0.6698, -0.0895,  0.9034,  0.2583,\n",
    "        -1.0019, -0.6530,  0.2442,  0.3329, -0.5051, -0.1699,  0.4667,  0.2443,\n",
    "        -0.2963, -0.2937, -0.6594, -0.5588,  0.5937,  1.1591, -0.1682, -0.3226,\n",
    "         0.8355,  0.2033,  0.1068, -0.2998,  0.3313, -0.1574,  0.1974, -1.0022,\n",
    "         0.7160,  0.5348, -0.6237,  0.7474, -0.1159, -0.4257, -0.6888, -0.4665,\n",
    "        -0.5394, -0.5916, -0.3950,  0.0524,  0.4405, -0.3407, -0.5185, -0.5657,\n",
    "        -0.3095])\n",
    "print(\"------------------\")\n",
    "print(\"target\",target, \"shape \", target.shape, \"type\", type(target))\n",
    "print(\"pred\",prediction, \"shape \", prediction.shape,\"type\" ,type(prediction))\n",
    "loss = loss_function(prediction, target.float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_AB_predictions = []\n",
    "    for i, row in enumerate(resistance_predictions):\n",
    "        AB_list = 0\n",
    "        AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "        current_abs = []\n",
    "        for ab in AB_list:\n",
    "            current_abs.append(row[ab].item())\n",
    "        current_abs = torch.tensor(current_abs, requires_grad=True)\n",
    "        list_AB_predictions.append(current_abs)\n",
    "    \n",
    "    processed_tensor = [row[row != -1] for row in SR_class]\n",
    "    ab_loss = 0\n",
    "    for i, row in enumerate(processed_tensor):\n",
    "        row = torch.tensor(row, dtype=torch.float32,requires_grad=True)\n",
    "        list_AB_predictions[i] = torch.tensor(list_AB_predictions[i], dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "        ab_loss += self.ab_criterion(list_AB_predictions[i], row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = res_pred[14]\n",
    "print(test)\n",
    "pred_res = torch.where(test > 0, torch.ones_like(test), torch.zeros_like(test))\n",
    "print(pred_res)\n",
    "indicies = [1,2,3,4]\n",
    "test = pred_res[indicies]\n",
    "print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from create_dataset import NCBIDataset\n",
    "def get_split_indices(size_to_split, val_share, random_state: int = 42):\n",
    "    indices = np.arange(size_to_split)\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_share = 1 - val_share\n",
    "    \n",
    "    train_size = int(train_share * size_to_split)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "max_length = 20\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BertTrainer_ft:\n",
    "    def __init__(self, model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, project_name, wandb_name):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.train_size = len(train_set)\n",
    "        self.val_set = val_set\n",
    "        self.epochs = epochs    \n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = self.train_size // self.batch_size\n",
    "        self.lr = lr\n",
    "        self.weight_decay = 0.1\n",
    "        self.current_epoch  = 0\n",
    "        self.early_stopping_counter = 0\t\n",
    "        self.patience = stop_patience\n",
    "        \n",
    "        self.wandb_mode = wandb_mode\n",
    "        self.project_name = project_name\n",
    "        self.wandb_name = wandb_name\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.token_criterion = nn.CrossEntropyLoss(ignore_index = -1).to(self.device)\n",
    "        self.ab_criterion = nn.BCEWithLogitsLoss().to(self.device)\n",
    "\n",
    "\n",
    "    def __call__(self):   \n",
    "        if self.wandb_mode:\n",
    "            self._init_wandb()   \n",
    "        self.val_set.prepare_dataset() \n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            #Training\n",
    "            self.model.train()\n",
    "            self.train_set.prepare_dataset()\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            avg_epoch_loss_geno, avg_epoch_loss_pheno = self.train(self.current_epoch)\n",
    "            self.train_losses_geno.append(avg_epoch_loss_geno) \n",
    "            self.train_losses_ab.append(avg_epoch_loss_pheno)  \n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min\")\n",
    "            \n",
    "            #Validation\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            val_results = self.evaluate(self.val_loader)\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            self.val_losses_geno.append(val_results[0])\n",
    "            self.val_losses_ab.append(val_results[1])\n",
    "            self.val_accs.append(val_results[2])\n",
    "            if self.wandb_mode:\n",
    "                self._report_epoch_results()\n",
    "            criterion = self.stop_early()\n",
    "            if criterion:\n",
    "                print(f\"Training interrupted at epoch: {self.current_epoch+1}\")\n",
    "                break\n",
    "        print(f\"-=Training completed=-\")\n",
    "        results = {\n",
    "            \"best_epoch\": self.best_epoch,\n",
    "            \"geno_train_losses\": self.train_losses_geno,\n",
    "            \"ab_train_losses\": self.train_losses_ab,\n",
    "            \"geno_val_losses\": self.val_losses_geno,\n",
    "            \"ab_val_losses\": self.val_losses_ab,\n",
    "            \"val_accs\": self.val_accs\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _init_result_lists(self):\n",
    "\n",
    "        self.train_losses_geno = []\n",
    "        self.train_losses_ab = []\n",
    "\n",
    "        self.val_losses_geno = []\n",
    "        self.val_losses_ab = []\n",
    "\n",
    "        self.val_accs = []\n",
    "    \n",
    "    def stop_early(self):\n",
    "        if self.val_losses_ab[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses_ab[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss_geno = 0\n",
    "        epoch_loss_pheno = 0\n",
    "\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "            \n",
    "            ABinclusion = torch.unique(AB_idx)\n",
    "            ABinclusion = ABinclusion[ABinclusion != -1]\n",
    "            ABinclusion = ABinclusion.tolist()\n",
    "            self.model.exclude_networks(ABinclusion)\n",
    "\n",
    "            self.optimizer.zero_grad() \n",
    "\n",
    "            token_predictions, resistance_predictions = self.model(input, attn_mask) \n",
    "            geno_loss = self.token_criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "            \n",
    "            result_list = []\n",
    "            for j in range(len(AB_idx)):\n",
    "                result_tensor = torch.full((81,), -1) \n",
    "                for idx, value in enumerate(AB_idx[j]):\n",
    "                    if value != -1:\n",
    "                        result_tensor[value.item()] = SR_class[j][idx]\n",
    "                result_list.append(result_tensor)\n",
    "            ab_loss = 0\n",
    "            pheno_loss = 0\n",
    "            for i, row in enumerate(resistance_predictions):\n",
    "                prediction = row\n",
    "                target = result_list[i]\n",
    "                ab_loss = custom_loss(prediction, target.float()) \n",
    "                pheno_loss += ab_loss\n",
    "            pheno_loss.backward() \n",
    "            epoch_loss_geno += geno_loss.item()\n",
    "            epoch_loss_pheno += pheno_loss.item()\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.model.reset_exclusion()   \n",
    "              \n",
    "\n",
    "        avg_epoch_loss_geno = epoch_loss_geno / self.num_batches\n",
    "        avg_epoch_loss_pheno = epoch_loss_pheno / self.num_batches\n",
    "\n",
    "        return avg_epoch_loss_geno, avg_epoch_loss_pheno\n",
    "\n",
    "    def custom_loss(logits, targets, pad_index=-1):\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        \n",
    "        mask = (targets != pad_index).float()\n",
    "        masked_loss = loss * mask\n",
    "        \n",
    "        average_loss = masked_loss.sum() / mask.sum()\n",
    "        \n",
    "        return average_loss\n",
    "\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        epoch_loss_geno = 0\n",
    "        epoch_loss_ab = 0\n",
    "        total_correct = 0\n",
    "        total_sum = 0\n",
    "  \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(loader):\n",
    "                input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "\n",
    "                token_predictions, resistance_predictions = self.model(input, attn_mask) \n",
    "                geno_loss = self.token_criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "                \n",
    "                result_list = []\n",
    "                for j in range(len(AB_idx)):\n",
    "                    result_tensor = torch.full((81,), -1)  # Create tensor filled with -1 values\n",
    "                    for idx, value in enumerate(AB_idx[j]):\n",
    "                        if value != -1:\n",
    "                            result_tensor[value.item()] = SR_class[j][idx]\n",
    "                    result_list.append(result_tensor)\n",
    "                ab_loss = 0\n",
    "                pheno_loss = 0\n",
    "                for i, row in enumerate(resistance_predictions):\n",
    "                    prediction = row\n",
    "                    target = result_list[i]\n",
    "                    ab_loss = custom_loss(prediction, target.float()) \n",
    "                    pheno_loss += ab_loss\n",
    "                epoch_loss_geno += geno_loss.item()\n",
    "                epoch_loss_ab += pheno_loss.item() \n",
    "                \n",
    "                list_AB_predictions = []\n",
    "                for i, row in enumerate(resistance_predictions):\n",
    "                    AB_list = 0\n",
    "                    AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                    current_abs = []\n",
    "                    for ab in AB_list:\n",
    "                        current_abs.append(row[ab].item())\n",
    "                    current_abs = torch.tensor(current_abs)\n",
    "                    current_abs = current_abs.type(torch.int16)\n",
    "                    list_AB_predictions.append(current_abs)\n",
    "                \n",
    "                    processed_tensor = [row[row != -1] for row in SR_class]\n",
    "                for i, row in enumerate(processed_tensor):\n",
    "                    total_correct += (row == list_AB_predictions[i]).sum().item()\n",
    "                    total_sum += len(row)\n",
    "\n",
    "        avg_epoch_loss_geno = epoch_loss_geno / self.num_batches\n",
    "        avg_epoch_loss_ab = epoch_loss_ab / self.num_batches\n",
    "\n",
    "        accuracy = total_correct / total_sum\n",
    "\n",
    "        return avg_epoch_loss_geno, avg_epoch_loss_ab, accuracy\n",
    "    \n",
    "    def _save_model(self, savepath: Path):\n",
    "        torch.save(self.best_model_state, savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        \n",
    "        \n",
    "    def _load_model(self, savepath: Path):\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "    def _init_wandb(self):\n",
    "        self.wandb_run = wandb.init(\n",
    "            project=self.project_name, # name of the project\n",
    "            name=self.wandb_name, # name of the run\n",
    "            \n",
    "            config={\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"num_heads\": self.model.attention_heads,\n",
    "                \"num_encoders\": self.model.num_encoders,\n",
    "                \"emb_dim\": self.model.dim_embedding,\n",
    "                'ff_dim': self.model.dim_embedding,\n",
    "                \"lr\": self.lr,\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "                \"max_seq_len\": self.model.max_length[0],\n",
    "                \"vocab_size\": len(self.train_set.vocab_geno),\n",
    "                \"num_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "            }\n",
    "        )\n",
    "        self.wandb_run.watch(self.model) # watch the model for gradients and parameters\n",
    "        self.wandb_run.define_metric(\"epoch\", hidden=True)\n",
    "        self.wandb_run.define_metric(\"batch\", hidden=True)\n",
    "\n",
    "        self.wandb_run.define_metric(\"GenoLosses/geno_train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"GenoLosses/geno_val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "        self.wandb_run.define_metric(\"AB_Losses/ab_train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"AB_Losses/ab_val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_acc\", summary=\"min\", step_metric=\"epoch\")\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/final_val_loss\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_acc\")\n",
    "        self.wandb_run.define_metric(\"final_epoch\")\n",
    "\n",
    "        return self.wandb_run\n",
    "    \n",
    "    def _report_epoch_results(self):\n",
    "        wandb_dict = {\n",
    "            \"epoch\": self.current_epoch+1,\n",
    "            \n",
    "            \"GenoLosses/geno_train_loss\": self.train_losses_geno[-1],\n",
    "            \"ABLosses/ab_train_loss\": self.train_losses_ab[-1],\n",
    "\n",
    "            \"GenoLosses/geno_val_loss\": self.val_losses_geno[-1],\n",
    "            \"ABLosses/ab_val_loss\": self.val_losses_ab[-1],\n",
    "            \n",
    "            \"Accuracies/val_acc\": self.val_accs[-1],\n",
    "        }\n",
    "        self.wandb_run.log(wandb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import data_loader\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from misc import get_split_indices\n",
    "from create_dataset import NCBIDataset\n",
    "include_pheno = False\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "\n",
    "\n",
    "include_pheno = False\n",
    "max_length = [51,81]\n",
    "mask_prob = 0.25\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_builder import BERT_pt\n",
    "from trainer import BertTrainer_pt\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BERT_pt(vocab_size=len(vocabulary_geno), max_length=51, dim_embedding = 128, attention_heads=8, num_encoders=2, dropout_prob=0.2)\n",
    "save_directory = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\savefiles'\n",
    "\n",
    "trainer = BertTrainer_pt(model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, save_directory)\n",
    "trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded, 6483 samples found\n",
      "length  of token vocabulary: 475\n"
     ]
    }
   ],
   "source": [
    "from bert_builder import BERT_ft\n",
    "from data_preprocessing import data_loader\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from misc import get_split_indices\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "include_pheno = True\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "#reduced_samples = 4000\n",
    "#NCBI = NCBI.head(reduced_samples)\n",
    "\n",
    "print(f\"Data loaded, {len(NCBI)} samples found\")\n",
    "print(f\"length  of token vocabulary:\",len(vocabulary_geno))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:u9d1xn4x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ABLosses/ab_train_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>ABLosses/ab_val_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ</td></tr><tr><td>Accuracies/val_acc</td><td>‚ñà‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>GenoLosses/geno_train_loss</td><td> ‚ñá‚ñà‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>GenoLosses/geno_val_loss</td><td>‚ñà‚ñÑ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÑ‚ñÖ</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ABLosses/ab_train_loss</td><td>8.47424</td></tr><tr><td>ABLosses/ab_val_loss</td><td>2.06436</td></tr><tr><td>epoch</td><td>12</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MaxSamples_2</strong> at: <a href='https://wandb.ai/strompfel/Trial_runs/runs/u9d1xn4x' target=\"_blank\">https://wandb.ai/strompfel/Trial_runs/runs/u9d1xn4x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240226_092206-u9d1xn4x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:u9d1xn4x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\erikw\\Desktop\\Exjobb kod\\base\\wandb\\run-20240226_093410-tvejcc2l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/strompfel/Trial_runs/runs/tvejcc2l' target=\"_blank\">MaxSamples_2</a></strong> to <a href='https://wandb.ai/strompfel/Trial_runs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/strompfel/Trial_runs' target=\"_blank\">https://wandb.ai/strompfel/Trial_runs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/strompfel/Trial_runs/runs/tvejcc2l' target=\"_blank\">https://wandb.ai/strompfel/Trial_runs/runs/tvejcc2l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:42\n",
      "Epoch 2/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:01:24\n",
      "Epoch 3/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:02:06\n",
      "Epoch 4/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:02:49\n",
      "Epoch 5/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:03:32\n",
      "Epoch 6/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:04:15\n",
      "Epoch 7/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:04:58\n",
      "Epoch 8/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:05:41\n",
      "Epoch 9/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:06:24\n",
      "Epoch 10/30\n",
      "Epoch completed in 0.7 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:07:08\n",
      "Epoch 11/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:07:51\n",
      "Epoch 12/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:08:34\n",
      "Epoch 13/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:09:17\n",
      "Epoch 14/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:10:00\n",
      "Epoch 15/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:10:42\n",
      "Epoch 16/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:11:26\n",
      "Epoch 17/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:12:10\n",
      "Epoch 18/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:12:53\n",
      "Epoch 19/30\n",
      "Epoch completed in 0.7 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:13:39\n",
      "Epoch 20/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:14:23\n",
      "Epoch 21/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:15:07\n",
      "Epoch 22/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:15:52\n",
      "Epoch 23/30\n",
      "Epoch completed in 0.7 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:16:39\n",
      "Epoch 24/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:17:22\n",
      "Epoch 25/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:18:04\n",
      "Epoch 26/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:18:45\n",
      "Epoch 27/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:19:26\n",
      "Epoch 28/30\n",
      "Epoch completed in 0.6 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:20:07\n",
      "Training interrupted at epoch: 28\n",
      "-=Training completed=-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 22,\n",
       " 'geno_train_losses': [6.509485895251051,\n",
       "  nan,\n",
       "  6.45091062710609,\n",
       "  6.452215127003046,\n",
       "  6.439010040259656,\n",
       "  6.451745283456496,\n",
       "  6.431238848486064,\n",
       "  6.447394635942247,\n",
       "  6.43535570450771,\n",
       "  6.438558978798949,\n",
       "  nan,\n",
       "  6.42722154841011,\n",
       "  6.433914914543246,\n",
       "  6.411501357584823,\n",
       "  6.431412431928846,\n",
       "  nan,\n",
       "  6.402492025752126,\n",
       "  nan,\n",
       "  6.398771545033396,\n",
       "  6.394085301293267,\n",
       "  6.404544156274678,\n",
       "  nan,\n",
       "  6.38123443391588,\n",
       "  6.395017715147984,\n",
       "  6.38099784909943,\n",
       "  nan,\n",
       "  6.391568416430626,\n",
       "  6.3692129482457664],\n",
       " 'ab_train_losses': [13.942978940260263,\n",
       "  11.35505305028256,\n",
       "  10.671379770393726,\n",
       "  10.255622599043964,\n",
       "  9.982498461265623,\n",
       "  9.783324732456679,\n",
       "  9.648679252024051,\n",
       "  9.51229977791692,\n",
       "  9.389829956822926,\n",
       "  9.278195781104358,\n",
       "  9.24832558558311,\n",
       "  9.194965418474174,\n",
       "  9.059573783182804,\n",
       "  9.055769923108596,\n",
       "  9.068263903076266,\n",
       "  8.987652490536371,\n",
       "  8.936795033422516,\n",
       "  8.893707693358998,\n",
       "  8.80360415338734,\n",
       "  8.802425288859709,\n",
       "  8.811089030754419,\n",
       "  8.699448058266698,\n",
       "  8.729426119798495,\n",
       "  8.689682143705863,\n",
       "  8.686881346651065,\n",
       "  8.674425933832003,\n",
       "  8.657020394816811,\n",
       "  8.56705690368458],\n",
       " 'geno_val_losses': [1.627972967830705,\n",
       "  1.627446245264124,\n",
       "  1.6273978021409776,\n",
       "  1.631415037461269,\n",
       "  1.6258088453316395,\n",
       "  1.6290773962750846,\n",
       "  1.6257476011912029,\n",
       "  1.6239319818991202,\n",
       "  1.6285125708874362,\n",
       "  1.6302716790893932,\n",
       "  1.6266148237534512,\n",
       "  1.6207647147002044,\n",
       "  1.6211781825548337,\n",
       "  1.615507396650903,\n",
       "  1.6164117565861456,\n",
       "  1.6168776382634669,\n",
       "  1.6171627809971938,\n",
       "  1.62274416582084,\n",
       "  1.6186590489046073,\n",
       "  1.6163957472200747,\n",
       "  1.6151356020091492,\n",
       "  1.617782316090148,\n",
       "  1.6153472853295596,\n",
       "  1.6176119733739782,\n",
       "  1.6174586054719524,\n",
       "  1.6135979493459065,\n",
       "  1.615301246996279,\n",
       "  1.6148669955171184],\n",
       " 'ab_val_losses': [2.7584146452538763,\n",
       "  2.5342675344443615,\n",
       "  2.4425255339822654,\n",
       "  2.347772424603686,\n",
       "  2.3297361032462414,\n",
       "  2.2939229835698636,\n",
       "  2.2627361232851757,\n",
       "  2.2551438837875555,\n",
       "  2.263362678480737,\n",
       "  2.2230048415101606,\n",
       "  2.225905041635772,\n",
       "  2.210646747071066,\n",
       "  2.2067349604618403,\n",
       "  2.1846818835647017,\n",
       "  2.1561573935143743,\n",
       "  2.16202978145929,\n",
       "  2.1296450120431407,\n",
       "  2.1414401207441167,\n",
       "  2.135686088491369,\n",
       "  2.1207497708591414,\n",
       "  2.126502178333424,\n",
       "  2.1330612765418158,\n",
       "  2.108253626175869,\n",
       "  2.146448338473285,\n",
       "  2.134442135139748,\n",
       "  2.1147105899857888,\n",
       "  2.1255854265189464,\n",
       "  2.151262624764148],\n",
       " 'val_accs': [0.2576535650799921,\n",
       "  0.24753110803871223,\n",
       "  0.25740667588386335,\n",
       "  0.23010073079202054,\n",
       "  0.24042069919020342,\n",
       "  0.23074264270195535,\n",
       "  0.23143393245111593,\n",
       "  0.23153268812956745,\n",
       "  0.23928500888801105,\n",
       "  0.23074264270195535,\n",
       "  0.23232273355717953,\n",
       "  0.23419909144775825,\n",
       "  0.24259332411613668,\n",
       "  0.23163144380801895,\n",
       "  0.2261505036539601,\n",
       "  0.23671736124827178,\n",
       "  0.22180525380209362,\n",
       "  0.23296464546711437,\n",
       "  0.2277305945091843,\n",
       "  0.22234841003357694,\n",
       "  0.2259529922970571,\n",
       "  0.2305945091842781,\n",
       "  0.23074264270195535,\n",
       "  0.23232273355717953,\n",
       "  0.23128579893343867,\n",
       "  0.2212620975706103,\n",
       "  0.22901441832905392,\n",
       "  0.2346928698400158]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "bert_test = BERT_ft(len(vocabulary_geno), max_length, dim_emb, dim_hidden, attention_heads, num_encoders, drop_prob, len(vocabulary_pheno), device)\n",
    "\n",
    "test = BertTrainer_ft(bert_test, train_set, val_set, 30, batch_size, lr, device, stop_patience,  True, \"Trial_runs\", \"MaxSamples_2\")\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "AB_idx = torch.tensor([49, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                       -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
    "\n",
    "SR_class = torch.tensor([1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                         -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
    "\n",
    "# Create a new tensor filled with -1 values\n",
    "result_tensor = torch.full_like(AB_idx, -1)\n",
    "\n",
    "# Iterate over AB_idx and SR_class to place the values at correct indices\n",
    "for idx, value in enumerate(AB_idx):\n",
    "    if value != -1:\n",
    "        result_tensor[value] = SR_class[idx]\n",
    "\n",
    "print(result_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
