{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from itertools import chain \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lokalt\n",
    "data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saga\n",
    "data_dir = \"/home/aeerik/data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>location</th>\n",
       "      <th>genes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>USA</td>\n",
       "      <td>[sul1, tet(A), aadA1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>USA</td>\n",
       "      <td>[sul2, aph(3'')-Ib, tet(A), aph(6)-Id]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT, pmrB_Y358N=POINT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>[uhpT_E350Q=POINT, cyaA_S352T=POINT, glpT_E448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1979</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year location                                              genes\n",
       "1  [PAD]      USA                              [sul1, tet(A), aadA1]\n",
       "4  [PAD]      USA             [sul2, aph(3'')-Ib, tet(A), aph(6)-Id]\n",
       "5  [PAD]      USA               [glpT_E448K=POINT, pmrB_Y358N=POINT]\n",
       "6  [PAD]   Sweden  [uhpT_E350Q=POINT, cyaA_S352T=POINT, glpT_E448...\n",
       "7   1979      USA                                 [glpT_E448K=POINT]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_preprocessing import data_loader\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "NCBI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1208\n"
     ]
    }
   ],
   "source": [
    "from build_vocabulary import make_vocabulary\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import NCBIDataset\n",
    "from build_vocabulary import make_vocabulary\n",
    "from data_preprocessing import data_loader\n",
    "\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "\n",
    "max_length = 20\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class JointEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, vocab_size, max_length, drop_prob):\n",
    "        super(JointEmbedding, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.drop_prob = drop_prob\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.token_emb = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\t\n",
    "        self.norm = nn.LayerNorm(self.embedding_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        token_embedding = self.token_emb(input_tensor)\n",
    "        token_embedding = self.norm(token_embedding)\n",
    "        token_embedding = self.dropout(token_embedding)\n",
    "\n",
    "        return token_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000, -0.0000,  0.3553, -1.1371,  0.3805,  1.6536, -1.9498,\n",
       "          0.1008,  0.9090,  1.2419, -1.7474,  1.6039,  2.3396, -0.1568,  0.0000,\n",
       "          2.4389, -0.3773, -1.4833,  0.0000, -0.0000,  0.2353, -0.0151,  0.0000,\n",
       "         -2.0182,  0.9652,  0.2159, -0.2871,  0.1594,  0.3802,  0.0000, -0.8101],\n",
       "        [-1.2321,  1.8984,  0.7912,  2.1833, -0.8161,  0.0000, -0.4657, -0.6055,\n",
       "          2.2380,  1.0590,  0.1244,  1.4381, -0.6233,  0.5699, -1.3460, -0.9220,\n",
       "         -0.3099, -0.2273,  1.8147, -0.0000,  1.1527, -0.7644,  0.2706, -0.2831,\n",
       "         -0.3776, -1.7533, -0.7721, -1.4761, -0.0000, -1.4792, -1.6105,  2.2412],\n",
       "        [-0.0000, -0.5733,  2.6986, -0.6970,  0.4722, -0.6928,  1.1357, -1.1755,\n",
       "         -0.8313,  0.2363, -0.4323,  0.3252, -3.1352, -2.6159, -0.6505, -0.0469,\n",
       "         -0.2779,  0.6093,  0.0724, -0.6377,  1.7117,  1.2104,  0.4811,  0.0000,\n",
       "          0.7456, -1.2218,  1.3354,  0.0000, -0.0000,  1.9296, -0.5335,  0.2506],\n",
       "        [ 0.0483, -2.1863, -0.1984, -0.0000,  2.5142,  0.4226, -0.4212, -1.4209,\n",
       "          0.5142,  0.0000,  1.3695, -1.0216,  0.0000, -0.0000, -0.0782, -1.3607,\n",
       "         -1.6200, -1.3455, -1.8450,  0.0403, -0.9688,  0.2238,  0.0000,  0.0000,\n",
       "          0.0000,  1.1369, -0.1612,  1.3033, -0.2404, -0.0000, -0.3699,  2.1908],\n",
       "        [ 0.0483, -2.1863, -0.0000, -0.0600,  2.5142,  0.0000, -0.4212, -1.4209,\n",
       "          0.5142,  1.0953,  1.3695, -1.0216,  1.2902, -0.0000, -0.0782, -1.3607,\n",
       "         -1.6200, -1.3455, -1.8450,  0.0403, -0.9688,  0.2238,  0.0000,  2.0444,\n",
       "          0.2472,  1.1369, -0.0000,  1.3033, -0.0000, -0.6661, -0.3699,  2.1908],\n",
       "        [-0.4572, -0.8608,  0.6184,  0.1133, -0.0288,  0.0444,  0.0000, -1.5800,\n",
       "          1.5961, -0.9108, -1.5981, -2.8929, -0.5853, -0.5235,  0.0754,  2.0375,\n",
       "          1.2000,  0.0000, -1.5157,  1.0948,  1.2391, -0.8109,  0.6651,  0.3519,\n",
       "          0.4310, -0.6082,  0.6509, -1.6977,  0.6373,  1.5686, -1.8282, -0.1870],\n",
       "        [ 0.0483, -2.1863, -0.1984, -0.0600,  2.5142,  0.4226, -0.0000, -0.0000,\n",
       "          0.5142,  1.0953,  1.3695, -1.0216,  1.2902, -2.1004, -0.0000, -1.3607,\n",
       "         -1.6200, -1.3455, -1.8450,  0.0403, -0.0000,  0.0000,  1.6236,  2.0444,\n",
       "          0.2472,  1.1369, -0.1612,  1.3033, -0.2404, -0.6661, -0.3699,  0.0000],\n",
       "        [-1.2321,  0.0000,  0.7912,  2.1833, -0.8161,  1.2926, -0.4657, -0.6055,\n",
       "          2.2380,  1.0590,  0.1244,  1.4381, -0.6233,  0.0000, -1.3460, -0.9220,\n",
       "         -0.3099, -0.2273,  1.8147, -0.0964,  1.1527, -0.7644,  0.2706, -0.0000,\n",
       "         -0.3776, -1.7533, -0.0000, -1.4761, -1.9138, -1.4792, -1.6105,  2.2412],\n",
       "        [-0.0000,  1.8984,  0.0000,  2.1833, -0.8161,  1.2926, -0.4657, -0.6055,\n",
       "          2.2380,  1.0590,  0.1244,  1.4381, -0.6233,  0.5699, -1.3460, -0.0000,\n",
       "         -0.3099, -0.2273,  1.8147, -0.0964,  1.1527, -0.7644,  0.2706, -0.2831,\n",
       "         -0.0000, -0.0000, -0.7721, -1.4761, -1.9138, -0.0000, -1.6105,  2.2412],\n",
       "        [-1.2321,  1.8984,  0.0000,  2.1833, -0.8161,  1.2926, -0.4657, -0.6055,\n",
       "          2.2380,  1.0590,  0.0000,  0.0000, -0.0000,  0.5699, -0.0000, -0.9220,\n",
       "         -0.0000, -0.2273,  1.8147, -0.0964,  1.1527, -0.7644,  0.2706, -0.2831,\n",
       "         -0.3776, -1.7533, -0.7721, -1.4761, -1.9138, -1.4792, -1.6105,  2.2412],\n",
       "        [-1.2321,  1.8984,  0.7912,  2.1833, -0.8161,  1.2926, -0.0000, -0.6055,\n",
       "          0.0000,  1.0590,  0.1244,  1.4381, -0.6233,  0.5699, -1.3460, -0.0000,\n",
       "         -0.3099, -0.2273,  0.0000, -0.0000,  1.1527, -0.7644,  0.2706, -0.2831,\n",
       "         -0.3776, -1.7533, -0.7721, -1.4761, -1.9138, -1.4792, -1.6105,  2.2412],\n",
       "        [-1.2321,  1.8984,  0.7912,  2.1833, -0.8161,  0.0000, -0.4657, -0.6055,\n",
       "          2.2380,  1.0590,  0.0000,  1.4381, -0.0000,  0.0000, -0.0000, -0.9220,\n",
       "         -0.3099, -0.2273,  1.8147, -0.0000,  0.0000, -0.7644,  0.0000, -0.2831,\n",
       "         -0.3776, -1.7533, -0.0000, -1.4761, -0.0000, -1.4792, -1.6105,  2.2412],\n",
       "        [-1.2321,  0.0000,  0.7912,  2.1833, -0.0000,  1.2926, -0.4657, -0.6055,\n",
       "          2.2380,  1.0590,  0.1244,  1.4381, -0.6233,  0.5699, -1.3460, -0.9220,\n",
       "         -0.3099, -0.2273,  1.8147, -0.0964,  0.0000, -0.7644,  0.0000, -0.2831,\n",
       "         -0.0000, -0.0000, -0.7721, -1.4761, -1.9138, -0.0000, -1.6105,  2.2412],\n",
       "        [-1.2321,  1.8984,  0.7912,  0.0000, -0.8161,  0.0000, -0.4657, -0.6055,\n",
       "          2.2380,  0.0000,  0.0000,  1.4381, -0.6233,  0.5699, -1.3460, -0.9220,\n",
       "         -0.3099, -0.2273,  1.8147, -0.0000,  1.1527, -0.7644,  0.0000, -0.2831,\n",
       "         -0.3776, -1.7533, -0.7721, -0.0000, -1.9138, -1.4792, -0.0000,  2.2412],\n",
       "        [-1.2321,  1.8984,  0.7912,  2.1833, -0.8161,  1.2926, -0.0000, -0.6055,\n",
       "          2.2380,  1.0590,  0.1244,  1.4381, -0.6233,  0.5699, -1.3460, -0.0000,\n",
       "         -0.3099, -0.2273,  1.8147, -0.0000,  1.1527, -0.7644,  0.2706, -0.2831,\n",
       "         -0.3776, -1.7533, -0.0000, -1.4761, -1.9138, -1.4792, -1.6105,  2.2412],\n",
       "        [-1.2321,  1.8984,  0.7912,  2.1833, -0.8161,  1.2926, -0.0000, -0.0000,\n",
       "          0.0000,  1.0590,  0.1244,  1.4381, -0.6233,  0.0000, -0.0000, -0.9220,\n",
       "         -0.0000, -0.2273,  1.8147, -0.0964,  1.1527, -0.7644,  0.0000, -0.2831,\n",
       "         -0.3776, -1.7533, -0.7721, -1.4761, -0.0000, -1.4792, -1.6105,  2.2412],\n",
       "        [-1.2321,  1.8984,  0.7912,  2.1833, -0.8161,  0.0000, -0.4657, -0.6055,\n",
       "          0.0000,  1.0590,  0.1244,  1.4381, -0.6233,  0.5699, -1.3460, -0.0000,\n",
       "         -0.3099, -0.2273,  1.8147, -0.0000,  0.0000, -0.7644,  0.2706, -0.2831,\n",
       "         -0.3776, -1.7533, -0.7721, -1.4761, -0.0000, -1.4792, -0.0000,  2.2412],\n",
       "        [-0.0000,  0.0000,  0.7912,  2.1833, -0.0000,  1.2926, -0.4657, -0.6055,\n",
       "          2.2380,  1.0590,  0.1244,  1.4381, -0.6233,  0.5699, -0.0000, -0.9220,\n",
       "         -0.3099, -0.2273,  1.8147, -0.0000,  1.1527, -0.7644,  0.2706, -0.2831,\n",
       "         -0.3776, -1.7533, -0.7721, -1.4761, -1.9138, -1.4792, -1.6105,  2.2412],\n",
       "        [-1.2321,  1.8984,  0.7912,  2.1833, -0.8161,  0.0000, -0.4657, -0.6055,\n",
       "          2.2380,  1.0590,  0.1244,  0.0000, -0.6233,  0.0000, -0.0000, -0.9220,\n",
       "         -0.3099, -0.2273,  1.8147, -0.0964,  0.0000, -0.0000,  0.0000, -0.2831,\n",
       "         -0.3776, -0.0000, -0.7721, -1.4761, -1.9138, -1.4792, -0.0000,  2.2412],\n",
       "        [-0.0000,  0.0000,  0.7912,  2.1833, -0.8161,  1.2926, -0.4657, -0.0000,\n",
       "          2.2380,  1.0590,  0.0000,  1.4381, -0.6233,  0.5699, -1.3460, -0.9220,\n",
       "         -0.0000, -0.2273,  1.8147, -0.0964,  1.1527, -0.7644,  0.2706, -0.2831,\n",
       "         -0.3776, -1.7533, -0.7721, -1.4761, -1.9138, -1.4792, -1.6105,  2.2412]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embedding import JointEmbedding\n",
    "embedding_dim = 32\n",
    "voca_size = len(vocabulary)\n",
    "max_length = 20\n",
    "drop_prob = 0.2\n",
    "\n",
    "emb_test = JointEmbedding(embedding_dim, voca_size, max_length, drop_prob)\n",
    "emb_test.forward(test_set[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,  59, 213, 214, 215,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.8843e-01, -1.3420e+00,  2.9944e-01, -4.1503e-01, -1.6473e+00,\n",
       "         -9.0050e-01, -2.0309e+00,  2.2498e-01, -1.7748e-01, -1.4642e+00,\n",
       "          1.2188e+00,  1.7456e-01, -2.1747e+00,  1.4280e+00,  4.9190e-01,\n",
       "          4.7069e-01, -3.4982e-01, -1.4850e-01, -6.1501e-01, -9.6792e-02,\n",
       "          1.4723e+00,  1.0816e+00, -2.4056e-03,  3.5799e-01,  4.8171e-01,\n",
       "          3.6956e-01,  1.4174e-01, -1.3759e-01,  1.5727e+00, -3.5759e-01,\n",
       "          1.4613e+00,  1.3010e+00],\n",
       "        [ 6.8182e-02, -1.2260e+00,  8.5642e-03, -5.8040e-01, -1.4073e+00,\n",
       "         -7.4120e-01, -1.6831e+00, -5.5534e-01, -1.5944e+00, -1.3240e+00,\n",
       "          4.4706e-01,  1.5174e-01, -1.2163e+00,  7.2857e-01,  5.9617e-01,\n",
       "          9.6149e-01, -1.0766e-01,  7.0815e-01, -1.5241e+00, -8.5263e-01,\n",
       "          2.4098e+00,  1.4359e+00,  6.4953e-02,  7.3426e-01,  3.8040e-01,\n",
       "          5.1044e-01,  9.2362e-02, -5.4984e-02,  1.4878e+00,  9.2974e-02,\n",
       "          7.9555e-01,  1.1931e+00],\n",
       "        [-5.3420e-01, -3.1101e-01,  9.1448e-01, -6.2579e-01, -1.1191e+00,\n",
       "         -4.9233e-01, -1.1976e+00, -1.2692e+00, -1.2329e+00, -1.5800e+00,\n",
       "          4.2753e-01, -1.2433e+00, -1.0946e+00,  2.1646e+00,  8.0172e-01,\n",
       "          5.9556e-01, -2.0099e-01, -7.8257e-01, -7.5365e-01,  3.9663e-02,\n",
       "          1.7988e+00,  1.8840e+00,  4.2598e-01,  5.5283e-01,  8.3122e-01,\n",
       "         -3.2931e-01,  3.1639e-01, -4.7608e-02,  2.8262e-01, -6.7683e-01,\n",
       "          8.6305e-01,  1.5926e+00],\n",
       "        [-1.2007e+00, -8.4010e-01,  3.0387e-01, -9.5293e-01, -2.0057e+00,\n",
       "         -4.7693e-01, -1.5115e+00, -4.4640e-01, -6.6563e-01, -1.0723e+00,\n",
       "          1.0770e+00, -4.9251e-01, -1.0766e+00,  1.3440e+00,  1.4551e-01,\n",
       "          8.3548e-01, -2.6660e-01, -3.4467e-02, -8.6457e-01, -4.1080e-01,\n",
       "          2.0381e+00,  1.8106e+00, -4.3715e-03,  2.4600e-01,  2.7536e-01,\n",
       "          6.0609e-01, -5.2084e-02,  2.9218e-02,  1.3008e+00, -6.1530e-01,\n",
       "          1.4573e+00,  1.5202e+00],\n",
       "        [-3.2441e-01, -7.7901e-01,  1.1443e+00, -2.0755e-01, -1.7596e+00,\n",
       "         -8.9729e-01, -1.6668e+00, -3.0993e-01, -2.1500e-01, -1.4980e+00,\n",
       "          3.4088e-01, -1.0213e+00, -1.4125e+00,  2.1884e+00,  5.0647e-01,\n",
       "          8.4540e-01, -2.0982e-01, -5.7407e-01, -6.0239e-01, -2.5921e-01,\n",
       "          2.5087e+00,  1.1859e+00,  2.3954e-01, -2.5791e-01,  4.6298e-01,\n",
       "          3.8062e-02, -3.3557e-02, -2.9453e-01,  1.0190e+00, -5.3901e-02,\n",
       "          1.2472e+00,  6.4981e-01],\n",
       "        [ 1.6253e-01, -9.9426e-01,  6.8030e-01, -1.1846e-01, -2.2703e+00,\n",
       "         -9.9417e-01, -1.6193e+00, -1.6625e-01, -6.3343e-01, -1.0749e+00,\n",
       "         -5.2164e-01, -4.8366e-01, -1.1108e+00,  2.0727e+00,  5.9019e-01,\n",
       "          1.2859e+00, -2.0288e-01, -8.6923e-01, -7.1075e-01, -3.5231e-01,\n",
       "          1.5392e+00,  1.6677e+00,  8.9875e-01,  5.1092e-01,  7.3435e-01,\n",
       "         -3.8484e-01, -4.0536e-02, -9.2659e-02,  6.7944e-01, -1.6933e-01,\n",
       "          1.8022e+00,  1.8552e-01],\n",
       "        [-1.3460e+00, -9.6211e-01,  1.1607e+00, -2.7760e-01, -2.1811e+00,\n",
       "         -9.9799e-01, -1.1682e+00, -5.0298e-01, -8.5629e-01, -1.2085e+00,\n",
       "          7.6061e-01,  1.5670e-01, -1.1960e+00,  1.6886e+00,  1.6893e-01,\n",
       "          1.0727e+00, -5.7700e-01, -3.3952e-01, -2.0947e-01,  2.1007e-01,\n",
       "          2.2883e+00,  9.9522e-01, -3.6477e-01,  4.3603e-01,  6.7435e-02,\n",
       "          6.7622e-01, -1.2237e-03, -6.7134e-01,  1.2922e+00, -2.6577e-01,\n",
       "          1.3489e+00,  8.0319e-01],\n",
       "        [ 3.3566e-01, -8.5052e-01,  1.1844e+00,  6.6648e-01, -7.5167e-01,\n",
       "         -1.1545e+00, -1.3600e+00, -1.2538e+00, -5.4545e-01, -1.9254e+00,\n",
       "          6.1519e-02, -1.6116e+00, -1.8109e+00,  1.2679e+00,  1.0188e+00,\n",
       "          1.7663e+00,  5.8913e-02, -1.8197e-01, -8.2718e-01, -7.1302e-01,\n",
       "          1.6041e+00,  1.0781e+00,  2.2048e-01,  8.5607e-02,  8.0841e-01,\n",
       "         -3.9627e-01,  6.8658e-01,  5.6943e-01,  2.2017e-01, -4.8462e-02,\n",
       "          7.4329e-01,  1.0545e+00],\n",
       "        [-7.2696e-01, -3.0789e-01,  9.0062e-01, -1.2958e-01, -1.4676e+00,\n",
       "         -9.4089e-01, -1.5244e+00, -1.2881e+00, -1.0081e+00, -1.4739e+00,\n",
       "          1.1720e+00, -6.9110e-01, -1.3206e+00,  1.0163e+00,  4.2435e-02,\n",
       "          1.3200e+00,  4.0309e-02, -2.3880e-01, -1.1981e+00, -5.3951e-01,\n",
       "          1.9246e+00,  1.6986e+00,  9.1625e-02, -2.2523e-01,  4.3622e-01,\n",
       "          3.5086e-01,  3.9978e-01,  5.3456e-01,  1.1516e+00, -3.3366e-01,\n",
       "          6.1809e-01,  1.7168e+00],\n",
       "        [-4.5920e-01, -1.2634e+00, -1.0293e-02, -7.4436e-01, -1.7099e+00,\n",
       "         -6.1053e-01, -2.0208e+00, -1.0112e-01, -1.8712e-01, -1.4247e+00,\n",
       "          1.1140e+00,  3.5608e-01, -1.8177e+00,  9.0978e-01,  1.0711e+00,\n",
       "          6.7827e-01, -8.1939e-02,  3.0935e-01, -9.0420e-01, -5.1531e-01,\n",
       "          1.6689e+00,  1.6525e+00, -3.5999e-01,  1.1101e+00,  4.3919e-01,\n",
       "         -1.9933e-01,  8.0106e-02, -2.4693e-01,  1.0547e+00, -3.7484e-01,\n",
       "          1.0626e+00,  1.5248e+00],\n",
       "        [-3.4159e-01, -6.8517e-01,  1.2252e-01, -6.3748e-01, -1.9588e+00,\n",
       "         -7.5065e-01, -1.8470e+00, -9.7772e-01, -2.3029e-01, -1.1239e+00,\n",
       "          1.0947e+00,  3.0255e-01, -1.3404e+00,  1.3611e+00, -2.5680e-01,\n",
       "          5.1106e-01, -6.8965e-01, -3.4634e-02, -5.3267e-01, -7.3708e-02,\n",
       "          1.6412e+00,  1.6915e+00,  4.6523e-01,  6.9097e-02,  1.5031e-01,\n",
       "          5.2743e-01, -1.9460e-02, -1.7571e-01,  1.0046e+00, -8.1173e-01,\n",
       "          2.4077e+00,  1.1383e+00],\n",
       "        [ 4.7613e-02, -1.0096e+00,  5.6599e-01, -7.3831e-01, -1.5680e+00,\n",
       "         -8.2754e-01, -1.8442e+00, -8.8409e-01, -7.4965e-01, -1.7207e+00,\n",
       "          6.3485e-01,  1.2463e-01, -1.4767e+00,  1.3699e+00,  4.7147e-01,\n",
       "          1.2082e+00, -2.1184e-02,  3.2301e-01, -9.3663e-01, -5.2041e-01,\n",
       "          2.3552e+00,  1.6077e+00, -1.2617e-01,  5.8375e-01,  4.1495e-01,\n",
       "          6.9562e-02,  5.5472e-01, -2.8518e-01,  8.9933e-01, -3.3636e-01,\n",
       "          5.6929e-01,  1.2444e+00],\n",
       "        [-1.0070e+00, -9.4007e-01,  5.1996e-01, -4.4119e-01, -1.7553e+00,\n",
       "         -5.9754e-01, -1.8732e+00, -8.1081e-01, -4.9952e-01, -1.5284e+00,\n",
       "          9.3446e-01, -1.2568e-01, -1.7469e+00,  1.3168e+00,  5.1753e-01,\n",
       "          1.0957e+00, -1.6559e-01, -5.6328e-01, -8.1886e-01, -8.8633e-02,\n",
       "          1.4312e+00,  1.8348e+00,  1.0644e-01,  2.8045e-01,  7.8605e-01,\n",
       "          5.8350e-02,  6.1447e-01,  2.1599e-01,  9.9645e-01, -4.1377e-01,\n",
       "          1.2807e+00,  1.3862e+00],\n",
       "        [-9.9876e-01, -8.9715e-01,  3.5594e-01, -6.7086e-01, -1.1762e+00,\n",
       "         -2.1449e-01, -1.9244e+00, -1.0333e+00,  9.5527e-02, -1.3780e+00,\n",
       "          1.4361e+00,  9.2321e-02, -1.9533e+00,  1.4501e+00,  1.0524e-01,\n",
       "          8.9538e-01, -2.3833e-01, -5.6230e-02, -1.0308e+00, -7.2001e-01,\n",
       "          1.3194e+00,  1.8931e+00, -8.5504e-02, -7.3290e-02,  6.0088e-01,\n",
       "         -1.3957e-02,  1.1433e+00,  1.9421e-01,  6.6317e-01, -3.9438e-01,\n",
       "          1.2424e+00,  1.3718e+00],\n",
       "        [-1.7480e-01, -1.6884e+00,  1.5623e-01, -6.6057e-01, -1.7213e+00,\n",
       "         -7.4195e-01, -1.5197e+00, -6.1073e-01, -7.7786e-01, -1.5405e+00,\n",
       "          4.3971e-01,  6.8858e-02, -1.9653e+00,  1.6729e+00,  5.9596e-01,\n",
       "          1.2959e+00, -5.6055e-02,  2.2097e-01, -7.9524e-01, -5.8288e-01,\n",
       "          1.5428e+00,  1.6447e+00,  2.8022e-01,  8.6898e-01,  6.1207e-01,\n",
       "          4.3316e-01,  5.5712e-01, -2.9711e-01,  3.9795e-01,  1.3937e-01,\n",
       "          1.2902e+00,  9.1543e-01],\n",
       "        [ 8.7450e-02, -9.1037e-01,  5.0730e-01, -2.0974e-01, -1.8259e+00,\n",
       "         -8.9749e-01, -1.8848e+00, -9.0795e-01, -9.5937e-01, -1.7226e+00,\n",
       "         -1.7711e-02, -8.0577e-01, -1.4751e+00,  1.1723e+00,  8.3886e-01,\n",
       "          1.1719e+00, -4.2941e-01, -4.3341e-01, -6.1656e-01, -4.3172e-02,\n",
       "          1.9292e+00,  1.4999e+00,  3.6475e-01,  8.1462e-01,  1.2268e+00,\n",
       "          1.5110e-01,  3.4299e-01,  2.6089e-01,  8.5997e-01, -1.6261e-01,\n",
       "          1.1465e+00,  9.2754e-01],\n",
       "        [ 3.1481e-01, -1.0736e+00,  5.9075e-01, -6.5734e-01, -1.9499e+00,\n",
       "         -1.1581e+00, -1.9131e+00, -2.6230e-01, -2.5413e-01, -1.8044e+00,\n",
       "          2.5697e-01,  5.9278e-01, -1.2488e+00,  1.7802e+00,  6.6310e-01,\n",
       "          8.9300e-01, -3.8539e-01, -2.3479e-01, -2.5000e-01, -1.2836e-01,\n",
       "          2.3010e+00,  8.3649e-01, -4.6809e-02,  3.8130e-01,  7.9078e-01,\n",
       "          1.2932e-01,  1.4121e-01, -5.0189e-01,  1.2289e+00, -8.3528e-01,\n",
       "          1.2285e+00,  5.7509e-01],\n",
       "        [ 6.3756e-01, -1.0052e+00,  1.0972e-01, -7.9456e-01, -1.1727e+00,\n",
       "         -8.4205e-01, -2.0532e+00, -1.1750e+00, -6.1088e-01, -1.5970e+00,\n",
       "          7.4929e-01,  4.8731e-01, -1.5590e+00,  1.5016e+00,  3.8474e-02,\n",
       "          1.3195e+00,  2.2680e-03,  1.2676e-01, -9.3601e-01, -6.1015e-01,\n",
       "          2.1815e+00,  1.7347e+00, -2.0991e-02, -1.3015e-01,  5.5324e-01,\n",
       "          1.9667e-01,  5.8794e-01, -2.0858e-01,  1.0223e+00, -1.0638e-01,\n",
       "          6.7257e-01,  9.0053e-01],\n",
       "        [-2.5724e-01, -8.6494e-01,  2.8239e-01, -3.6647e-01, -1.7244e+00,\n",
       "         -1.2537e+00, -1.7131e+00, -7.2528e-01, -5.3892e-01, -1.4454e+00,\n",
       "          4.3015e-01,  1.9394e-01, -1.6646e+00,  1.7466e+00,  9.5477e-01,\n",
       "          7.0297e-01, -4.0787e-01, -3.8729e-01, -3.7513e-01, -2.2056e-01,\n",
       "          1.8332e+00,  1.5319e+00,  3.4635e-01,  1.6232e-01,  9.8302e-01,\n",
       "         -1.6256e-01, -5.9262e-02, -1.4060e-01,  1.3304e+00, -7.7068e-01,\n",
       "          1.4654e+00,  1.1146e+00],\n",
       "        [-2.3180e-01, -8.3932e-01,  1.5316e-01, -2.8397e-02, -1.5117e+00,\n",
       "         -5.5884e-01, -1.8519e+00, -9.4614e-01, -6.1517e-02, -1.3433e+00,\n",
       "          1.0965e+00, -4.1095e-01, -2.1505e+00,  1.2744e+00,  7.1879e-01,\n",
       "          1.1575e+00,  1.0470e-02, -6.0925e-01, -9.2076e-01, -8.6001e-01,\n",
       "          1.1840e+00,  1.8504e+00,  4.7327e-01,  1.5831e-01,  6.8421e-01,\n",
       "         -4.0986e-01,  3.7704e-01,  2.5706e-01,  5.8304e-01, -2.9590e-01,\n",
       "          1.3820e+00,  1.6699e+00]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out, drop_prob):\n",
    "        super(AttentionHead, self).__init__()\n",
    "\n",
    "        self.dim_inp = dim_inp\n",
    "        self.drop_prob = drop_prob\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.q = nn.Linear(dim_inp, dim_out)\n",
    "        self.k = nn.Linear(dim_inp, dim_out)\n",
    "        self.v = nn.Linear(dim_inp, dim_out)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)\n",
    "\n",
    "        scale = query.size(1) ** 0.5\n",
    "        scores = torch.matmul(query, key.transpose(-1, -2)) / scale\n",
    "\n",
    "        scores = scores.masked_fill_(attention_mask, -1e9)\n",
    "        attn = f.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, value)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, dim_inp, dim_out,drop_prob):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(dim_inp, dim_out,drop_prob) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(dim_out * num_heads, dim_inp)\n",
    "        self.norm = nn.LayerNorm(dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        s = [head(input_tensor, attention_mask) for head in self.heads]\n",
    "        scores = torch.cat(s, dim=-1)\n",
    "        scores = self.linear(scores)\n",
    "        return self.norm(scores)\n",
    "    \n",
    "attention_test = AttentionHead(32, 32, 0.2)\n",
    "attention_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])\n",
    "\n",
    "multihead_test = MultiHeadAttention(8, 32, 32,0.2)\n",
    "multihead_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7308e-02,  1.2452e-01, -6.6884e-03,  5.5833e-02,  1.2116e-01,\n",
       "         -1.9990e-02,  8.4978e-01, -1.6624e-01, -3.9880e-01,  2.6900e-01,\n",
       "          2.3491e-01, -6.2202e-02, -6.1828e-01,  4.7934e-01,  2.3262e-02,\n",
       "         -3.9426e-01,  8.2099e-01, -2.4035e-01, -5.6881e-02, -8.0375e-01,\n",
       "          7.5708e-01, -5.1566e-01, -2.3887e-01, -1.1806e+00,  6.1366e-01,\n",
       "          5.7547e-01,  2.9847e-01, -2.2838e-01, -7.8742e-01, -3.8740e-01,\n",
       "          5.2382e-01, -7.7262e-01],\n",
       "        [ 9.2858e-02, -7.6474e-02,  8.5543e-02,  5.2141e-02,  2.9463e-01,\n",
       "         -1.1200e-01,  3.4645e-01,  1.0705e-01,  1.1619e-01,  1.1212e-01,\n",
       "          5.6340e-01,  2.1974e-01, -5.4030e-01,  6.6198e-01, -1.4842e-02,\n",
       "         -6.3676e-02,  5.5643e-01, -2.7908e-01, -1.3047e-01, -6.1584e-01,\n",
       "          3.9355e-01, -2.7492e-01, -2.3767e-01, -5.9836e-01,  4.4019e-01,\n",
       "          4.3393e-01,  2.5858e-01,  5.8096e-02, -3.7804e-01, -1.3082e-01,\n",
       "          2.9361e-01, -5.0309e-01],\n",
       "        [-2.4001e-02,  2.0574e-01, -5.1683e-02,  1.2036e-01, -7.1270e-02,\n",
       "          1.2636e-02,  7.7694e-01, -4.1153e-01, -4.7838e-01,  3.3182e-01,\n",
       "          1.3167e-01, -2.4725e-01, -6.2504e-01,  4.8090e-01,  1.1431e-01,\n",
       "         -3.2768e-01,  6.5651e-01, -1.1754e-01, -6.8457e-02, -5.9123e-01,\n",
       "          6.2971e-01, -5.6020e-01, -2.4978e-01, -8.5320e-01,  4.8104e-01,\n",
       "          5.4221e-01,  2.0567e-01, -5.2234e-01, -7.3440e-01, -2.8546e-01,\n",
       "          6.3449e-01, -5.3740e-01],\n",
       "        [ 4.4169e-02,  1.1977e-01,  2.0790e-02,  1.2544e-01,  8.7403e-02,\n",
       "         -5.3465e-02,  7.3250e-01, -2.7207e-01, -3.0519e-01,  3.1280e-01,\n",
       "          3.8576e-01, -7.8035e-02, -7.6302e-01,  7.1435e-01,  7.7536e-02,\n",
       "         -2.7103e-01,  7.5841e-01, -2.1770e-01, -1.2759e-01, -7.6092e-01,\n",
       "          6.7379e-01, -5.5765e-01, -3.3260e-01, -9.1361e-01,  5.8953e-01,\n",
       "          6.2107e-01,  2.8825e-01, -3.9360e-01, -7.3327e-01, -2.6139e-01,\n",
       "          6.4039e-01, -6.3331e-01],\n",
       "        [ 2.3131e-01,  2.0638e-01,  2.5453e-01, -4.7305e-02,  3.5778e-02,\n",
       "          9.3718e-03,  1.6934e-01, -2.9936e-02, -4.0490e-01,  9.7364e-02,\n",
       "         -9.6622e-03,  6.5696e-02, -4.4185e-01,  2.4919e-01, -6.4009e-02,\n",
       "         -2.9345e-01,  3.4635e-01, -5.4926e-02, -4.7234e-02, -5.3706e-01,\n",
       "          5.5690e-01, -2.0949e-01, -2.6181e-01, -6.7480e-01,  5.8023e-01,\n",
       "          1.9269e-01,  2.1294e-01, -2.2245e-01, -4.3202e-01, -2.5053e-01,\n",
       "          2.5303e-01, -3.9330e-01],\n",
       "        [ 1.8852e-01,  2.9316e-01,  1.9866e-01,  1.0915e-01, -5.1317e-02,\n",
       "          4.0655e-02,  4.0191e-01, -3.0537e-01, -5.4619e-01,  3.0674e-01,\n",
       "         -4.2827e-03, -1.3498e-01, -6.7374e-01,  4.9157e-01, -1.7104e-03,\n",
       "         -2.5482e-01,  4.6762e-01, -1.9355e-02, -9.4489e-02, -6.1372e-01,\n",
       "          6.4284e-01, -3.9200e-01, -3.9487e-01, -6.6545e-01,  6.2915e-01,\n",
       "          3.8475e-01,  1.9727e-01, -5.8410e-01, -6.5859e-01, -2.1574e-01,\n",
       "          5.5810e-01, -3.0498e-01],\n",
       "        [ 3.0186e-01,  2.8376e-01,  3.3998e-01,  7.2077e-02,  1.5185e-01,\n",
       "          1.1902e-02,  3.9689e-01, -9.7105e-02, -4.3355e-01,  2.8784e-01,\n",
       "          3.0998e-01,  1.0913e-01, -9.4497e-01,  8.6311e-01, -4.0144e-02,\n",
       "         -2.4387e-01,  7.0290e-01, -1.7989e-01, -1.5477e-01, -8.9427e-01,\n",
       "          8.0712e-01, -4.6744e-01, -4.9218e-01, -9.2157e-01,  9.3375e-01,\n",
       "          5.6050e-01,  3.2076e-01, -4.5406e-01, -8.1633e-01, -3.1498e-01,\n",
       "          6.0237e-01, -5.5730e-01],\n",
       "        [ 2.8135e-01,  3.2616e-01,  3.0443e-01,  6.7426e-02, -7.0694e-02,\n",
       "          1.7412e-02,  2.6933e-01, -2.9663e-01, -6.0754e-01,  2.7041e-01,\n",
       "         -2.6855e-02, -8.8655e-02, -7.2280e-01,  4.8306e-01, -2.2848e-02,\n",
       "         -3.1111e-01,  4.2069e-01,  8.6807e-03, -1.1236e-01, -6.7112e-01,\n",
       "          7.1084e-01, -3.6848e-01, -4.5953e-01, -6.8301e-01,  7.1821e-01,\n",
       "          3.2515e-01,  2.4136e-01, -6.1978e-01, -6.2238e-01, -2.1704e-01,\n",
       "          5.4913e-01, -3.2481e-01],\n",
       "        [ 3.4291e-01,  3.4936e-01,  4.2642e-01,  5.6842e-02,  8.8936e-02,\n",
       "          1.2400e-01, -8.1417e-02,  2.6073e-02, -3.4654e-01,  1.9806e-01,\n",
       "          6.6192e-02,  1.4387e-01, -6.8503e-01,  6.7171e-01, -9.1579e-02,\n",
       "          5.7705e-03,  2.8387e-01, -2.5024e-02, -1.0621e-01, -4.7281e-01,\n",
       "          4.5063e-01, -1.9446e-01, -3.9389e-01, -3.1265e-01,  7.5050e-01,\n",
       "          2.8504e-01,  1.1738e-01, -4.2482e-01, -5.5109e-01, -1.7587e-01,\n",
       "          3.4861e-01, -6.7216e-02],\n",
       "        [ 3.2415e-01,  2.7893e-01,  3.7458e-01,  5.3948e-02,  1.6536e-01,\n",
       "          7.7397e-03,  2.9331e-01, -5.2241e-02, -3.7669e-01,  2.5289e-01,\n",
       "          3.5400e-01,  1.6119e-01, -9.4772e-01,  8.9870e-01, -3.9555e-02,\n",
       "         -2.0489e-01,  6.5774e-01, -1.8293e-01, -1.6409e-01, -8.5862e-01,\n",
       "          7.6096e-01, -4.3954e-01, -4.8886e-01, -8.3787e-01,  9.3640e-01,\n",
       "          5.3636e-01,  3.1535e-01, -4.2504e-01, -7.6365e-01, -2.9537e-01,\n",
       "          5.6600e-01, -5.3036e-01],\n",
       "        [ 1.3210e-01, -7.7501e-02,  1.5499e-01,  2.6828e-03,  3.0041e-01,\n",
       "         -9.7492e-02,  6.1665e-02,  2.2028e-01,  2.3720e-01,  1.5055e-02,\n",
       "          5.7946e-01,  3.2023e-01, -4.6067e-01,  6.4703e-01, -2.2189e-02,\n",
       "          4.4131e-02,  3.7974e-01, -2.5237e-01, -1.2841e-01, -4.4840e-01,\n",
       "          2.2903e-01, -1.6077e-01, -1.8973e-01, -3.3016e-01,  3.8949e-01,\n",
       "          3.1764e-01,  2.0487e-01,  1.3981e-01, -2.1166e-01, -7.4285e-02,\n",
       "          1.5375e-01, -3.6692e-01],\n",
       "        [ 3.7457e-01,  2.5887e-01,  4.4091e-01,  1.4398e-02,  1.3967e-01,\n",
       "         -1.6358e-02,  3.0192e-02, -6.8853e-03, -2.9855e-01,  1.6761e-01,\n",
       "          3.4447e-01,  2.2072e-01, -8.7184e-01,  8.3493e-01, -4.7418e-02,\n",
       "         -1.5121e-01,  4.7103e-01, -1.3202e-01, -1.7377e-01, -7.3720e-01,\n",
       "          6.4191e-01, -3.3049e-01, -4.8292e-01, -5.9957e-01,  8.6740e-01,\n",
       "          3.9641e-01,  2.9202e-01, -3.8895e-01, -5.6944e-01, -2.1042e-01,\n",
       "          4.5933e-01, -4.0477e-01],\n",
       "        [ 3.5636e-01,  2.6033e-01,  3.9921e-01,  7.5122e-02,  9.5669e-02,\n",
       "         -1.8796e-02,  1.3716e-01, -1.1781e-01, -3.9067e-01,  2.4171e-01,\n",
       "          2.3447e-01,  1.0855e-01, -8.5557e-01,  7.5989e-01, -6.0481e-02,\n",
       "         -1.7869e-01,  4.7250e-01, -8.3678e-02, -1.7075e-01, -7.6756e-01,\n",
       "          6.8253e-01, -3.3696e-01, -5.2662e-01, -6.2254e-01,  8.2101e-01,\n",
       "          3.9237e-01,  2.8447e-01, -4.8809e-01, -6.0204e-01, -1.7579e-01,\n",
       "          5.2552e-01, -3.3876e-01],\n",
       "        [ 3.4369e-01,  2.8762e-01,  4.4360e-01,  3.3833e-02,  2.5039e-01,\n",
       "          1.0688e-01, -7.3842e-02,  2.0681e-01, -1.3831e-01,  1.5824e-01,\n",
       "          3.5236e-01,  3.2224e-01, -7.9272e-01,  9.2720e-01, -8.7989e-02,\n",
       "          8.6226e-02,  4.2670e-01, -1.6571e-01, -1.3555e-01, -5.5542e-01,\n",
       "          4.3810e-01, -2.3299e-01, -3.7350e-01, -3.8505e-01,  8.4595e-01,\n",
       "          4.1443e-01,  1.5979e-01, -2.4322e-01, -5.9916e-01, -2.2057e-01,\n",
       "          3.2608e-01, -2.1696e-01],\n",
       "        [ 3.4202e-01,  3.5688e-01,  3.9638e-01,  7.0757e-02,  6.0215e-02,\n",
       "          2.4301e-02,  2.8163e-01, -1.8796e-01, -5.1395e-01,  2.9490e-01,\n",
       "          2.5531e-01,  6.6318e-02, -9.9674e-01,  8.8358e-01, -1.2760e-02,\n",
       "         -2.4532e-01,  6.1508e-01, -1.1480e-01, -1.6982e-01, -8.4016e-01,\n",
       "          8.0081e-01, -4.8189e-01, -5.3594e-01, -8.0059e-01,  9.5770e-01,\n",
       "          5.2337e-01,  3.0534e-01, -6.0983e-01, -7.9588e-01, -2.8790e-01,\n",
       "          6.4978e-01, -4.7498e-01],\n",
       "        [ 3.2928e-01,  2.4330e-01,  3.7963e-01,  5.8628e-02,  1.8419e-01,\n",
       "         -1.6364e-02,  2.4107e-01, -3.4806e-02, -3.0646e-01,  2.3705e-01,\n",
       "          4.1256e-01,  1.9011e-01, -9.4829e-01,  9.3128e-01, -3.8189e-02,\n",
       "         -1.6896e-01,  6.3040e-01, -1.8973e-01, -1.7997e-01, -8.4631e-01,\n",
       "          7.2070e-01, -4.1892e-01, -4.9782e-01, -7.6708e-01,  9.0791e-01,\n",
       "          5.2500e-01,  3.1901e-01, -3.9807e-01, -7.0665e-01, -2.5390e-01,\n",
       "          5.5167e-01, -5.0526e-01],\n",
       "        [ 4.2737e-01,  1.7011e-01,  4.9870e-01,  4.6640e-02,  2.8705e-01,\n",
       "         -3.4847e-02, -1.1950e-01,  1.7149e-01, -9.4963e-02,  1.4435e-01,\n",
       "          4.5083e-01,  3.6236e-01, -8.5188e-01,  9.2806e-01, -1.2553e-01,\n",
       "          1.0087e-02,  4.2073e-01, -1.6529e-01, -1.9982e-01, -7.4567e-01,\n",
       "          5.3977e-01, -1.9944e-01, -5.1733e-01, -4.4298e-01,  8.4903e-01,\n",
       "          3.6465e-01,  2.7944e-01, -2.2626e-01, -4.7373e-01, -1.1685e-01,\n",
       "          3.6024e-01, -2.7438e-01],\n",
       "        [ 2.2003e-01,  4.1274e-01,  2.8005e-01,  3.3155e-02, -4.1172e-02,\n",
       "          1.5913e-01,  2.3094e-01, -1.4592e-01, -5.6248e-01,  2.4029e-01,\n",
       "         -8.4187e-02, -4.1023e-02, -6.2837e-01,  4.9032e-01, -1.7125e-02,\n",
       "         -1.8566e-01,  4.0298e-01, -1.6677e-02, -4.5701e-02, -4.6719e-01,\n",
       "          5.6276e-01, -3.3902e-01, -2.9534e-01, -5.8643e-01,  7.1815e-01,\n",
       "          3.4618e-01,  1.1679e-01, -5.2080e-01, -6.9877e-01, -3.1340e-01,\n",
       "          4.3203e-01, -2.5263e-01],\n",
       "        [ 3.3068e-01,  2.5615e-01,  3.8633e-01,  6.0099e-02,  1.9139e-01,\n",
       "          2.5425e-04,  2.2890e-01, -1.7089e-02, -3.0191e-01,  2.3916e-01,\n",
       "          4.0885e-01,  1.9882e-01, -9.5031e-01,  9.4768e-01, -4.1585e-02,\n",
       "         -1.4744e-01,  6.2826e-01, -1.9106e-01, -1.7652e-01, -8.3107e-01,\n",
       "          7.0734e-01, -4.1418e-01, -4.9106e-01, -7.4954e-01,  9.1772e-01,\n",
       "          5.3020e-01,  3.0585e-01, -3.9567e-01, -7.1982e-01, -2.5998e-01,\n",
       "          5.4478e-01, -4.8696e-01],\n",
       "        [ 2.2060e-01,  1.5193e-01,  2.2274e-01,  8.5284e-03,  6.7031e-02,\n",
       "         -1.9420e-02,  2.3986e-01, -5.6179e-02, -3.7585e-01,  1.4331e-01,\n",
       "          2.8582e-03,  3.7324e-02, -4.4242e-01,  2.5586e-01, -8.6887e-02,\n",
       "         -2.7172e-01,  3.6956e-01, -5.5963e-02, -6.2068e-02, -5.9165e-01,\n",
       "          5.6561e-01, -1.9654e-01, -3.0495e-01, -6.7823e-01,  5.3731e-01,\n",
       "          2.0903e-01,  2.2343e-01, -2.2212e-01, -4.2853e-01, -1.9691e-01,\n",
       "          2.8580e-01, -3.5397e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import AttentionHead\n",
    "\n",
    "attention_test = AttentionHead(32, 32, 0.2)\n",
    "attention_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset\n",
    "- behöver vocabulary\n",
    "- saknar: getitem och prepare dataset som kallar på construct_masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data importerad\n",
      "här är lugnt\n",
      "[['[CLS]', '[PAD]', 'USA', 'aadA1', 'sul1', 'tet(A)'], ['[CLS]', '[PAD]', 'Sweden', '[MASK]', '[MASK]', 'tet(A)', '[MASK]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT'], ['[CLS]', '[PAD]', 'Sweden', 'glpT_E448K=POINT', 'cyaA_S352T=POINT', 'uhpT_E350Q=POINT'], ['[CLS]', '1979', 'USA', 'glpT_E448K=POINT'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', '[MASK]', '[MASK]'], ['[CLS]', '[PAD]', 'USA', '[MASK]', 'pmrB_E123D=POINT', '[MASK]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT'], ['[CLS]', '[PAD]', 'Sweden', 'parC_A56T=POINT', 'glpT_E448K=POINT', '[MASK]', '[MASK]'], ['[CLS]', '[PAD]', 'Sweden', 'parE_I355T=POINT'], ['[CLS]', '[PAD]', 'Sweden', 'glpT_E448K=POINT'], ['[CLS]', '[PAD]', 'Sweden', \"aph(3'')-Ib\", 'tet(B)', 'sul2', '[MASK]'], ['[CLS]', '[PAD]', 'Indonesia', 'glpT_E448K=POINT'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_E123D=POINT'], ['[CLS]', '[PAD]', 'USA', \"aph(3'')-Ib\", 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', 'aph(6)-Id', 'tet(B)'], ['[CLS]', '1979', 'USA', '[MASK]', '1979', 'pmrB_Y358N=POINT', 'aph(6)-Id', 'aph(6)-Ic'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_E123D=POINT'], ['[CLS]', '[PAD]', 'USA', '[MASK]', 'pmrB_Y358N=POINT'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT']]\n",
      "[[-1, -1, -1, -1, -1, -1], [-1, -1, -1, 216, 217, -1, 218], [-1, -1, -1, 219, -1], [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1], [-1, -1, -1, 219, 223, 224], [-1, -1, -1, 219, -1, 222], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, 225, -1, 215, 220], [-1, -1, -1, -1], [-1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, 218], [-1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, 216, 219, -1, -1, 227], [-1, -1, -1, -1, -1], [-1, -1, -1, 219, -1], [-1, -1, -1, -1, -1]]\n",
      "[['[CLS]', '[PAD]', 'USA', 'aadA1', 'sul1', 'tet(A)', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', '[MASK]', '[MASK]', 'tet(A)', '[MASK]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', 'glpT_E448K=POINT', 'cyaA_S352T=POINT', 'uhpT_E350Q=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '1979', 'USA', 'glpT_E448K=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', '[MASK]', '[MASK]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', '[MASK]', 'pmrB_E123D=POINT', '[MASK]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', 'parC_A56T=POINT', 'glpT_E448K=POINT', '[MASK]', '[MASK]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', 'parE_I355T=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', 'glpT_E448K=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', \"aph(3'')-Ib\", 'tet(B)', 'sul2', '[MASK]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Indonesia', 'glpT_E448K=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_E123D=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', \"aph(3'')-Ib\", 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', 'aph(6)-Id', 'tet(B)', '[PAD]', '[PAD]'], ['[CLS]', '1979', 'USA', '[MASK]', '1979', 'pmrB_Y358N=POINT', 'aph(6)-Id', 'aph(6)-Ic', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_E123D=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', '[MASK]', 'pmrB_Y358N=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
      "range(0, 347995)\n",
      "hit men inte längre \n",
      "                          masked_indices  \\\n",
      "0  [0, 1, 59, 213, 214, 215, 1, 1, 1, 1]   \n",
      "1      [0, 1, 60, 2, 2, 215, 2, 1, 1, 1]   \n",
      "2    [0, 1, 59, 219, 220, 1, 1, 1, 1, 1]   \n",
      "3  [0, 1, 60, 219, 221, 222, 1, 1, 1, 1]   \n",
      "4      [0, 4, 59, 219, 1, 1, 1, 1, 1, 1]   \n",
      "\n",
      "                                       indices  \n",
      "0     [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
      "1  [-1, -1, -1, 216, 217, -1, 218, -1, -1, -1]  \n",
      "2    [-1, -1, -1, 219, -1, -1, -1, -1, -1, -1]  \n",
      "3     [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
      "4     [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import vocab \n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "from data_preprocessing import data_loader\n",
    "\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year)\n",
    "NCBI.fillna('[PAD]', inplace=True)\n",
    "print('data importerad')\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MASK_PERCENTAGE = 0.15\n",
    "\n",
    "\n",
    "# data, vocabulary, max sequence length, mask probability, include sequences, some random state\n",
    "class NCBIDataset(Dataset):\n",
    "\n",
    "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
    "    TARGET_COLUMN = 'indices'\n",
    "    NSP_TARGET_COLUMN = 'is_next'\n",
    "    TOKEN_MASK_COLUMN = 'token_mask'\n",
    "\n",
    "    def __init__(self,\n",
    "                 data: pd.DataFrame,\n",
    "                 vocab: vocab,\n",
    "                 max_seq_len: int,\n",
    "                 mask_prob: float,\n",
    "                 random_state: int = 23,\n",
    "                 ):\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        CLS = '[CLS]'\n",
    "        PAD = '[PAD]'\n",
    "        MASK = '[MASK]'\n",
    "        UNK = '[UNK]'\n",
    "\n",
    "        self.data = data.reset_index(drop=True) \n",
    "        self.num_samples = self.data.shape[0]\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.CLS = CLS \n",
    "        self.PAD = PAD\n",
    "        self.MASK = MASK\n",
    "        self.UNK = UNK\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        input = torch.Tensor(item[self.MASKED_INDICES_COLUMN],device=device).long()\n",
    "        token_mask  = torch.tensor(item[self.TARGET_COLUMN], device=device).long()\n",
    "        attention_mask = (input == self.vocab[self.PAD]).unsqueeze(0)\n",
    "\n",
    "        return input, token_mask , attention_mask\n",
    "\n",
    "    def _construct_masking(self):\n",
    "        sequences = deepcopy(self.data['genes'].tolist())\n",
    "        masked_sequences = list()\n",
    "        target_indices_list = list()\n",
    "        seq_starts = [[self.CLS, self.data['year'].iloc[i], self.data['location'].iloc[i]] for i in range(self.data.shape[0])]\n",
    "\n",
    "        for i, geno_seq in enumerate(sequences):\n",
    "            seq_len = len(geno_seq)\n",
    "            masking_index = np.random.rand(seq_len) < self.mask_prob   \n",
    "            target_indices = np.array([-1]*seq_len)\n",
    "            indices = masking_index.nonzero()[0]\n",
    "            target_indices[indices] = self.vocab.lookup_indices([geno_seq[i] for i in indices])\n",
    "            for i in indices:\n",
    "                r = np.random.rand()\n",
    "                if r < 0.8:\n",
    "                    geno_seq[i] = self.MASK\n",
    "                elif r > 0.9:\n",
    "                    geno_seq[i] = self.vocab.lookup_token(np.random.randint(self.vocab_size))\n",
    "            geno_seq = seq_starts[i] + geno_seq\n",
    "            target_indices = [-1]*3 + target_indices.tolist() \n",
    "            masked_sequences.append(geno_seq)\n",
    "            target_indices_list.append(target_indices)\n",
    "        print('här är lugnt')\n",
    "        print(masked_sequences[:20])\n",
    "        print(target_indices_list[:20])\n",
    "        masked_sequences = [seq + [self.PAD]*(self.max_seq_len - len(seq)) for seq in masked_sequences]\n",
    "        print(masked_sequences[:20])\n",
    "        print(range(len(target_indices_list)))\n",
    "        for i in range(len(target_indices_list)):\n",
    "            indices = target_indices_list[i]\n",
    "            padding = [-1] * (self.max_seq_len - len(indices))\n",
    "            target_indices_list[i] = indices + padding\n",
    "        print('hit men inte längre ')\n",
    "        return masked_sequences, target_indices_list \n",
    "        \n",
    "    def prepare_dataset(self):\n",
    "        masked_sequences, target_indices = self._construct_masking()\n",
    "        indices_masked = [self.vocab.lookup_indices(masked_seq) for masked_seq in masked_sequences]\n",
    "\n",
    "        rows = zip(indices_masked, target_indices)\n",
    "        self.data = pd.DataFrame(rows, columns=self.columns)\n",
    "        print(self.data.head())\n",
    "\n",
    "def make_vocabulary(dataset: pd.DataFrame):\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    token_list = Counter()\n",
    "    data = dataset.copy()\n",
    "\n",
    "    location_tokens = list(dict.fromkeys(list(chain(list(data['location'])))))\n",
    "    year_tokens = list(dict.fromkeys(list(chain(list(data['year'])))))\n",
    "    genes_tokens = list(dict.fromkeys(list(chain(*data['genes']))))\n",
    "   \n",
    "    token_list.update(map(str, year_tokens))\n",
    "    token_list.update(map(str, location_tokens))\n",
    "    token_list.update(map(str, genes_tokens))\n",
    "    vocabulary = vocab(token_list,specials = [CLS, PAD, MASK, UNK])\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "max_length = 10\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data importerad\n",
      "här är lugnt\n",
      "[['[CLS]', nan, 'USA', 'sul1', 'tet(A)', 'aadA1'], ['[CLS]', nan, 'Sweden', '[MASK]', '[MASK]', 'aph(6)-Id', '[MASK]'], ['[CLS]', nan, 'USA', 'pmrB_Y358N=POINT', 'glpT_E448K=POINT'], ['[CLS]', nan, 'Sweden', 'uhpT_E350Q=POINT', 'cyaA_S352T=POINT', 'glpT_E448K=POINT'], ['[CLS]', '1979', 'USA', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', 'pmrB_E123D=POINT', '[MASK]', '[MASK]'], ['[CLS]', nan, 'USA', '[MASK]', 'pmrB_E123D=POINT', '[MASK]'], ['[CLS]', nan, 'USA', 'pmrB_Y358N=POINT', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', 'pmrB_Y358N=POINT', 'glpT_E448K=POINT'], ['[CLS]', nan, 'Sweden', 'pmrB_Y358N=POINT', 'tet(A)', '[MASK]', '[MASK]'], ['[CLS]', nan, 'Sweden', 'parE_I355T=POINT'], ['[CLS]', nan, 'Sweden', 'glpT_E448K=POINT'], ['[CLS]', nan, 'Sweden', 'sul2', \"aph(3'')-Ib\", 'aph(6)-Id', '[MASK]'], ['[CLS]', nan, 'Indonesia', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', 'pmrB_E123D=POINT', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', 'aph(6)-Id', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', \"aph(3'')-Ib\", 'tet(B)'], ['[CLS]', '1979', 'USA', '[MASK]', 'nan', 'pmrB_Y358N=POINT', \"aph(3'')-Ib\", \"aph(3')-IIa\"], ['[CLS]', nan, 'USA', 'pmrB_E123D=POINT', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', '[MASK]', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', 'pmrB_Y358N=POINT', 'glpT_E448K=POINT']]\n",
      "[[-1, -1, -1, -1, -1, -1], [-1, -1, -1, 217, 218, -1, 215], [-1, -1, -1, 220, -1], [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1], [-1, -1, -1, 224, 225, 221], [-1, -1, -1, 222, -1, 221], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, 220, -1, 221, 226], [-1, -1, -1, -1], [-1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, 228], [-1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, 219, 221, -1, -1, 228], [-1, -1, -1, -1, -1], [-1, -1, -1, 220, -1], [-1, -1, -1, -1, -1]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import vocab \n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "from data_preprocessing import data_loader\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year)\n",
    "print('data importerad')\n",
    "\n",
    "def make_vocabulary(dataset: pd.DataFrame):\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    token_list = Counter()\n",
    "    data = dataset.copy()\n",
    "\n",
    "    location_tokens = list(dict.fromkeys(list(chain(list(data['location'])))))\n",
    "    year_tokens = list(dict.fromkeys(list(chain(list(data['year'])))))\n",
    "    genes_tokens = list(dict.fromkeys(list(chain(*data['genes']))))\n",
    "   \n",
    "    token_list.update(map(str, year_tokens))\n",
    "    token_list.update(map(str, location_tokens))\n",
    "    token_list.update(map(str, genes_tokens))\n",
    "    vocabulary = vocab(token_list,specials = [CLS, PAD, MASK, UNK])\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "max_length = 10\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import typing\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer \n",
    "\n",
    "print(list(chain(*NCBI['genes']))[:20])\n",
    "\n",
    "mylist = list(dict.fromkeys(list(chain(*NCBI['genes']))))\n",
    "print(len(mylist))\n",
    "print(mylist[:20])\n",
    "\n",
    "test = list(chain(list(NCBI['year'])))\n",
    "print(test[:40])\n",
    "test = list(dict.fromkeys(list(chain(list(NCBI['year'])))))\n",
    "print(len(test))\n",
    "print(test[:40])\n",
    "\n",
    "test = list(chain(list(NCBI['location'])))\n",
    "print(test[:20])\n",
    "test = list(dict.fromkeys(list(chain(list(NCBI['location'])))))\n",
    "print(len(test))\n",
    "print(test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'nan': 2, '1979': 1, '2013': 1, '1996': 1, '1975': 1, '2009': 1, '1974': 1, '1998': 1, '1983': 1, '2008': 1, '1995': 1, '1989': 1, '2003': 1, '1997': 1, '1994': 1, '2001': 1, '1980': 1, '1970': 1, '1971': 1, '2007': 1, '2004': 1, '2006': 1, '1982': 1, '1991': 1, '1990': 1, '2002': 1, '1988': 1, '1987': 1, '2011': 1, '1986': 1, '1985': 1, '2010': 1, '2005': 1, '2012': 1, '1984': 1, '2000': 1, '2014': 1, '2015': 1, '1999': 1, '1992': 1, '1973': 1, '1981': 1, '1993': 1, '2016': 1, '1972': 1, '1976': 1, '1977': 1, '2017': 1, '1978': 1, '2018': 1, '2019': 1, '2020': 1, '2021': 1, '2022': 1, '2023': 1, '2024': 1, 'USA': 1, 'Sweden': 1, 'Indonesia': 1, 'Canada': 1, 'Papua New Guinea': 1, 'Japan': 1, 'China': 1, 'Germany': 1, 'India': 1, 'Bangladesh': 1, 'Brazil': 1, 'UK': 1, 'Denmark': 1, 'South Korea': 1, 'France': 1, 'Norway': 1, 'Malaysia': 1, 'Slovenia': 1, 'Thailand': 1, 'Italy': 1, 'Israel': 1, 'Austria': 1, 'Belgium': 1, 'Guinea-Bissau': 1, 'Australia': 1, 'Hungary': 1, 'Lebanon': 1, 'Spain': 1, 'Tanzania': 1, 'Hong Kong': 1, 'Kenya': 1, 'Guatemala': 1, 'Netherlands': 1, 'Singapore': 1, 'Colombia': 1, 'Chile': 1, 'Argentina': 1, 'Russia': 1, 'Haiti': 1, 'Egypt': 1, 'Madagascar': 1, 'Pakistan': 1, 'Sri Lanka': 1, 'Cuba': 1, 'Viet Nam': 1, 'Nigeria': 1, 'Burkina Faso': 1, 'Mexico': 1, 'Morocco': 1, 'French Guiana': 1, 'Senegal': 1, 'Iran': 1, 'Dominican Republic': 1, 'Uzbekistan': 1, 'Peru': 1, 'Nepal': 1, 'Portugal': 1, 'Gambia': 1, 'Mali': 1, 'Mozambique': 1, 'South Africa': 1, 'DRC': 1, 'Bolivia': 1, 'Czech Republic': 1, 'Serbia': 1, 'Finland': 1, 'Tonga': 1, 'UAE': 1, 'Ireland': 1, 'Turkey': 1, 'Macedonia': 1, 'Poland': 1, 'Switzerland': 1, 'Saudi Arabia': 1, 'Zambia': 1, 'Brunei': 1, 'Estonia': 1, 'Slovakia': 1, 'Jordan': 1, 'Guam': 1, 'Bulgaria': 1, 'Zaire': 1, 'Venezuela': 1, 'Paraguay': 1, 'Costa Rica': 1, 'Uruguay': 1, 'Uganda': 1, 'New Zealand': 1, 'Taiwan': 1, 'Ghana': 1, 'Laos': 1, 'Tunisia': 1, 'Georgia': 1, 'Somalia': 1, 'Puerto Rico': 1, 'West Bank': 1, 'Burundi': 1, 'Greece': 1, 'Qatar': 1, 'Romania': 1, 'Togo': 1, 'Ecuador': 1, 'Guinea': 1, 'Niger': 1, 'Kosovo': 1, 'Afghanistan': 1, 'Ukraine': 1, 'Croatia': 1, 'Cameroon': 1, 'Myanmar': 1, 'Pacific Ocean': 1, 'Malawi': 1, 'Sudan': 1, 'Latvia': 1, 'Lithuania': 1, 'Algeria': 1, 'Oman': 1, 'Cambodia': 1, 'Philippines': 1, 'Rwanda': 1, 'Kuwait': 1, 'Mongolia': 1, 'Antarctica': 1, 'Gabon': 1, 'Syria': 1, 'Korea': 1, 'Mauritius': 1, 'Belarus': 1, 'Ethiopia': 1, 'Guyana': 1, 'Reunion': 1, 'New Caledonia': 1, 'Guadeloupe': 1, 'Cyprus': 1, 'Luxembourg': 1, 'Atlantic Ocean': 1, 'Benin': 1, 'Kazakhstan': 1, 'Bahrain': 1, 'Angola': 1, 'Botswana': 1, 'Armenia': 1, 'Albania': 1, 'Greenland': 1, 'Jamaica': 1, 'Iraq': 1, 'South Sudan': 1, 'Djibouti': 1, \"Cote d'Ivoire\": 1, 'Gaza Strip': 1, 'Iceland': 1, 'Honduras': 1, 'Svalbard': 1, 'Libya': 1, 'sul1': 1, 'tet(A)': 1, 'aadA1': 1, \"aph(3'')-Ib\": 1, 'aph(6)-Id': 1, 'sul2': 1, 'glpT_E448K=POINT': 1, 'pmrB_Y358N=POINT': 1, 'cyaA_S352T=POINT': 1, 'uhpT_E350Q=POINT': 1, 'parE_D475E=POINT': 1, 'pmrB_E123D=POINT': 1, 'parC_A56T=POINT': 1, 'parE_I355T=POINT': 1, 'tet(B)': 1, 'tet(C)': 1, 'catA1': 1, \"aph(3')-Ia\": 1, 'parC_S57T=POINT': 1, 'aac(3)-VIa': 1, 'blaTEM-1': 1, 'aac(3)-IId': 1, 'gyrA_S83L=POINT': 1, 'blaCMY-2': 1, 'aac(3)-IVa': 1, 'aph(4)-Ia': 1, 'parE_I529L=POINT': 1, 'ptsI_V25I=POINT': 1, 'catB3': 1, 'dfrA1': 1, 'aadA5': 1, 'blaSHV-2A': 1, 'ampC_C-42T=POINT': 1, 'cmlA5': 1, \"ant(2'')-Ia\": 1, 'aadA2': 1, 'dfrA12': 1, 'dfrA14': 1, 'dfrA16': 1, 'blaCARB-2': 1, 'ere(A)': 1, 'tet(M)': 1, 'blaPSE': 1, 'nfsA_Q44STOP=POINT': 1, 'blaHER-3': 1, 'fosA7.5': 1, 'blaTEM': 1, 'nfsA_Q113STOP=POINT': 1, 'marR_S3N=POINT': 1, 'pmrB_E121K=POINT': 1, 'parC_E84G=POINT': 1, 'parC_S80I=POINT': 1, 'catA2': 1, 'gyrA_D87N=POINT': 1, 'mph(A)': 1, 'dfrA17': 1, 'sat2': 1, 'ompF_Q88STOP=POINT': 1, 'parE_S458T=POINT': 1, 'dfrA36': 1, 'floR': 1, 'gyrA_D87Y=POINT': 1, 'aadA22': 1, 'parE_S458A=POINT': 1, 'parC_A108T=POINT': 1, 'blaTEM-10': 1, 'blaOXA-1': 1, 'nfsA_G154E=POINT': 1, 'dfrA5': 1, 'blaCTX-M-15': 1, \"aac(6')-Ib-cr5\": 1, 'parC_E84V=POINT': 1, 'mph(B)': 1, 'ampC_T-32A=POINT': 1, 'gyrA_S83A=POINT': 1, 'dfrA7': 1, 'parC_E84A=POINT': 1, 'soxS_A12S=POINT': 1, \"aac(6')-Ib'\": 1, 'blaOXA-9': 1, 'dfrB1': 1, 'blaTEM-116': 1, 'cmlA1': 1, 'sul3': 1, 'aadA13': 1, 'oqxB': 1, 'oqxA': 1, 'bleO': 1, '16S_A964G=POINT': 1, 'pmrB_V161G=POINT': 1, '23S_T754A=POINT': 1, 'ble': 1, \"aph(3')-IIa\": 1, 'aph(6)-Ic': 1, '16S_A1408G=POINT': 1, '16S_C1192T=POINT': 1, 'folP_P64S=POINT': 1, 'aadA7': 1, '16S_A794G=POINT': 1, '16S_A1055G=POINT': 1, 'parC_S80R=POINT': 1, 'cmlA6': 1, 'blaCTX-M-14': 1, 'blaSHV-1': 1, 'tet(D)': 1, 'pmrB_A159V=POINT': 1, 'parE_L416F=POINT': 1, 'rpoB_H526L=POINT': 1, 'dfrA15': 1, 'blaTEM-181': 1, 'dfrA8': 1, 'lnu(F)': 1, 'gyrA_D87G=POINT': 1, 'blaCMY-23': 1, 'qnrS1': 1, 'dfrB4': 1, 'tufA_Q125R=POINT': 1, 'parC_E84K=POINT': 1, 'rmtB1': 1, 'nfsA_W159STOP=POINT': 1, 'pmrB_T92P=POINT': 1, 'blaI': 1, 'blaPC1': 1, 'ompC_Q171STOP=POINT': 1, 'blaTEM-40': 1, 'aadA6': 1, 'aac(3)-IIe': 1, 'folP_P64A=POINT': 1, 'nfsA_Q67STOP=POINT': 1, 'blaTEM-30': 1, '23S_T2609C=POINT': 1, 'blaKPC-3': 1, 'fabI_F203L=POINT': 1, 'blaSHV-12': 1, \"aac(6')-Ib3\": 1, 'nfsA_G126R=POINT': 1, 'emrR_L113P=POINT': 1, 'blaCTX-M-55': 1, 'marR_A70T=POINT': 1, 'rpoB_Q148L=POINT': 1, 'ftsI_I336IKYRI=POINT': 1, 'blaCTX-M-1': 1, 'msr(E)': 1, 'mph(E)': 1, 'lnu(G)': 1, 'blaTEM-52': 1, 'dfrA25': 1, 'qnrB2': 1, \"aac(2')-IIa\": 1, 'erm(B)': 1, 'parC_A108V=POINT': 1, 'aadA15': 1, 'nfsA_G131D=POINT': 1, 'dfrA4': 1, 'rpoB_V146F=POINT': 1, 'soxR_G121D=POINT': 1, 'blaKPC-2': 1, \"aac(6')-IIc\": 1, 'arr': 1, 'aac(3)-IIg': 1, 'blaSHV': 1, 'acrB_R717L=POINT': 1, 'arr-3': 1, 'blaCTX-M-3': 1, 'qnrA3': 1, 'ompC_Q82STOP=POINT': 1, 'blaCTX-M-27': 1, 'blaNDM-1': 1, \"aac(6')-Ib\": 1, 'blaTEM-33': 1, 'dfrA21': 1, 'aadA4': 1, 'catB11': 1, 'blaFOX-5': 1, 'dfrA19': 1, 'ampC_C-11T=POINT': 1, 'blaTEM-105': 1, 'blaTEM-135': 1, 'folP_F28L=POINT': 1, 'dfrA32': 1, 'qnrB19': 1, 'rmtE1': 1, 'dfrA34': 1, 'nfsA_S33R=POINT': 1, 'blaOXA-181': 1, 'dfrA27': 1, 'qnrB6': 1, 'aadA16': 1, 'blaCTX-M-104': 1, 'blaTEM-19': 1, 'ampC_G-15GG=POINT': 1, 'blaTEM-12': 1, 'nfsB_W94STOP=POINT': 1, 'parE_L445H=POINT': 1, 'gyrA_S83W=POINT': 1, 'gyrA_D87H=POINT': 1, 'blaCMY': 1, 'qnrA1': 1, \"aac(6')-Ib4\": 1, 'fosA4': 1, 'mcr-1.1': 1, 'tet(X5)': 1, 'blaTEM-176': 1, 'mcr-3.1': 1, 'ftsI_N337NYRIN=POINT': 1, 'blaNDM-5': 1, 'blaCMY-42': 1, 'blaNDM': 1, \"aac(6')-Ian\": 1, 'aadA10': 1, 'mef(B)': 1, 'aadA25': 1, 'qnrS2': 1, 'fosA3': 1, 'armA': 1, 'fosA7': 1, 'floR2': 1, 'qnrE4': 1, 'tet(G)': 1, 'tet(Y)': 1, 'aadA12': 1, 'estT': 1, 'dfrA23': 1, 'aadA8': 1, 'aadA31': 1, 'qepA1': 1, 'uhpA_G97D=POINT': 1, 'blaOXA-2': 1, 'fosA': 1, 'blaOXA': 1, 'blaTEM-148': 1, 'rmtB3': 1, 'rpoB_Q513L=POINT': 1, 'rpoB_D516G=POINT': 1, 'blaIMP-4': 1, \"ant(3'')-Ia\": 1, 'blaTEM-32': 1, 'dfrA3b': 1, 'nfsA_R203C=POINT': 1, 'rpoB_S531F=POINT': 1, '23S_G2032A=POINT': 1, 'catB8': 1, 'blaOXA-10': 1, 'blaCMY-58': 1, 'nfsA_R133S=POINT': 1, 'qepA4': 1, 'blaCMY-4': 1, 'nfsA_H11Y=POINT': 1, \"aac(6')-Ib-cr\": 1, 'blaCTX-M-32': 1, 'tet(31)': 1, 'blaCTX-M': 1, 'blaHER': 1, 'blaLAP': 1, 'rmtC': 1, 'blaCMY-6': 1, 'blaOXA-48': 1, \"aph(3')-VI\": 1, 'blaVIM-29': 1, \"aac(6')-Il\": 1, 'arr-2': 1, 'blaCTX-M-24': 1, 'blaIMP-26': 1, 'blaNDM-7': 1, 'parE_E460D=POINT': 1, 'blaVIM-4': 1, 'blaTEM-169': 1, 'mcr-10': 1, 'blaTEM-190': 1, 'ompR_G63V=POINT': 1, 'ampC_T-14TGT=POINT': 1, 'marR_R77L=POINT': 1, 'qnrB1': 1, 'blaTEM-156': 1, 'blaCTX-M-123': 1, 'blaTEM-84': 1, 'qepA7': 1, 'dfrA26': 1, 'qnrS11': 1, 'oqxB29': 1, 'blaCTX-M-65': 1, 'nfsA_E223STOP=POINT': 1, \"aph(3')-VIa\": 1, 'blaIMP-14': 1, 'aacA34': 1, 'soxR_R20H=POINT': 1, \"aph(3')-VIb\": 1, 'qnrB10': 1, 'blaOXA-163': 1, \"aac(6')-Ib-cr7\": 1, 'parE_I464F=POINT': 1, 'fosA8': 1, 'blaTEM-57': 1, 'blaLAP-2': 1, 'basR_G53E=POINT': 1, 'blaTEM-106': 1, 'blaTEM-15': 1, 'blaTEM-17': 1, 'blaTEM-20': 1, 'blaSHV-2': 1, 'rmtB': 1, 'blaEC-15': 1, 'blaTEM-35': 1, 'qnrS13': 1, 'qnrB4': 1, 'blaDHA-1': 1, 'gyrA_S83V=POINT': 1, 'blaTEM-210': 1, 'qnrB7': 1, 'blaTEM-31': 1, 'rpoB_Q513P=POINT': 1, 'blaCTX-M-115': 1, 'basR_L105P=POINT': 1, 'parC_G78C=POINT': 1, \"aac(6')-Ib11\": 1, 'blaTEM-208': 1, 'dfrA29': 1, 'blaOXA-244': 1, 'blaIMP-1': 1, 'ftsI_I336IPYRI=POINT': 1, 'blaTEM-37': 1, 'mcr-1': 1, 'blaCTX-M-166': 1, 'blaKPC-4': 1, 'blaSHV-7': 1, 'aac(3)-Ib': 1, 'blaIMP-27': 1, 'qepA': 1, 'qnrD1': 1, 'blaOXA-4': 1, \"aph(3')-XV\": 1, 'blaSHV-5': 1, 'parE_E460K=POINT': 1, 'blaSCO-1': 1, 'blaSCO': 1, 'hugA': 1, 'pmrB_T156M=POINT': 1, 'blaTEM-3': 1, 'blaOXA-50': 1, 'blaCMY-16': 1, 'aac(3)-Id': 1, 'npmB2': 1, 'blaCMY-7': 1, 'erm(T)': 1, 'qnrS': 1, 'pmrB_E166K=POINT': 1, 'fosA7.2': 1, 'ere(B)': 1, 'blaTEM-209': 1, 'oqxA2': 1, 'oqxB2': 1, 'blaOXA-129': 1, 'erm(C)': 1, 'mph(C)': 1, 'ampC_C-42A=POINT': 1, '16S_G527T=POINT': 1, 'qnrS12': 1, 'erm(42)': 1, 'blaNDM-9': 1, 'blaCTX-M-125': 1, 'blaCMY-140': 1, 'blaCTX-M-267': 1, 'aadA21': 1, 'blaTEMp_C32T=POINT': 1, 'blaCTX-M-8': 1, 'blaCTX-M-9': 1, 'rpoB_R529C=POINT': 1, 'blaTEM-39': 1, 'qnrVC4': 1, 'basR_G53R=POINT': 1, 'parC_S80W=POINT': 1, 'blaTEM-154': 1, 'acrR_R45C=POINT': 1, 'nfsA_R15C=POINT': 1, 'dfrA10': 1, 'aacA16': 1, 'qepA8': 1, 'fosA5': 1, 'blaCTX-M-182': 1, 'mcr-3.4': 1, 'tet(X4)': 1, 'qnrS4': 1, 'blaGES-5': 1, 'blaTEM-215': 1, 'mcr-3.5': 1, 'blaVEB-1': 1, \"aac(6')-Ia\": 1, 'blaCTX-M-2': 1, 'lnu(A)': 1, 'qnrB': 1, 'mcr-10.1': 1, 'fosA2': 1, \"aac(6')-33\": 1, 'blaTEM-171': 1, 'tet(X3)': 1, 'folP_P64L=POINT': 1, 'nfsB_F84S=POINT': 1, 'blaCTX-M-179': 1, 'blaCMY-136': 1, 'blaCTX-M-153': 1, 'nfsA_E75STOP=POINT': 1, 'blaTEM-54': 1, 'tufA_A376V=POINT': 1, 'cmlA4': 1, 'fosA10': 1, 'mcr-1.4': 1, 'mcr-1.7': 1, 'fabI_G93S=POINT': 1, 'fabI_G93V=POINT': 1, 'mcr-1.5': 1, 'blaNDM-4': 1, 'gyrB_D426N=POINT': 1, 'blaKPC-18': 1, 'aac(3)-Ia': 1, 'blaCMY-111': 1, 'blaNDM-6': 1, 'mcr-1.26': 1, 'lnu(C)': 1, 'aadD1': 1, 'blaZ': 1, 'mcr-3.2': 1, 'blaOXA-232': 1, 'mcr-2.3': 1, 'blaTEM-103': 1, 'mcr-3.24': 1, 'aadA3': 1, 'aad9': 1, 'blaKPC': 1, 'blaROB-11': 1, 'parE_P439S=POINT': 1, 'blaTEM-166': 1, 'nfsA_R203L=POINT': 1, 'qepA6': 1, \"aph(3')-Ib\": 1, 'blaTEM-207': 1, 'mcr-5.1': 1, 'catA3': 1, 'gyrA_Q106H=POINT': 1, 'catB2': 1, 'pmrB_RPISLR6del=POINT': 1, 'fosL': 1, 'catB6': 1, 'blaVIM-1': 1, 'blaACC-1': 1, 'qnrVC1': 1, 'blaPER-2': 1, 'dfrA46': 1, 'ble-Sh': 1, 'blaNDM-21': 1, 'blaLAP-1': 1, 'dfrF': 1, 'rmtD1': 1, 'dfrA22': 1, 'tufA_G317D=POINT': 1, 'mcr-3': 1, 'blaCTX-M-64': 1, 'marR_R94S=POINT': 1, 'blaLAT': 1, 'blaCMY-44': 1, 'lon_P403L=POINT': 1, 'acrB_R620C=POINT': 1, 'mef(C)': 1, 'mph(G)': 1, 'blaSFO-1': 1, 'blaCTX-M-132': 1, 'cfr': 1, 'mcr-1.18': 1, 'tet(H)': 1, 'blaVEB-5': 1, 'tet(E)': 1, 'rpoB_H526Y=POINT': 1, 'blaTEM-4': 1, 'blaTEM-5': 1, 'blaTEM-6': 1, 'aac(3)-Ic': 1, 'blaTEM-7': 1, 'blaIMP-6': 1, 'blaTEM-8': 1, 'rpoB_L533P=POINT': 1, 'blaTEM-2': 1, 'blaCMY-24': 1, 'blaTEM-9': 1, 'sat4': 1, 'parC_G78D=POINT': 1, 'qepA9': 1, 'npmB1': 1, 'blaVEB-17': 1, \"aac(6')-IIa\": 1, 'qepA10': 1, 'mcr-1.2': 1, 'pmrB_L14Q=POINT': 1, 'blaCTX-M-134': 1, 'blaCTX-M-174': 1, 'blaCMY-69': 1, 'blaCMY-33': 1, 'pmrB_C84Y=POINT': 1, 'pmrB_L10P=POINT': 1, 'basR_R81S=POINT': 1, 'basR_G53W=POINT': 1, 'pmrB_P94L=POINT': 1, 'basR_G15R=POINT': 1, 'blaCMY-59': 1, 'basR_G53S=POINT': 1, 'pmrB_C84R=POINT': 1, 'pmrB_A159P=POINT': 1, 'basR_R81L=POINT': 1, 'pmrB_E121Q=POINT': 1, 'pmrB_L10R=POINT': 1, 'pmrB_L14R=POINT': 1, 'basR_A80V=POINT': 1, 'basR_G53C=POINT': 1, 'basR_G53A=POINT': 1, 'blaTEM-34': 1, 'marR_R94H=POINT': 1, 'erm(F)': 1, 'blaTEM-214': 1, 'gyrA_D87V=POINT': 1, 'nfsB_G192S=POINT': 1, 'blaCTX-M-199': 1, 'mcr-3.19': 1, 'blaFRI': 1, 'blaCTX-M-33': 1, \"aac(6')-Ib-cr10\": 1, 'qnrE1': 1, 'qnrVC': 1, 'blaCMY-166': 1, 'blaSHV-11': 1, 'cmlA': 1, 'ompF_G141D=POINT': 1, 'catB': 1, 'rmtF1': 1, 'qepA2': 1, 'blaTEM-71': 1, 'blaTEM-79': 1, 'qnrB17': 1, 'blaGES-7': 1, 'erm(52)': 1, 'blaSHV-210': 1, 'marR_R77C=POINT': 1, 'blaDHA-7': 1, 'erm(G)': 1, 'blaCTX-M-101': 1, 'blaCTX-M-22': 1, 'blaCTX-M-61': 1, 'blaCTX-M-28': 1, 'blaOXA-21': 1, 'blaACC': 1, 'blaOXA-204': 1, 'blaSHV-108': 1, 'qnrB91': 1, 'marR_R73C=POINT': 1, 'blaOXA-427': 1, 'marR_V45E=POINT': 1, 'rmtE2': 1, 'blaNDM-24': 1, 'blaOXA-17': 1, 'blaGES-6': 1, 'blaTEM-191': 1, 'blaCMY-146': 1, 'blaCMY-148': 1, 'marR_L78M=POINT': 1, 'blaSHV-44': 1, 'dfrA30': 1, 'blaTEM-63': 1, 'blaTEM-242': 1, 'blaOXA-58': 1, 'mcr-1.9': 1, 'blaNDM-19': 1, 'blaNDM-13': 1, 'dfrA3': 1, 'blaTEM-238': 1, 'nfsB_G192D=POINT': 1, 'blaNDM-16b': 1, 'blaCTX-M-98': 1, 'qnrB77': 1, 'blaCMY-141': 1, '16S_G926T=POINT': 1, 'blaTEM-198': 1, 'mcr-5': 1, '23S_G2057A=POINT': 1, 'nfsA_K141STOP=POINT': 1, 'blaCTX-M-136': 1, 'blaCTX-M-5': 1, 'blaCTX-M-206': 1, 'blaCMY-32': 1, 'blaTEM-29': 1, 'blaCTX-M-88': 1, 'rpoB_I572L=POINT': 1, 'gyrA_G81D=POINT': 1, 'mcr-3.20': 1, 'blaDHA': 1, 'rpoB_L511Q=POINT': 1, 'blaIMP': 1, 'parE_D476N=POINT': 1, 'blaNDM-27': 1, 'dfrG': 1, 'catA': 1, \"aac(6')-Ie/aph(2'')-Ia\": 1, 'basR_S39I=POINT': 1, \"aph(3')-IIIa\": 1, 'pmrB_G206D=POINT': 1, 'mcr-4.2': 1, 'blaOXA-23': 1, 'blaACT': 1, 'erm(53)': 1, 'blaCTX-M-71': 1, 'blaCTX-M-189': 1, 'blaCMY-43': 1, 'blaCTX-M-157': 1, 'pmrB_P94S=POINT': 1, 'qnrD': 1, 'blaNDM-20': 1, 'qnrB52': 1, 'blaCTX-M-73': 1, 'blaCTX-M-90': 1, 'basR_G53V=POINT': 1, 'gyrA_G81C=POINT': 1, 'tet(X2)': 1, 'aadS': 1, 'aacA37': 1, 'blaPDC': 1, 'blaOXA-488': 1, 'blaCTX-M-130': 1, '23S_C2611T=POINT': 1, 'pmrB_P94Q=POINT': 1, 'tet(K)': 1, 'mcr-4.6': 1, \"aac(6')-Ib-cr4\": 1, 'blaMOX-9': 1, 'estX/sat2': 1, 'blaIMP-11': 1, 'blaCMY-133': 1, 'blaCTX-M-186': 1, 'blaCMY-145': 1, 'tet(J)': 1, 'dfrE': 1, 'rpoB_P564L=POINT': 1, 'blaCTX-M-190': 1, \"aac(6')-Ib-cr9\": 1, 'tet(Q)': 1, '16S_T1406A=POINT': 1, 'blaCTX-M-79': 1, 'aadE': 1, 'rpoB_T563P=POINT': 1, 'blaVEB-25': 1, 'dfrA33': 1, 'blaVIM-2': 1, 'blaVEB-9': 1, 'blaOXA-20': 1, 'blaCTX-M-218': 1, 'fosL1': 1, 'mcr-1.27': 1, 'blaCTX-M-216': 1, 'aphA16': 1, 'qnrA': 1, 'dfrA9': 1, 'pmrB_P94A=POINT': 1, 'emrR_L64R=POINT': 1, 'blaCMY-132': 1, 'blaCMY-62': 1, 'tet(32)': 1, 'blaCMY-154': 1, 'blaTEM-28': 1, 'blaCTX-M-232': 1, 'blaTEM-219': 1, 'fosL2': 1, 'tet(39)': 1, 'rmtG': 1, 'blaCTX-M-12': 1, 'tufA_R334C=POINT': 1, 'tufA_Y161N=POINT': 1, 'blaTEM-122': 1, 'blaCTX-M-165': 1, '16S_G926A=POINT': 1, 'ompF_L15STOP=POINT': 1, 'blaVIM': 1, 'aadA9': 1, 'mcr-1.28': 1, 'blaCMY-162': 1, 'msr(A)': 1, 'blaR1': 1, 'tet(38)': 1, 'parE_I444F=POINT': 1, 'blaCTX-M-240': 1, 'blaTEMp_G162T=POINT': 1, 'gyrA_D82G=POINT': 1, 'blaTEM-183': 1, 'rpoB_I572F=POINT': 1, 'gyrA_A84P=POINT': 1, 'blaCTX-M-215': 1, 'blaCMY-60': 1, 'mcr-3.39': 1, 'toprJ': 1, 'tmexC': 1, 'tmexD': 1, 'tmexC3': 1, 'toprJ1': 1, 'mcr-3.29': 1, 'mcr-1.32': 1, 'tet(O)': 1, 'blaTEM-237': 1, 'blaPER-7': 1, 'blaCTX-M-191': 1, 'blaCTX-M-127': 1, 'blaCMY-131': 1, 'blaTEM-36': 1, 'cirA_R86S=POINT': 1, 'blaCMY-143': 1, 'blaCTX-M-231': 1, 'blaCTX-M-39': 1, 'blaARL': 1, 'fosB6': 1, 'blaOXA-484': 1, 'blaOXA-162': 1, 'mcr-4': 1, 'mcr-4.5': 1, 'mcr-4.1': 1, 'blaCMY-153': 1, 'mcr-5.3': 1, 'cmlA10': 1, 'dfrB3': 1, 'blaSHV-30': 1, 'catB9': 1, 'rmtF': 1, 'blaCTX-M-62': 1, 'blaCTX-M-44': 1, 'blaVEB-3': 1, 'parC_A85T=POINT': 1, 'blaEC-13': 1, 'blaSHV-31': 1, 'blaADC-265': 1, 'blaOXA-832': 1, 'tetB(58)': 1, 'tetA(58)': 1, 'vgbC': 1, 'blaOXA-283': 1, 'blaOXA-1181': 1, 'mcr-2.1': 1, 'blaTEM-168': 1, 'blaCTX-M-235': 1, 'tufA_R231V=POINT': 1, 'blaFONA': 1, 'blaKPC-49': 1, 'blaCMH': 1, 'cepA': 1, \"aph(7'')-Ia\": 1, 'blaTEM-231': 1, 'blaTEM-141': 1, 'blaCTX-M-196': 1, 'blaCTX-M-105': 1, 'blaCTX-M-19': 1, 'aadA11': 1, 'vanH-A': 1, 'vanS-A': 1, 'vga(A)': 1, 'blaGES': 1, 'blaGES-20': 1, 'blaCTX-M-42': 1, 'blaOXA-347': 1, 'blaTEM-132': 1, 'fosE': 1, \"aac(6')-31\": 1, \"ant(3'')-IIa\": 1, 'fosA7.7': 1, 'blaCMY-75': 1, 'blaCTX-M-56': 1, 'sat3': 1, \"aph(3')-Id\": 1, 'erm(49)': 1, 'blaTEM-83': 1, 'dfrB2': 1, 'aac(3)-If': 1, 'blaTEM-150': 1, 'blaCTX-M-82': 1, 'blaMOX': 1, 'blaCMY-27': 1, 'blaCTX-M-121': 1, 'lsa(A)': 1, 'tet(L)': 1, 'blaCTX-M-137': 1, 'blaPER-1': 1, 'blaNDM-3': 1, 'lsa(C)': 1, 'msr(D)': 1, 'tetB(60)': 1, 'mef(A)': 1, 'blaCTX-M-178': 1, 'blaCTX-M-255': 1, 'blaCTX-M-40': 1, 'qnrB38': 1, 'mcr-1.20': 1, 'blaL1': 1, 'blaCTX-M-140': 1, 'blaSHV-120': 1, \"aac(6')-Iai\": 1, 'tet(W)': 1, 'blaTEM-76': 1, 'ant(9)-Ia': 1, 'erm(A)': 1, 'qnrD2': 1, 'spw': 1, 'ant(6)-Ia': 1, 'lnu(B)': 1, 'str': 1, 'lsa(E)': 1, 'cmx': 1, 'blaGES-11': 1, 'blaNDM-36': 1, 'blaNDM-37': 1, 'blaACC-1d': 1, 'blaADC-2': 1, 'blaCTX-M-223': 1, 'blaCMY-10': 1, 'blaOXA-392': 1, 'blaCTX-M-243': 1, 'blaTEM-77': 1, 'mcr-2.8': 1, 'blaCMY-173': 1, 'dfrA35': 1, 'blaIMI-2': 1, \"aac(6')-Iq\": 1, 'mcr-1.11': 1, 'tmexD1': 1, 'tmexC1': 1, 'blaNDM-15': 1, 'blaIMP-64': 1, 'blaTEM-251': 1, 'mcr-3.21': 1, 'blaNDM-33': 1, 'catB1': 1, 'blaIMP-38': 1, 'fosC2': 1, 'blaNDM-29': 1, 'blaTEM-234': 1, 'aacA43': 1, 'blaIMP-18': 1, 'blaKPC-45': 1, 'tmexD3': 1, 'mcr-1.12': 1, 'mcr-1.31': 1, 'mcr-1.34': 1, 'blaIMP-8': 1, 'fabI_G93A=POINT': 1, 'blaACC-1a': 1, 'qnrB9': 1, 'blaNDM-39': 1, 'blaCTX-M-58': 1, 'blaFOX-14': 1, 'blaTEM-185': 1, 'blaNDM-12': 1, 'blaSED': 1, 'blaCTX-M-193': 1, 'blaSHV-161': 1, 'blaSHV-154': 1, 'marR_V96E=POINT': 1, 'rmtB4': 1, 'blaKPC-21': 1, 'blaCMY-156': 1, 'blaOXA-245': 1, 'cblA': 1, 'blaCMY-172': 1, 'mcr-1.8': 1, 'blaOXA-1213': 1, 'blaVIM-86': 1, 'blaCMY-13': 1, 'blaPER-3': 1, 'blaCTX-M-252': 1, \"aac(6')-29\": 1, 'blaKPC-6': 1, 'gyrA_A196E=POINT': 1, 'mcr-1.33': 1, 'msr(C)': 1, \"aac(6')-I\": 1, 'rmtB2': 1, 'marR_E31STOP=POINT': 1, 'blaCTX-M-143': 1, 'blaNDM-48': 1, 'blaGES-1': 1, 'blaGES-2': 1, 'qnrA6': 1, 'blaSHV-26': 1, 'blaCTX-M-169': 1, 'blaIMP-59': 1, 'blaVIM-23': 1, 'blaTEM-143': 1, 'aacA56': 1, 'blaOXA-1041': 1, 'blaCTX-M-63': 1, 'cfxA': 1, 'nimE': 1, 'satA': 1, 'oqxB32': 1, 'vmlR': 1, 'blaCMY-25': 1, 'blaOXA-1042': 1, 'blaTEM-80': 1, 'blaTEM-81': 1, 'blaCTX-M-150': 1, 'blaCMY-121': 1, 'qnrE2': 1, 'blaCTX-M-170': 1, 'blaCTX-M-195': 1, 'blaMIR-22': 1, 'blaCTX-M-96': 1, 'blaCTX-M-214': 1, 'blaTEM-70': 1, 'blaKPC-31': 1, 'blaSHV-49': 1, 'blaOXA-926': 1, 'blaCARB-12': 1, 'tufA_L121Q=POINT': 1, 'mcr-4.3': 1, 'gar': 1, 'ampC_C-42G=POINT': 1, 'blaOXA-101': 1, 'blaCMY-8b': 1, 'blaCTX-M-25': 1, 'blaCTX-M-202': 1, 'blaCTX-M-35': 1, 'parC_E84R=POINT': 1, \"aac(6')\": 1, 'marR_R73S=POINT': 1, 'blaDHA-6': 1, 'vanH-B': 1, 'vanX-B': 1, 'vanB': 1, 'vanR-B': 1, 'vanY-B': 1, 'vanS-B': 1, 'vanW-B': 1, 'dfrA45': 1, 'blaVEB-8': 1, 'tetB(P)': 1, 'blaCFE': 1, 'blaSRT-2': 1, 'blaNDM-55': 1, 'blaCMY-178': 1, 'blaCTX-M-102': 1, 'blaOXA-1205': 1, 'blaTEM-240': 1, 'blaNDM-35': 1, 'blaTEM-26': 1, 'blaOXA-1207': 1, 'blaOXA-24': 1, 'gyrA_A84V=POINT': 1, \"aph(3')-IIb\": 1, 'blaOXA-851': 1, 'crpP': 1, 'catB7': 1, 'blaACT-15': 1, 'blaCMY-101': 1, 'blaNDM-57': 1, 'qnrB65': 1, 'blaIMP-66': 1, 'blaCTX-M-67': 1, 'ftsI_A498T=POINT': 1, 'blaTEM-236': 1, 'blaDHA-27': 1, 'blaCTX-M-200': 1, 'blaCTX-M-269': 1, 'blaCTX-M-30': 1, 'blaCTX-M-52': 1, 'blaKLUC-3': 1, 'blaOXA-932': 1, 'blaCTX-M-38': 1, 'blaNDM-56': 1, '16S_C1066T=POINT': 1, 'blaVEB': 1, 'rpoB_I572N=POINT': 1, 'vga(A)-LC': 1, 'blaFOX': 1, 'parC_S80Y=POINT': 1, 'blaACC-1c': 1, 'blaCTX-M-192': 1, 'blaCMY-185': 1, 'blaCTX-M-220': 1, 'fosI': 1, \"ant(4')-IIb\": 1, 'blaCTX-M-238': 1, 'blaSHV-8': 1, 'blaSHV-102': 1, 'blaCTX-M-36': 1, 'blaOXA-72': 1, 'blaEC-18': 1, 'blaSHV-230': 1, 'blaTEM-244': 1, 'blaTEMp_C141G=POINT': 1, 'blaVIM-24': 1, 'rmtH': 1, 'tetA(P)': 1, 'blaNDM-26': 1, 'mcr-8.1': 1, 'blaCMY-82': 1, \"aph(3')-I(H957)\": 1, 'blaOXY-1-1': 1})\n",
      "Vocab()\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import typing\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer \n",
    "\n",
    "def make_vocabulary(dataset: pd.DataFrame):\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    SEP = '[SEP]'\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    token_list = Counter()\n",
    "    data = dataset.copy()\n",
    "\n",
    "    location_tokens = list(dict.fromkeys(list(chain(list(data['location'])))))\n",
    "    year_tokens = list(dict.fromkeys(list(chain(list(data['year'])))))\n",
    "    genes_tokens = list(dict.fromkeys(list(chain(*data['genes']))))\n",
    "   \n",
    "    token_list.update(map(str, year_tokens))\n",
    "    token_list.update(map(str, location_tokens))\n",
    "    token_list.update(map(str, genes_tokens))\n",
    "    print(token_list)\n",
    "    vocabulary = vocab(token_list,specials = [CLS, PAD, MASK, SEP, UNK])\n",
    "    print(vocabulary)\n",
    "    return vocabulary\n",
    "vocabulary = make_vocabulary(NCBI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = make_vocabulary(NCBI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
