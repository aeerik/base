{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from itertools import chain \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lokalt\n",
    "data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saga\n",
    "data_dir = \"/home/aeerik/data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import data_loader\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "NCBI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import make_vocabulary\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import NCBIDataset\n",
    "from build_vocabulary import make_vocabulary\n",
    "from data_preprocessing import data_loader\n",
    "\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "\n",
    "max_length = 20\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class JointEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, vocab_size, max_length, drop_prob):\n",
    "        super(JointEmbedding, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.drop_prob = drop_prob\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.token_emb = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\t\n",
    "        self.norm = nn.LayerNorm(self.embedding_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        token_embedding = self.token_emb(input_tensor)\n",
    "        token_embedding = self.norm(token_embedding)\n",
    "        token_embedding = self.dropout(token_embedding)\n",
    "\n",
    "        return token_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3906, -0.0000,  1.5145, -0.0000,  1.7002,  1.1402, -0.0000, -0.9546,\n",
       "          1.3977,  1.9938,  0.4370,  0.2733, -0.0000,  1.3827,  0.7178, -0.9676,\n",
       "         -1.1515, -1.2852, -1.8969, -1.5310, -0.8330,  0.5568,  0.2334, -0.6911,\n",
       "         -0.9162, -0.0750, -0.0000,  2.3868,  0.3402,  0.5523,  1.0168,  0.4211],\n",
       "        [ 0.6452,  0.0000,  0.0568, -0.5182, -0.0000,  1.4908,  2.0379, -0.0504,\n",
       "         -0.0000,  0.0000,  0.0790, -2.6247, -0.5794,  1.0984,  0.6177, -0.9772,\n",
       "          1.2881,  0.1932, -0.1510, -0.3476,  1.4219,  0.7506, -0.0000, -0.0000,\n",
       "         -1.6010,  1.2559, -0.4444, -3.2181, -0.3482,  1.6645,  0.0000,  0.8628],\n",
       "        [ 1.9827,  0.2361,  1.7897, -2.7533, -0.0000,  0.9367, -0.2634,  0.0000,\n",
       "         -1.1148,  0.0227,  1.5904,  1.7657,  0.4263, -1.4521,  1.1401, -1.3388,\n",
       "          0.4797,  0.5975,  0.1636, -2.2488,  1.7180, -0.3915, -0.7171, -0.7517,\n",
       "         -0.0000, -0.6126, -0.0949, -1.5568,  0.6963,  1.1347,  0.3446, -0.0000],\n",
       "        [-1.4923,  0.0000,  0.2472, -1.3585,  0.9248,  0.4235, -0.5308, -0.0331,\n",
       "         -1.0439,  2.2338, -0.6334,  1.7594, -2.0141, -0.9644,  0.0052,  1.7081,\n",
       "          0.2607, -0.8450, -1.2157, -1.3042, -1.0953,  3.5175,  0.2710, -0.1087,\n",
       "         -0.7512,  0.0000, -0.4024, -1.6938,  0.0000,  0.0000,  0.1747, -0.0000],\n",
       "        [-1.4923,  0.5446,  0.2472, -1.3585,  0.9248,  0.4235, -0.5308, -0.0000,\n",
       "         -1.0439,  2.2338, -0.6334,  1.7594, -2.0141, -0.9644,  0.0052,  1.7081,\n",
       "          0.2607, -0.8450, -1.2157, -0.0000, -0.0000,  3.5175,  0.2710, -0.1087,\n",
       "         -0.7512,  1.8552, -0.0000, -1.6938,  1.2489,  0.6444,  0.1747, -0.0000],\n",
       "        [-0.1407,  0.0000, -0.8341,  0.2144, -1.9850, -0.0808,  2.2454, -0.6873,\n",
       "         -0.3415, -1.3018, -0.4943,  0.0000, -0.0036,  1.5486,  0.3568,  0.0000,\n",
       "         -0.9532,  0.0100, -0.0000,  1.1809,  2.1728, -0.4200, -0.0000,  2.0701,\n",
       "          0.0000, -1.6408,  1.5152, -0.0000, -0.0000,  1.4604,  0.0000, -2.3983],\n",
       "        [-1.4923,  0.5446,  0.2472, -1.3585,  0.0000,  0.4235, -0.0000, -0.0331,\n",
       "         -1.0439,  2.2338, -0.0000,  0.0000, -0.0000, -0.9644,  0.0052,  1.7081,\n",
       "          0.0000, -0.0000, -1.2157, -1.3042, -1.0953,  3.5175,  0.2710, -0.1087,\n",
       "         -0.0000,  0.0000, -0.0000, -1.6938,  1.2489,  0.6444,  0.1747, -0.3323],\n",
       "        [ 0.6452,  1.8433,  0.0000, -0.5182, -1.2863,  1.4908,  0.0000, -0.0504,\n",
       "         -1.1669,  0.0000,  0.0790, -2.6247, -0.0000,  0.0000,  0.6177, -0.9772,\n",
       "          1.2881,  0.1932, -0.1510, -0.3476,  1.4219,  0.7506, -1.1629, -1.5169,\n",
       "         -1.6010,  1.2559, -0.4444, -3.2181, -0.3482,  1.6645,  0.5680,  0.8628],\n",
       "        [ 0.6452,  0.0000,  0.0568, -0.5182, -1.2863,  1.4908,  2.0379, -0.0000,\n",
       "         -1.1669,  0.0000,  0.0790, -2.6247, -0.5794,  1.0984,  0.6177, -0.9772,\n",
       "          1.2881,  0.1932, -0.1510, -0.3476,  1.4219,  0.7506, -1.1629, -1.5169,\n",
       "         -1.6010,  1.2559, -0.4444, -3.2181, -0.3482,  1.6645,  0.5680,  0.8628],\n",
       "        [ 0.6452,  1.8433,  0.0568, -0.5182, -0.0000,  1.4908,  2.0379, -0.0504,\n",
       "         -1.1669,  0.1193,  0.0790, -0.0000, -0.0000,  1.0984,  0.6177, -0.9772,\n",
       "          0.0000,  0.0000, -0.1510, -0.0000,  1.4219,  0.7506, -1.1629, -0.0000,\n",
       "         -0.0000,  1.2559, -0.4444, -3.2181, -0.0000,  1.6645,  0.5680,  0.8628],\n",
       "        [ 0.6452,  1.8433,  0.0568, -0.5182, -1.2863,  1.4908,  2.0379, -0.0504,\n",
       "         -1.1669,  0.1193,  0.0790, -2.6247, -0.5794,  1.0984,  0.6177, -0.9772,\n",
       "          1.2881,  0.1932, -0.1510, -0.0000,  1.4219,  0.7506, -1.1629, -1.5169,\n",
       "         -1.6010,  1.2559, -0.4444, -0.0000, -0.3482,  1.6645,  0.5680,  0.8628],\n",
       "        [ 0.6452,  1.8433,  0.0568, -0.5182, -1.2863,  0.0000,  2.0379, -0.0504,\n",
       "         -1.1669,  0.1193,  0.0790, -2.6247, -0.0000,  1.0984,  0.6177, -0.9772,\n",
       "          1.2881,  0.0000, -0.1510, -0.3476,  1.4219,  0.7506, -1.1629, -1.5169,\n",
       "         -0.0000,  1.2559, -0.4444, -3.2181, -0.0000,  1.6645,  0.5680,  0.8628],\n",
       "        [ 0.6452,  1.8433,  0.0568, -0.5182, -0.0000,  1.4908,  2.0379, -0.0000,\n",
       "         -1.1669,  0.1193,  0.0790, -2.6247, -0.5794,  1.0984,  0.6177, -0.0000,\n",
       "          0.0000,  0.1932, -0.1510, -0.0000,  1.4219,  0.7506, -1.1629, -0.0000,\n",
       "         -1.6010,  1.2559, -0.4444, -3.2181, -0.3482,  1.6645,  0.5680,  0.8628],\n",
       "        [ 0.6452,  1.8433,  0.0568, -0.5182, -1.2863,  0.0000,  0.0000, -0.0504,\n",
       "         -1.1669,  0.1193,  0.0000, -2.6247, -0.0000,  1.0984,  0.6177, -0.9772,\n",
       "          1.2881,  0.1932, -0.0000, -0.3476,  1.4219,  0.7506, -1.1629, -1.5169,\n",
       "         -1.6010,  1.2559, -0.0000, -3.2181, -0.0000,  0.0000,  0.0000,  0.8628],\n",
       "        [ 0.6452,  1.8433,  0.0568, -0.5182, -1.2863,  1.4908,  2.0379, -0.0504,\n",
       "         -1.1669,  0.1193,  0.0790, -2.6247, -0.5794,  1.0984,  0.0000, -0.9772,\n",
       "          1.2881,  0.1932, -0.1510, -0.3476,  0.0000,  0.7506, -1.1629, -1.5169,\n",
       "         -1.6010,  1.2559, -0.4444, -0.0000, -0.3482,  1.6645,  0.5680,  0.8628],\n",
       "        [ 0.0000,  1.8433,  0.0000, -0.5182, -0.0000,  1.4908,  2.0379, -0.0504,\n",
       "         -0.0000,  0.1193,  0.0790, -2.6247, -0.5794,  1.0984,  0.6177, -0.9772,\n",
       "          1.2881,  0.1932, -0.1510, -0.3476,  1.4219,  0.7506, -1.1629, -1.5169,\n",
       "         -0.0000,  1.2559, -0.4444, -3.2181, -0.0000,  1.6645,  0.5680,  0.0000],\n",
       "        [ 0.6452,  0.0000,  0.0568, -0.5182, -1.2863,  1.4908,  2.0379, -0.0000,\n",
       "         -1.1669,  0.1193,  0.0790, -2.6247, -0.5794,  1.0984,  0.6177, -0.9772,\n",
       "          1.2881,  0.1932, -0.1510, -0.3476,  1.4219,  0.7506, -1.1629, -1.5169,\n",
       "         -1.6010,  1.2559, -0.4444, -0.0000, -0.0000,  1.6645,  0.5680,  0.8628],\n",
       "        [ 0.6452,  1.8433,  0.0568, -0.5182, -0.0000,  0.0000,  2.0379, -0.0504,\n",
       "         -0.0000,  0.1193,  0.0000, -2.6247, -0.0000,  1.0984,  0.6177, -0.9772,\n",
       "          1.2881,  0.1932, -0.0000, -0.3476,  1.4219,  0.7506, -1.1629, -1.5169,\n",
       "         -1.6010,  1.2559, -0.4444, -3.2181, -0.0000,  1.6645,  0.5680,  0.0000],\n",
       "        [ 0.6452,  1.8433,  0.0568, -0.5182, -1.2863,  1.4908,  0.0000, -0.0504,\n",
       "         -1.1669,  0.1193,  0.0790, -2.6247, -0.5794,  0.0000,  0.6177, -0.9772,\n",
       "          1.2881,  0.1932, -0.1510, -0.3476,  1.4219,  0.0000, -1.1629, -1.5169,\n",
       "         -1.6010,  1.2559, -0.4444, -3.2181, -0.0000,  1.6645,  0.5680,  0.0000],\n",
       "        [ 0.0000,  1.8433,  0.0568, -0.5182, -1.2863,  1.4908,  2.0379, -0.0504,\n",
       "         -1.1669,  0.1193,  0.0790, -0.0000, -0.5794,  0.0000,  0.6177, -0.9772,\n",
       "          1.2881,  0.0000, -0.1510, -0.3476,  1.4219,  0.7506, -0.0000, -1.5169,\n",
       "         -1.6010,  1.2559, -0.4444, -0.0000, -0.3482,  1.6645,  0.5680,  0.8628]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embedding import JointEmbedding\n",
    "embedding_dim = 32\n",
    "voca_size = len(vocabulary)\n",
    "max_length = 20\n",
    "drop_prob = 0.2\n",
    "\n",
    "emb_test = JointEmbedding(embedding_dim, voca_size, max_length, drop_prob)\n",
    "emb_test.forward(test_set[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out, drop_prob):\n",
    "        super(AttentionHead, self).__init__()\n",
    "\n",
    "        self.dim_inp = dim_inp\n",
    "        self.drop_prob = drop_prob\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.q = nn.Linear(dim_inp, dim_out)\n",
    "        self.k = nn.Linear(dim_inp, dim_out)\n",
    "        self.v = nn.Linear(dim_inp, dim_out)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)\n",
    "\n",
    "        scale = query.size(1) ** 0.5\n",
    "        scores = torch.matmul(query, key.transpose(-1, -2)) / scale\n",
    "\n",
    "        scores = scores.masked_fill_(attention_mask, -1e9)\n",
    "        attn = f.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, value)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, dim_inp, dim_out,drop_prob):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(dim_inp, dim_out,drop_prob) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(dim_out * num_heads, dim_inp)\n",
    "        self.norm = nn.LayerNorm(dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        s = [head(input_tensor, attention_mask) for head in self.heads]\n",
    "        scores = torch.cat(s, dim=-1)\n",
    "        scores = self.linear(scores)\n",
    "        return self.norm(scores)\n",
    "    \n",
    "attention_test = AttentionHead(32, 32, 0.2)\n",
    "attention_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])\n",
    "\n",
    "multihead_test = MultiHeadAttention(8, 32, 32,0.2)\n",
    "multihead_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "#dropout \n",
    "#num heads\n",
    "#dim in, dim out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out, attention_heads, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.attention_heads = attention_heads\n",
    "        self.dim_inp = dim_inp\n",
    "        self.dim_out = dim_out\n",
    "        \n",
    "        \n",
    "        self.attention = MultiHeadAttention(self.attention_heads, self.dim_inp, self.dim_out, self.dropout_prob)  # batch_size x sentence size x dim_inp\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(self.dim_inp, self.dim_out),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.dim_out, self.dim_inp),\n",
    "            nn.Dropout(self.dropout_prob)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(self.dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        context = self.attention(input_tensor, attention_mask)\n",
    "        res = self.feed_forward(context)\n",
    "        return self.norm(res)\n",
    "\n",
    "encoder_test = Encoder(32, 32, 8, 0.2)\n",
    "encoder_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, max_length, dim_inp, dim_out, attention_heads, num_encoders, dropout_prob):\n",
    "        super(BERT, self).__init__()\n",
    "        self.attention_heads = attention_heads\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_inp = dim_inp  \n",
    "        self.dim_out = dim_out\n",
    "        self.num_encoders = num_encoders\n",
    "        self.dropout_prob = dropout_prob  \n",
    "\n",
    "        self.embedding = JointEmbedding(self.dim_inp, self.vocab_size, self.max_length, self.dropout_prob)\n",
    "        self.encoders = nn.ModuleList([Encoder(self.dim_inp, self.dim_out, self.attention_heads, self.dropout_prob) for _ in range(self.num_encoders)])\n",
    "\n",
    "        self.token_prediction_layer = nn.Linear(self.dim_inp, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        embedded = self.embedding(input_tensor)\n",
    "        for layer in self.encoders:\n",
    "            embedded = layer(embedded, attention_mask)\n",
    "\n",
    "        token_predictions = self.token_prediction_layer(embedded)\n",
    "        return self.softmax(token_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bert_test \u001b[38;5;241m=\u001b[39m \u001b[43mBERT\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m bert_test\u001b[38;5;241m.\u001b[39mforward(test_set[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], test_set[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m2\u001b[39m])\n",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m, in \u001b[0;36mBERT.__init__\u001b[1;34m(self, vocab_size, max_length, dim_inp, dim_out, attention_heads, num_encoders, dropout_prob)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;241m=\u001b[39m dropout_prob  \n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m JointEmbedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_inp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([Encoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_inp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_encoders)])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_prediction_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_inp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLogSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;241m=\u001b[39m dropout_prob  \n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m JointEmbedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_inp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mEncoder\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_inp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_encoders)])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_prediction_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_inp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLogSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Encoder' is not defined"
     ]
    }
   ],
   "source": [
    "bert_test = BERT(len(vocabulary), 20, 32, 32, 8, 2, 0.2)\n",
    "bert_test.forward(test_set[1][0], test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0241e-02,  1.5273e-01,  2.6295e-01,  6.6555e-01, -1.0036e-01,\n",
       "         -6.5821e-01, -1.2605e-02,  7.7466e-01,  4.7497e-01, -8.1123e-01,\n",
       "         -5.8339e-01, -6.4419e-01, -1.4982e-01,  2.8228e-01,  6.2414e-01,\n",
       "          5.8298e-01, -7.0692e-01,  1.1555e-03, -4.7954e-01, -1.7100e-01,\n",
       "          3.6572e-01,  5.3296e-02, -1.7360e-01,  3.0050e-01, -9.3018e-01,\n",
       "         -1.6014e-01,  2.2848e-02, -3.6400e-02,  6.6105e-01, -4.2749e-01,\n",
       "         -5.0100e-01,  1.4583e-01],\n",
       "        [-2.6248e-01, -1.3555e-01,  1.8824e-01,  6.2918e-01, -3.6234e-03,\n",
       "         -4.0952e-01, -1.8347e-01,  7.8743e-01,  5.0052e-01, -6.9197e-01,\n",
       "         -6.6147e-01, -6.2018e-01, -5.7642e-02,  2.5905e-01,  8.6495e-01,\n",
       "          5.9880e-01, -4.1036e-01,  9.8861e-02, -3.3702e-01, -5.4992e-02,\n",
       "          3.4315e-01, -1.9010e-01, -1.3948e-01,  2.2546e-01, -7.8482e-01,\n",
       "         -9.8081e-02,  7.9229e-02,  2.0034e-01,  7.7362e-01, -5.6719e-01,\n",
       "         -4.7929e-01,  3.8551e-01],\n",
       "        [-4.7225e-01, -2.1066e-01,  2.0930e-01,  5.0922e-01,  1.4735e-01,\n",
       "         -2.9070e-01, -2.8593e-01,  5.7339e-01,  5.1089e-01, -3.3328e-01,\n",
       "         -6.0757e-01, -6.0098e-01,  1.2605e-01,  1.3218e-01,  8.1880e-01,\n",
       "          5.4696e-01, -3.4045e-01,  6.7047e-02, -2.9025e-01, -4.3714e-02,\n",
       "          5.0209e-01, -2.4841e-01, -1.2479e-01, -1.2770e-01, -5.7078e-01,\n",
       "          3.8940e-02,  2.2092e-01,  2.8241e-01,  7.9749e-01, -5.7999e-01,\n",
       "         -4.4701e-01,  4.9420e-01],\n",
       "        [ 5.7179e-02,  5.1271e-01,  3.5021e-01,  3.0296e-01, -9.8754e-02,\n",
       "         -4.8651e-01,  4.0590e-01,  1.1205e-01,  2.4458e-01, -3.5387e-01,\n",
       "          7.2005e-02, -4.0586e-01,  1.8311e-01, -3.8023e-02,  1.2511e-01,\n",
       "          3.7142e-01, -9.5499e-01, -1.2099e-01, -4.5379e-01, -2.6003e-01,\n",
       "          8.6455e-02,  5.5483e-01, -1.9822e-02,  1.5740e-01, -7.6532e-01,\n",
       "          7.8971e-02, -3.6142e-01,  5.4396e-02,  1.5313e-01,  2.1362e-01,\n",
       "         -1.4496e-01, -3.8856e-01],\n",
       "        [-2.5144e-01,  1.9842e-01,  2.0050e-01,  2.2385e-01, -9.1819e-02,\n",
       "         -3.1744e-01,  1.8931e-01,  1.8843e-01,  2.1126e-01, -3.7385e-01,\n",
       "         -9.7763e-02, -3.3263e-01,  1.8323e-01, -6.1804e-03,  4.7819e-01,\n",
       "          2.4016e-01, -6.1146e-01, -9.9930e-02, -2.8578e-01, -3.0439e-01,\n",
       "          2.1525e-02,  1.7285e-01, -4.3746e-02,  3.1054e-01, -5.7107e-01,\n",
       "         -3.0686e-02, -1.8791e-01,  2.4625e-01,  1.1293e-01, -9.7929e-02,\n",
       "         -6.1647e-02, -2.6196e-02],\n",
       "        [-2.3625e-01,  2.1118e-02,  2.8285e-01,  7.6164e-01, -5.0108e-02,\n",
       "         -5.9435e-01, -9.5469e-02,  8.7645e-01,  5.9822e-01, -8.4829e-01,\n",
       "         -7.1484e-01, -7.5239e-01, -4.6718e-02,  3.0161e-01,  9.3192e-01,\n",
       "          7.2253e-01, -6.7189e-01,  7.5509e-02, -4.7506e-01, -1.3321e-01,\n",
       "          3.5857e-01, -4.6221e-02, -1.3385e-01,  3.1508e-01, -1.0801e+00,\n",
       "         -1.3079e-01, -2.2370e-02,  2.0133e-01,  8.6070e-01, -5.6959e-01,\n",
       "         -5.7640e-01,  2.8705e-01],\n",
       "        [ 7.1345e-02,  5.6456e-01,  3.8502e-01,  6.1145e-01, -1.5886e-01,\n",
       "         -7.7330e-01,  2.6574e-01,  4.6778e-01,  4.6093e-01, -6.7138e-01,\n",
       "         -3.3943e-01, -5.9945e-01,  1.2375e-02,  2.1826e-01,  3.5848e-01,\n",
       "          5.6624e-01, -1.0237e+00, -6.5990e-02, -5.7228e-01, -2.8227e-01,\n",
       "          1.5224e-01,  4.9808e-01,  1.6277e-04,  3.1851e-01, -1.2291e+00,\n",
       "         -1.4420e-01, -3.3170e-01,  3.7517e-04,  4.8964e-01, -1.2516e-01,\n",
       "         -4.8006e-01, -2.9427e-01],\n",
       "        [-4.0795e-02,  3.7023e-01,  3.2749e-01,  5.3145e-01, -1.7374e-01,\n",
       "         -6.2633e-01,  2.8238e-01,  5.1689e-01,  3.8214e-01, -7.6574e-01,\n",
       "         -2.6417e-01, -5.7878e-01,  4.3943e-02,  1.3589e-01,  5.5744e-01,\n",
       "          5.1636e-01, -9.6009e-01, -7.3843e-02, -5.1345e-01, -3.0913e-01,\n",
       "          1.1764e-01,  3.3819e-01, -9.6185e-02,  4.6095e-01, -1.0193e+00,\n",
       "         -8.8271e-02, -2.8810e-01,  1.1805e-01,  3.7175e-01, -1.2325e-01,\n",
       "         -2.9117e-01, -1.4390e-01],\n",
       "        [-2.0539e-01,  3.1592e-01,  3.8489e-01,  6.3369e-01, -3.9699e-02,\n",
       "         -6.9231e-01,  8.1633e-02,  5.5636e-01,  5.4698e-01, -6.0624e-01,\n",
       "         -4.7732e-01, -7.0898e-01,  1.0626e-01,  1.7056e-01,  6.3366e-01,\n",
       "          6.2804e-01, -9.4440e-01, -5.6736e-02, -5.5805e-01, -2.7164e-01,\n",
       "          3.7420e-01,  2.4751e-01, -8.6243e-02,  1.4945e-01, -1.0887e+00,\n",
       "         -5.9093e-02, -1.0573e-01,  1.3409e-01,  6.6376e-01, -3.3616e-01,\n",
       "         -4.9869e-01,  2.7532e-02],\n",
       "        [-2.3350e-01,  2.2877e-01,  3.5827e-01,  6.6609e-01, -3.6956e-02,\n",
       "         -6.6853e-01,  2.3474e-02,  6.4228e-01,  5.6465e-01, -6.6446e-01,\n",
       "         -5.4969e-01, -7.2605e-01,  7.1747e-02,  2.0237e-01,  7.2704e-01,\n",
       "          6.5059e-01, -8.7227e-01, -2.7002e-02, -5.3712e-01, -2.4331e-01,\n",
       "          3.8562e-01,  1.5548e-01, -1.0495e-01,  1.8409e-01, -1.0781e+00,\n",
       "         -7.7939e-02, -6.5212e-02,  1.5617e-01,  7.2005e-01, -4.1472e-01,\n",
       "         -5.1956e-01,  1.1603e-01],\n",
       "        [-2.8807e-01,  2.6363e-01,  3.7807e-01,  5.1991e-01, -1.5893e-02,\n",
       "         -5.8964e-01,  1.2761e-01,  4.4651e-01,  4.7840e-01, -5.1771e-01,\n",
       "         -3.3943e-01, -6.6826e-01,  2.0115e-01,  5.7183e-02,  6.4298e-01,\n",
       "          5.6019e-01, -9.5216e-01, -9.5474e-02, -5.2674e-01, -3.0663e-01,\n",
       "          3.7311e-01,  2.2861e-01, -1.2133e-01,  1.1804e-01, -9.1210e-01,\n",
       "          2.9065e-02, -9.0008e-02,  2.0197e-01,  5.3674e-01, -2.5136e-01,\n",
       "         -3.5050e-01,  5.2328e-02],\n",
       "        [-2.7223e-01,  1.7837e-01,  3.5159e-01,  6.7872e-01, -3.4943e-02,\n",
       "         -6.5772e-01,  2.9876e-03,  6.8719e-01,  5.7446e-01, -7.0293e-01,\n",
       "         -5.7750e-01, -7.4736e-01,  7.1556e-02,  2.0315e-01,  8.0256e-01,\n",
       "          6.6406e-01, -8.6216e-01, -2.2600e-02, -5.3573e-01, -2.4752e-01,\n",
       "          4.0221e-01,  1.0428e-01, -1.2815e-01,  2.0910e-01, -1.0663e+00,\n",
       "         -7.6521e-02, -3.9187e-02,  1.8372e-01,  7.3997e-01, -4.5459e-01,\n",
       "         -5.1298e-01,  1.7413e-01],\n",
       "        [-2.0375e-01,  2.9113e-01,  3.6774e-01,  6.4752e-01, -5.5157e-02,\n",
       "         -6.9107e-01,  6.7476e-02,  5.9799e-01,  5.4675e-01, -6.5398e-01,\n",
       "         -5.0788e-01, -7.0731e-01,  7.8104e-02,  1.9412e-01,  6.7203e-01,\n",
       "          6.2830e-01, -9.1434e-01, -4.5841e-02, -5.4772e-01, -2.6884e-01,\n",
       "          3.5362e-01,  2.1496e-01, -9.0234e-02,  2.0149e-01, -1.0990e+00,\n",
       "         -8.6241e-02, -1.0347e-01,  1.3820e-01,  6.6924e-01, -3.6840e-01,\n",
       "         -5.0342e-01,  5.1674e-02],\n",
       "        [-1.8290e-01, -1.0300e-01,  1.6035e-01,  5.3127e-01,  2.0007e-02,\n",
       "         -4.1148e-01, -2.1378e-01,  6.7143e-01,  4.2433e-01, -5.4023e-01,\n",
       "         -6.0426e-01, -5.2999e-01, -1.0166e-01,  2.3177e-01,  6.3801e-01,\n",
       "          4.7596e-01, -3.4532e-01,  4.8837e-02, -3.0460e-01, -5.7629e-02,\n",
       "          4.2067e-01, -1.9122e-01, -1.6291e-01,  7.9556e-02, -5.8858e-01,\n",
       "         -9.8388e-02,  2.0514e-01,  3.9946e-02,  6.7318e-01, -5.2776e-01,\n",
       "         -4.4614e-01,  3.6837e-01],\n",
       "        [-2.3192e-01,  2.6076e-01,  3.7275e-01,  6.5078e-01, -3.1626e-02,\n",
       "         -6.7513e-01,  4.5507e-02,  6.0225e-01,  5.6003e-01, -6.3071e-01,\n",
       "         -5.1732e-01, -7.2339e-01,  9.5308e-02,  1.8239e-01,  6.9217e-01,\n",
       "          6.4388e-01, -9.0637e-01, -4.0956e-02, -5.4748e-01, -2.5548e-01,\n",
       "          3.9108e-01,  1.9169e-01, -9.9677e-02,  1.5621e-01, -1.0762e+00,\n",
       "         -6.1345e-02, -7.6411e-02,  1.5238e-01,  7.0083e-01, -3.8198e-01,\n",
       "         -5.0927e-01,  8.5300e-02],\n",
       "        [ 2.5629e-02,  4.5166e-01,  3.5398e-01,  5.6105e-01, -8.4241e-02,\n",
       "         -7.5176e-01,  1.2832e-01,  4.5972e-01,  4.4447e-01, -5.4712e-01,\n",
       "         -3.9281e-01, -5.8765e-01, -1.6788e-02,  1.9293e-01,  3.0801e-01,\n",
       "          5.0149e-01, -9.1603e-01, -9.8604e-02, -5.4190e-01, -2.6920e-01,\n",
       "          3.5268e-01,  3.4637e-01, -8.1417e-02,  1.1908e-01, -9.8894e-01,\n",
       "         -1.1734e-01, -8.1322e-02, -1.0251e-01,  5.1423e-01, -2.2396e-01,\n",
       "         -4.8066e-01, -1.2489e-01],\n",
       "        [-1.3657e-01,  2.1069e-02,  9.3963e-02,  2.5205e-01,  7.1510e-02,\n",
       "         -1.5570e-01, -1.5427e-01,  1.7620e-01,  2.5983e-01, -3.5457e-02,\n",
       "         -3.1478e-01, -2.1453e-01,  3.6690e-02,  1.3500e-01,  2.1297e-01,\n",
       "          2.4705e-01, -7.4664e-02,  7.5323e-02, -1.0278e-01,  4.3696e-02,\n",
       "          1.3686e-01, -8.7322e-03,  7.7540e-02, -1.2618e-01, -3.9990e-01,\n",
       "         -4.5296e-02,  3.0382e-03,  9.5723e-02,  4.1527e-01, -2.5264e-01,\n",
       "         -3.2432e-01,  9.1390e-02],\n",
       "        [-5.6567e-02,  3.7712e-01,  3.4756e-01,  5.5428e-01, -5.5641e-02,\n",
       "         -7.1712e-01,  8.3247e-02,  4.7573e-01,  4.5704e-01, -5.2904e-01,\n",
       "         -4.1863e-01, -6.0719e-01,  1.2761e-02,  1.7355e-01,  3.8851e-01,\n",
       "          5.0617e-01, -8.8614e-01, -9.8211e-02, -5.2969e-01, -2.7119e-01,\n",
       "          3.9893e-01,  2.7259e-01, -1.0551e-01,  8.7363e-02, -9.3566e-01,\n",
       "         -9.4212e-02, -2.3849e-02, -5.6740e-02,  5.4169e-01, -2.7622e-01,\n",
       "         -4.6803e-01, -3.5267e-02],\n",
       "        [ 1.4153e-01,  5.1969e-01,  2.3964e-01,  2.2612e-01, -2.5857e-02,\n",
       "         -4.8313e-01,  1.3583e-01, -3.1846e-02,  2.1384e-01, -1.4261e-02,\n",
       "         -9.7271e-02, -2.1674e-01,  3.5098e-02,  9.1415e-02, -2.2256e-01,\n",
       "          1.8667e-01, -5.5473e-01, -9.8308e-02, -3.0566e-01, -1.5517e-01,\n",
       "          1.5256e-01,  4.4405e-01,  8.9830e-02, -1.4380e-01, -5.9868e-01,\n",
       "         -7.3908e-02, -1.3335e-01, -1.8454e-01,  1.9430e-01,  3.6635e-02,\n",
       "         -3.1976e-01, -3.3768e-01],\n",
       "        [-1.6641e-01,  3.7178e-01,  2.8717e-01,  4.6932e-01, -8.6283e-02,\n",
       "         -5.9987e-01,  9.0076e-02,  3.5112e-01,  4.1277e-01, -4.5264e-01,\n",
       "         -3.8499e-01, -4.8925e-01,  8.2769e-02,  1.8113e-01,  4.4887e-01,\n",
       "          4.1541e-01, -7.3459e-01, -6.8543e-02, -4.2018e-01, -2.8783e-01,\n",
       "          1.5316e-01,  2.6776e-01,  2.1881e-02,  2.1871e-01, -9.7658e-01,\n",
       "         -1.5976e-01, -1.7182e-01,  1.1504e-01,  4.3098e-01, -2.8358e-01,\n",
       "         -4.0808e-01, -7.3802e-02]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import AttentionHead\n",
    "\n",
    "attention_test = AttentionHead(32, 32, 0.2)\n",
    "attention_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4130, -0.0907,  0.1716, -0.5289, -0.8205, -1.7303,  1.2896, -0.5984,\n",
       "         -0.5620,  1.1284,  1.3726,  0.1151, -1.5881, -0.3556,  0.2278, -1.8129,\n",
       "         -0.3096,  0.9635, -0.1761, -0.8337,  1.0715,  1.9798, -0.8971,  0.8563,\n",
       "         -0.7656, -0.8133, -0.7476, -0.6008, -0.4917,  0.5151,  1.1399,  1.4786],\n",
       "        [ 1.3435, -0.9398, -0.1196, -1.3160, -0.5235, -1.0834, -0.4125, -0.2887,\n",
       "         -0.4767,  1.0614,  0.8628,  0.8140, -1.0553, -0.7731,  0.7286,  0.1749,\n",
       "         -0.4641,  2.0798, -0.4040, -1.3819,  0.6538,  1.6230,  0.1166,  1.1454,\n",
       "         -0.7040, -1.6622, -0.8966, -1.3125,  0.7005,  0.8835, -0.0376,  1.6638],\n",
       "        [ 1.3853, -1.0699, -0.0670, -1.0698, -0.4439, -1.5180,  0.2586, -0.1898,\n",
       "         -0.0194,  1.1598,  0.6623,  0.1378, -1.0881, -0.3579,  0.3343, -0.0998,\n",
       "         -0.2816,  2.0297, -0.1493, -1.2266,  1.1505,  1.4389, -0.1601,  1.1152,\n",
       "         -0.5965, -2.1915, -0.9558, -0.9647,  0.0598,  1.2299, -0.2373,  1.7252],\n",
       "        [ 1.4894, -1.1520, -0.3811, -1.4463, -0.1725, -0.8700,  0.3475, -0.4631,\n",
       "         -0.0704,  0.7499,  0.3112,  0.0428, -2.0577, -0.9804, -0.0816, -0.3660,\n",
       "          0.3495,  1.8344,  0.3621, -1.2763,  1.0491,  2.0741, -0.3797,  0.8328,\n",
       "         -0.6251, -1.2273, -0.5100, -0.3301, -0.5364,  1.0887,  0.4925,  1.9023],\n",
       "        [ 1.4319, -0.6647, -0.5904, -1.2488, -0.0186, -0.3080,  0.7379, -0.6594,\n",
       "         -0.5089,  0.1831,  0.3171,  0.1646, -2.2353, -0.5140, -0.0950, -0.5714,\n",
       "          0.1301,  1.8987,  0.2850, -0.9496,  1.2397,  1.4702, -0.2456,  0.5877,\n",
       "         -1.2052, -0.9442, -1.2116, -0.4558, -0.4168,  2.0688,  0.6123,  1.7163],\n",
       "        [ 1.8355, -0.8260,  0.4311, -1.2707,  0.0397, -0.9967,  0.1781, -0.0984,\n",
       "         -0.0454,  0.6619,  0.3060,  0.1489, -1.8643, -0.5744,  0.1151, -0.1472,\n",
       "         -0.2016,  2.5386, -0.5070, -1.4470,  0.5610,  1.4371, -0.1597,  0.9300,\n",
       "         -0.7691, -1.2884, -0.9337, -1.0373,  0.0392,  0.6651,  0.3143,  1.9654],\n",
       "        [ 1.8416, -0.8399, -0.2038, -1.0938, -0.5559, -0.6821,  0.7969, -0.9405,\n",
       "         -0.2919,  0.7051,  0.6231,  0.0573, -1.6381, -0.8317,  0.1858, -0.7298,\n",
       "         -0.2079,  1.4487,  0.0723, -1.0964,  0.9214,  1.7417, -0.5342,  1.0749,\n",
       "         -1.2018, -0.8633, -1.3578, -0.2353, -0.1490,  1.9482,  0.3867,  1.6495],\n",
       "        [ 1.3354, -1.4839, -0.2654, -1.3226, -0.2376, -0.8552, -0.3342,  0.1629,\n",
       "         -0.4235,  0.3137,  1.2751,  0.8131, -0.8952, -0.7300, -0.0178,  0.7156,\n",
       "          0.0216,  1.7837, -0.2989, -0.9377,  1.0187,  1.9028,  0.1680,  0.7427,\n",
       "         -1.1444, -2.2506, -0.5617, -1.2653,  0.4283,  1.2988, -0.0936,  1.1373],\n",
       "        [ 1.5657, -1.3340,  0.1210, -1.2225, -0.2252, -0.9758, -0.2754, -0.1690,\n",
       "         -0.2934,  0.5790,  0.6540,  0.5038, -1.2543, -0.9683,  0.2966,  0.2644,\n",
       "          0.2757,  1.8367, -0.3510, -1.4440,  0.8187,  1.5076, -0.1620,  0.9591,\n",
       "         -1.0887, -1.5144, -0.5598, -1.3093,  0.6170,  1.4844, -0.2534,  1.9172],\n",
       "        [ 1.7849, -1.2847,  0.2652, -0.6537, -0.9922, -1.4465, -0.4321, -0.5534,\n",
       "         -0.5250,  1.2357,  1.1762,  0.7733, -0.7390, -1.0481,  0.5989,  0.1409,\n",
       "         -0.3248,  1.7034, -0.3338, -1.2106,  0.7011,  1.2749, -0.4525,  1.3720,\n",
       "         -0.9756, -1.4793, -0.7929, -0.9906,  0.8632,  0.8560, -0.0170,  1.5062],\n",
       "        [ 1.7241, -1.0701, -0.0998, -0.9665, -0.0570, -0.9958,  0.0835,  0.1685,\n",
       "         -0.4369,  1.0540,  0.6279,  0.4716, -0.9835, -1.5002, -0.3372,  0.8110,\n",
       "         -0.0777,  2.7730,  0.1607, -0.9588,  0.4495,  1.0280, -0.9405,  0.4183,\n",
       "         -1.2930, -1.8381, -0.0695, -0.3345, -0.6799,  1.0825,  0.3277,  1.4589],\n",
       "        [ 1.4447, -1.3039, -0.0349, -1.3004, -0.3249, -0.8121, -0.6398, -0.1147,\n",
       "         -0.7658,  0.9842,  0.3216,  0.3402, -1.6499, -0.1906,  0.2944,  0.4371,\n",
       "          0.2940,  2.0790,  0.2309, -0.6151,  1.6015,  1.5703, -0.5292,  0.2847,\n",
       "         -0.7758, -1.7825, -0.8092, -1.3971,  0.4206,  1.4625,  0.0222,  1.2580],\n",
       "        [ 1.6357, -1.2526,  0.3370, -1.2458, -1.0425, -1.3684, -0.1070, -0.7416,\n",
       "         -0.5426,  0.8877,  0.9591,  0.3817, -0.8529, -0.6590,  1.0887,  0.3892,\n",
       "         -0.1513,  1.7101, -0.5631, -1.6015,  0.6713,  1.3536, -0.1024,  1.1269,\n",
       "         -0.3626, -1.1649, -0.8853, -1.0525,  0.0961,  1.5406, -0.1377,  1.6562],\n",
       "        [ 1.4192, -1.4872, -0.3447, -0.7163, -0.5518, -1.4592, -0.1749,  0.1564,\n",
       "         -0.9526,  1.1494,  0.3062,  0.5413, -1.1279, -0.7475,  0.0584,  0.2482,\n",
       "          0.5991,  2.3126,  0.0512, -0.8647,  0.7958,  1.1031, -0.5298,  0.9563,\n",
       "         -1.1206, -1.8543, -0.2022, -0.9361,  0.5418,  1.7759, -0.1864,  1.2413],\n",
       "        [ 1.1281, -1.3988,  0.4166, -0.8048, -0.4766, -1.4537, -0.2417, -0.4981,\n",
       "         -0.6747,  1.0947,  0.8356,  0.7308, -0.5451, -0.9208, -0.1439,  0.6781,\n",
       "          0.0758,  1.6962,  0.3549, -0.5168,  0.4014,  2.0618, -0.8846,  0.8814,\n",
       "         -1.4389, -1.9988, -0.8433, -0.9901,  0.8948,  1.0428,  0.1725,  1.3652],\n",
       "        [ 1.7330, -1.5019,  0.1424, -0.6572, -0.7315, -1.4582, -0.0938, -0.3673,\n",
       "         -0.6011,  1.4409,  0.7906,  0.3881, -0.6597, -0.9677,  0.3366,  0.2934,\n",
       "         -0.1609,  1.8058,  0.0830, -1.0126,  0.9149,  1.4013, -0.9045,  1.5937,\n",
       "         -0.9589, -1.8900, -0.6736, -0.8696,  0.4257,  0.7097,  0.1005,  1.3489],\n",
       "        [ 1.9142, -0.9629, -0.2160, -0.8165, -0.3205, -0.6714, -0.2413, -0.0616,\n",
       "         -0.5719,  0.7007,  0.7797,  0.8428, -1.5203, -0.4644, -0.1846,  0.0114,\n",
       "          0.0711,  2.1636,  0.3296, -0.5220,  0.8872,  1.4585, -0.3832,  0.7371,\n",
       "         -1.7057, -2.2652, -0.8892, -1.0326,  0.5318,  1.3693,  0.2724,  0.7597],\n",
       "        [ 1.3259, -1.1950,  0.2049, -1.0745, -0.0494, -1.0845, -0.4337,  0.1398,\n",
       "         -0.6837,  0.9326,  0.9007,  0.7503, -1.0718, -0.8917,  0.2061,  0.3781,\n",
       "          0.0610,  2.1527, -0.3568, -1.0963,  0.9931,  1.4681, -0.3800,  0.6677,\n",
       "         -0.9968, -2.2896, -0.1989, -1.1976,  0.3540,  1.3704, -0.2817,  1.3767],\n",
       "        [ 0.6095, -0.7657,  0.8700, -0.5778,  0.0393, -1.8224,  1.2497, -0.3721,\n",
       "         -0.4711,  0.8782, -0.0524,  0.3469, -0.7804, -1.3035,  0.3146, -0.9192,\n",
       "          0.1927,  2.0197, -0.1393, -1.7939, -0.4286,  0.4584, -0.6869,  1.4873,\n",
       "         -1.0902, -1.0134, -0.0267, -0.4716,  0.5240,  0.9515,  0.2346,  2.5387],\n",
       "        [ 1.4170, -1.1427,  0.0327, -0.9334, -0.7819, -1.7925,  0.3816, -0.5548,\n",
       "         -0.7326,  0.6020,  0.4904,  0.0307, -1.4262, -0.4894,  0.8741, -0.7908,\n",
       "          0.0957,  1.3572, -0.1456, -1.3401,  1.0293,  1.6987, -0.1230,  1.5001,\n",
       "         -0.7924, -0.8570, -0.5958, -1.0030,  0.6922,  1.7821, -0.0533,  1.5706]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import MultiHeadAttention\n",
    "multihead_test = MultiHeadAttention(num_heads=8, dim_inp=32, dim_out=32, drop_prob=0.2)\n",
    "multihead_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0248e-01,  2.0311e+00, -1.6133e+00,  7.2024e-01, -1.7753e+00,\n",
       "         -3.3829e-01, -2.0464e+00,  5.9467e-01, -1.1393e-01, -2.7632e-01,\n",
       "          1.5230e+00,  1.0057e+00, -1.0793e+00, -1.7486e-01, -4.3740e-01,\n",
       "         -8.5663e-01,  2.2595e-01,  1.3982e-01, -5.1596e-03,  8.6017e-01,\n",
       "          1.1884e+00,  1.0518e-01,  1.3982e-01,  1.3982e-01,  1.3982e-01,\n",
       "         -1.7964e+00,  1.0775e+00, -1.7099e+00,  2.0946e-01,  1.5297e-01,\n",
       "          6.2984e-01,  1.1371e+00],\n",
       "        [ 1.3799e-01,  2.1013e+00, -1.2263e-01,  7.0940e-01, -2.5010e+00,\n",
       "          7.5570e-01, -9.2116e-01,  6.5230e-01, -5.8747e-01, -3.7981e-01,\n",
       "         -1.8622e-01,  4.5554e-02,  1.3799e-01, -1.6211e-01,  1.3799e-01,\n",
       "         -1.2445e+00, -1.1914e+00,  1.3799e-01,  1.0154e-01,  8.0911e-01,\n",
       "          1.3799e-01,  1.1312e+00,  1.1770e+00,  9.9842e-01,  1.3799e-01,\n",
       "         -1.5109e+00,  9.8507e-02, -2.0704e+00, -2.8472e-01, -7.0512e-01,\n",
       "          5.8916e-01,  1.8703e+00],\n",
       "        [ 8.4869e-01,  5.9055e-01, -1.1643e-01,  3.9839e-01, -1.3028e+00,\n",
       "         -1.1643e-01, -1.2831e-01, -2.8418e-01, -1.2862e+00,  1.3018e+00,\n",
       "          1.1104e+00,  6.6069e-02, -4.4912e-02, -8.5453e-01,  3.0196e-01,\n",
       "         -5.7168e-01, -1.1643e-01, -2.3696e+00, -1.1643e-01, -1.3572e-02,\n",
       "          1.0670e+00,  2.4426e+00,  8.9759e-01,  6.2502e-01,  5.3997e-01,\n",
       "         -1.8858e+00,  5.9359e-01, -8.2598e-01,  4.5937e-01,  2.8991e-01,\n",
       "         -1.9673e+00,  4.6758e-01],\n",
       "        [-1.8318e-01,  1.4402e+00, -6.7101e-01,  2.5289e-01, -3.0270e+00,\n",
       "         -7.0319e-01, -1.9617e+00,  5.9873e-01,  6.9388e-01, -7.1967e-01,\n",
       "          4.5389e-01,  1.9110e+00, -4.5337e-01,  6.3931e-01,  2.7699e-01,\n",
       "         -1.6144e-02, -2.4736e-01, -8.3176e-01, -4.7224e-01,  1.0531e+00,\n",
       "          1.5719e+00, -1.6144e-02,  1.4623e+00,  1.8884e-01, -1.6144e-02,\n",
       "         -1.0508e+00,  1.7638e-01, -9.9195e-01, -3.6161e-01,  2.9636e-02,\n",
       "         -1.6144e-02,  9.9030e-01],\n",
       "        [ 1.8058e-01,  1.8058e-01, -1.5658e+00,  1.2242e+00,  1.8058e-01,\n",
       "          1.5000e-01, -8.5859e-01,  1.8058e-01,  2.5413e-01,  2.6008e-01,\n",
       "          1.6562e+00,  9.2676e-01,  2.4579e-01, -4.5694e-01,  2.2764e-01,\n",
       "         -1.3552e+00,  1.8058e-01, -2.0688e+00,  1.0301e+00, -1.3099e+00,\n",
       "          1.7139e+00,  1.3567e+00,  5.5052e-01, -2.5845e-02,  1.8058e-01,\n",
       "         -9.6257e-01,  5.7212e-01, -2.7522e+00, -3.8564e-01,  1.2854e-01,\n",
       "          1.8058e-01,  1.8058e-01],\n",
       "        [ 7.0496e-01,  1.0379e-01, -4.2148e-02,  3.0493e-01, -8.1355e-01,\n",
       "          8.9418e-01, -1.1732e+00,  8.3396e-01, -2.6851e+00,  1.1697e+00,\n",
       "          8.1980e-01,  6.0391e-01,  9.6267e-01, -1.2820e+00,  8.3591e-01,\n",
       "         -4.2148e-02, -5.7143e-01, -1.5375e+00, -7.6325e-01,  7.6806e-01,\n",
       "         -5.5258e-01,  1.2658e+00, -4.2148e-02,  3.3281e-01,  2.8092e-01,\n",
       "         -1.9201e+00, -6.5201e-01, -1.4243e-01, -1.4334e-01,  1.3090e+00,\n",
       "         -5.2250e-01,  1.6950e+00],\n",
       "        [ 1.2897e-01,  1.7457e+00, -1.2987e+00,  3.4429e-01, -2.3615e+00,\n",
       "          3.0002e-01, -6.4718e-01,  8.7976e-01,  1.2897e-01,  7.8334e-02,\n",
       "          1.0120e+00,  6.1652e-01, -7.3345e-01, -8.2973e-02,  3.3468e-01,\n",
       "         -1.9761e+00, -9.8938e-01, -2.1001e+00,  7.8375e-01, -6.0054e-01,\n",
       "          1.1665e+00,  1.0964e+00,  1.2897e-01,  1.2897e-01,  5.1189e-01,\n",
       "         -9.0831e-01,  1.5988e-01, -7.3953e-01,  7.3134e-01,  1.2897e-01,\n",
       "          2.9311e-01,  1.7387e+00],\n",
       "        [ 8.2854e-03,  1.8760e+00, -3.0718e-01,  8.2854e-03, -2.3058e+00,\n",
       "          7.5669e-01, -6.6309e-01,  8.2902e-01, -5.1956e-01,  2.6883e-01,\n",
       "          1.1545e-01,  7.0731e-01,  1.2284e-01,  8.2854e-03,  6.8232e-01,\n",
       "         -1.5303e+00, -7.8176e-01, -1.7810e+00, -3.7318e-01,  8.2854e-03,\n",
       "          1.3597e+00,  1.3573e+00,  1.2496e+00,  2.5809e-01,  8.2854e-03,\n",
       "         -1.6642e+00,  8.2854e-03, -1.2975e+00,  8.2854e-03, -2.0578e-01,\n",
       "         -1.9062e-01,  1.9789e+00],\n",
       "        [ 8.2606e-02,  2.1898e+00,  8.2606e-02,  6.3132e-01, -1.6400e+00,\n",
       "          8.3259e-01, -2.3187e-01, -1.5054e-01, -1.5223e+00,  1.5269e-01,\n",
       "          1.0132e+00,  8.5167e-02,  8.2606e-02,  8.2606e-02, -1.4139e-01,\n",
       "         -1.8203e+00, -1.1617e+00, -1.9387e+00,  8.2606e-02,  6.2916e-01,\n",
       "          8.2606e-02,  1.7227e+00,  9.3384e-01,  8.2606e-02,  2.6116e-01,\n",
       "         -9.5819e-01,  2.7010e-01, -1.5341e+00, -2.3029e-01, -9.4610e-02,\n",
       "          1.9163e-01,  1.9324e+00],\n",
       "        [-2.8713e-01,  2.3677e+00, -1.1796e+00,  1.1812e+00, -1.7360e+00,\n",
       "          2.7258e-01, -4.2004e-01,  2.6219e-01,  1.8555e-01,  1.8555e-01,\n",
       "          1.4830e+00,  7.6127e-01,  4.9565e-01, -1.1745e-01, -3.1475e-01,\n",
       "         -1.5337e+00, -1.4892e+00, -1.8738e+00,  1.3081e+00, -8.9848e-02,\n",
       "          1.8555e-01,  1.8555e-01,  2.6296e-01, -3.0021e-02, -1.8165e-01,\n",
       "          1.8555e-01,  4.2701e-01, -2.0763e+00, -7.0278e-02,  1.8555e-01,\n",
       "         -9.5505e-02,  1.5603e+00],\n",
       "        [ 1.4412e-03,  2.1707e+00, -9.3882e-01,  9.2846e-01, -2.1649e+00,\n",
       "          3.7095e-01, -3.7763e-01,  2.9340e-01, -6.5212e-02,  5.2263e-01,\n",
       "          2.1545e-01, -4.2974e-01,  6.4019e-01, -5.6221e-01,  3.0711e-01,\n",
       "         -6.5212e-02, -1.6919e+00, -2.1833e+00,  3.5535e-01, -7.9567e-01,\n",
       "          1.1926e+00,  1.7972e+00,  6.1980e-01, -4.1709e-01,  6.9974e-01,\n",
       "         -1.4037e+00,  4.6854e-03, -6.5212e-02, -6.5212e-02, -1.1481e-01,\n",
       "         -4.6891e-01,  1.6898e+00],\n",
       "        [ 3.0695e-02,  1.6326e+00,  3.0695e-02, -5.0706e-01, -3.1438e+00,\n",
       "          2.8290e-01, -2.7614e-01,  5.9452e-01, -4.7059e-01, -6.4264e-03,\n",
       "          2.0658e-01,  3.0695e-02,  6.8512e-02,  3.0695e-02, -1.0703e+00,\n",
       "          3.0695e-02, -8.9353e-01,  3.0695e-02,  1.7619e-01,  9.1802e-01,\n",
       "          1.9857e+00,  3.0695e-02,  1.3114e+00,  4.9481e-01, -3.6511e-02,\n",
       "         -1.6559e+00,  3.0695e-02,  3.0695e-02, -6.1147e-01, -1.5042e+00,\n",
       "          1.6137e-01,  2.0671e+00],\n",
       "        [ 5.4413e-01,  1.4072e+00,  1.1008e+00,  3.9455e-01, -2.5567e+00,\n",
       "          9.6531e-01, -1.1532e+00,  8.3059e-01,  1.5817e-01,  3.0719e-02,\n",
       "         -6.8182e-01,  8.6159e-01,  8.9176e-01, -1.1161e+00,  4.1111e-01,\n",
       "         -5.6867e-01, -8.8322e-01, -6.7094e-01, -1.1118e+00,  1.3797e+00,\n",
       "          3.0719e-02,  3.0719e-02,  1.9316e+00,  1.3564e+00, -1.1357e+00,\n",
       "         -7.4628e-01,  3.0719e-02, -1.5169e+00, -3.7973e-01, -5.8583e-01,\n",
       "          3.0719e-02,  7.2026e-01],\n",
       "        [ 6.0653e-01,  2.0213e+00,  8.0425e-01,  8.0708e-01, -3.0191e+00,\n",
       "          5.6317e-01, -4.9010e-01, -1.9658e-02, -1.0086e+00,  8.9805e-01,\n",
       "         -1.9658e-02,  6.4863e-02,  1.5576e+00, -4.7990e-01,  1.8926e-01,\n",
       "         -1.3936e+00, -1.0411e+00, -1.9658e-02, -1.3370e+00, -1.9658e-02,\n",
       "         -1.9658e-02, -1.9658e-02, -1.9658e-02,  5.9051e-01,  1.4269e+00,\n",
       "         -1.1837e+00,  5.8800e-01, -9.7119e-01,  8.3883e-01, -1.9658e-02,\n",
       "         -7.8844e-01,  9.1377e-01],\n",
       "        [ 8.0479e-01,  1.8134e+00, -7.1214e-01,  8.7325e-01, -2.1122e+00,\n",
       "         -2.3627e-02, -5.7608e-01,  1.5107e+00,  8.1842e-02,  7.4167e-01,\n",
       "          8.1842e-02,  3.3112e-01,  4.1658e-01,  8.1842e-02,  1.4764e-01,\n",
       "          1.6467e-01, -2.1110e+00, -1.8727e+00,  6.6229e-02, -8.5655e-01,\n",
       "          5.0249e-01,  1.5250e+00,  1.0019e+00,  5.5778e-01, -9.5014e-02,\n",
       "         -7.6789e-01, -2.3258e-01, -2.1754e+00, -3.9938e-01,  8.1842e-02,\n",
       "          5.2677e-01,  6.2329e-01],\n",
       "        [-2.9413e-01,  1.6467e+00,  2.3174e-01,  3.3240e-02, -2.3128e+00,\n",
       "          6.9970e-01, -1.5987e-01,  1.4568e-02,  1.4880e-01, -1.8597e-02,\n",
       "         -1.8962e-01,  9.8961e-01,  4.8796e-01, -2.6827e-02, -3.4461e-01,\n",
       "         -1.7636e+00, -8.9705e-01, -1.2180e+00, -1.9798e-01,  5.1843e-01,\n",
       "          2.1708e+00,  1.3971e+00,  1.3507e+00, -1.8597e-02,  1.9787e-02,\n",
       "         -1.7014e+00,  5.4653e-01, -8.6676e-01,  5.6654e-02, -1.2725e+00,\n",
       "         -4.2361e-01,  1.3938e+00],\n",
       "        [ 6.3935e-02,  2.2418e+00, -6.7620e-01,  5.7443e-01, -2.2838e+00,\n",
       "          6.3935e-02,  1.7372e-01,  6.4012e-01, -9.1446e-01,  7.3026e-01,\n",
       "          5.0586e-01,  1.9845e-01,  8.3633e-01, -6.8380e-01,  4.4804e-01,\n",
       "         -1.7066e+00, -1.4000e+00, -1.8543e+00,  8.0213e-01, -1.6615e-01,\n",
       "          6.3935e-02,  1.7184e+00,  6.3935e-02,  1.6440e-01,  6.3935e-02,\n",
       "          6.3935e-02,  5.5317e-01, -1.2669e+00, -1.0701e-01, -1.6765e-01,\n",
       "         -5.4900e-01,  1.8051e+00],\n",
       "        [-1.3499e-02,  1.6175e+00, -1.7470e+00, -1.3499e-02, -1.6956e+00,\n",
       "          1.8650e-01, -9.5341e-01,  8.3424e-01, -3.2837e-01,  2.2647e-01,\n",
       "          1.0547e+00,  9.8030e-01, -9.0910e-01, -3.4769e-01,  2.1016e-01,\n",
       "         -1.3499e-02, -1.3499e-02, -1.6564e+00,  1.8444e-01, -6.0084e-01,\n",
       "          2.2657e+00,  8.6155e-01,  1.6972e-01, -2.9483e-01,  1.9874e-01,\n",
       "         -2.2476e+00,  5.1200e-01, -1.3499e-02,  4.1332e-01, -4.5474e-01,\n",
       "         -3.1719e-01,  1.9049e+00],\n",
       "        [-8.5390e-01,  1.7799e+00, -7.5620e-01,  3.8543e-01, -2.3113e+00,\n",
       "          4.3950e-01,  1.4780e-02, -5.1835e-02, -2.9250e-01,  8.9696e-01,\n",
       "          3.4972e-01, -5.1835e-02, -5.1835e-02, -3.7874e-01, -5.1835e-02,\n",
       "         -5.1835e-02, -1.0938e+00, -1.7362e+00,  2.8770e-01, -3.0265e-01,\n",
       "          2.2297e+00,  1.4423e+00,  8.2938e-01,  9.8009e-02,  7.5064e-01,\n",
       "         -1.4031e+00, -5.1835e-02, -1.5267e+00,  4.6682e-02, -5.1835e-02,\n",
       "         -4.0541e-01,  1.8725e+00],\n",
       "        [ 2.5382e-01,  1.5554e+00,  2.8638e-03,  2.8638e-03, -2.9067e+00,\n",
       "         -1.2767e-01,  2.8638e-03,  7.1129e-01,  2.8638e-03,  1.1649e+00,\n",
       "          1.4725e+00,  1.4026e+00,  2.8638e-03, -5.7636e-01,  2.8638e-03,\n",
       "         -1.8046e+00, -1.2505e+00, -2.2229e+00, -1.5990e-01,  2.8638e-03,\n",
       "          1.5088e+00,  2.8638e-03, -4.0857e-02,  7.6122e-01,  2.8638e-03,\n",
       "         -9.4333e-01,  9.8668e-01,  2.8638e-03,  1.9065e-01,  2.8861e-01,\n",
       "         -2.9516e-01,  2.8638e-03]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import Encoder\n",
    "\n",
    "encoder_test = Encoder(32, 32, 8, 0.2)\n",
    "encoder_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.6870, -8.2247, -8.0846,  ..., -7.4633, -7.4026, -7.4531],\n",
       "        [-7.1619, -7.7226, -8.5259,  ..., -8.0916, -7.3004, -7.0482],\n",
       "        [-6.8304, -7.1615, -8.2725,  ..., -8.0485, -6.7234, -6.3459],\n",
       "        ...,\n",
       "        [-6.9619, -7.4826, -8.2713,  ..., -7.4195, -6.7692, -7.4150],\n",
       "        [-7.1781, -7.1967, -8.1278,  ..., -7.6795, -6.7403, -6.7293],\n",
       "        [-6.9291, -7.2373, -8.1712,  ..., -7.1587, -6.9085, -7.2713]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import BERT\n",
    "\n",
    "bert_test = BERT(vocab_size=len(vocabulary), max_length=20, dim_inp=32, dim_out=32, attention_heads=8, num_encoders=2, dropout_prob=0.2)\n",
    "bert_test.forward(test_set[1][0], test_set[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from create_dataset import NCBIDataset\n",
    "def get_split_indices(size_to_split, val_share, random_state: int = 42):\n",
    "    indices = np.arange(size_to_split)\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_share = 1 - val_share\n",
    "    \n",
    "    train_size = int(train_share * size_to_split)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "max_length = 20\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'genes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'genes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mc:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\create_dataset.py:91\u001b[0m, in \u001b[0;36mNCBIDataset.prepare_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 91\u001b[0m     masked_sequences, target_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_masking\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     indices_masked \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mlookup_indices(masked_seq) \u001b[38;5;28;01mfor\u001b[39;00m masked_seq \u001b[38;5;129;01min\u001b[39;00m masked_sequences]\n\u001b[0;32m     94\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(indices_masked, target_indices)\n",
      "File \u001b[1;32mc:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\create_dataset.py:62\u001b[0m, in \u001b[0;36mNCBIDataset._construct_masking\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_construct_masking\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 62\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m deepcopy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     63\u001b[0m     masked_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m     64\u001b[0m     target_indices_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'genes'"
     ]
    }
   ],
   "source": [
    "train_set.prepare_dataset() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import NCBIDataset\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'genes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'genes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\create_dataset.py:91\u001b[0m, in \u001b[0;36mNCBIDataset.prepare_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 91\u001b[0m     masked_sequences, target_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_masking\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     indices_masked \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mlookup_indices(masked_seq) \u001b[38;5;28;01mfor\u001b[39;00m masked_seq \u001b[38;5;129;01min\u001b[39;00m masked_sequences]\n\u001b[0;32m     94\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(indices_masked, target_indices)\n",
      "File \u001b[1;32mc:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\create_dataset.py:62\u001b[0m, in \u001b[0;36mNCBIDataset._construct_masking\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_construct_masking\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 62\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m deepcopy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     63\u001b[0m     masked_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m     64\u001b[0m     target_indices_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'genes'"
     ]
    }
   ],
   "source": [
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class BertTrainer:\n",
    "    def __init__(self, model, train_set, val_set, epochs, batch_size, lr, device, results_dir):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.val_set = val_set\n",
    "        self.epochs = epochs    \n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.weight_decay = 0.01\n",
    "        self.current_epoch  = 0\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index = -1).to(device)\n",
    "\n",
    "        self.device = device\n",
    "        self.results_dir = results_dir\n",
    "\n",
    "\n",
    "    def __call__(self):      \n",
    "        self.val_set.prepare_dataset() \n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            self.model.train()\n",
    "            self.train_set.prepare_dataset()\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            avg_epoch_loss = self.train(self.current_epoch)\n",
    "\n",
    "\n",
    "    def _init_result_lists(self):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accs = []\n",
    "        self.val_iso_accs = []\n",
    "        self.val_iso_stats = []\n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        reporting_loss = 0\n",
    "        printing_loss = 0\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            print(f\"starting\")\n",
    "            input, token_target, attn_mask = batch\n",
    "            \n",
    "            self.optimizer.zero_grad() \n",
    "            tokens = self.model(input, attn_mask) \n",
    "            print(f\"tokens: {tokens.shape}\")\n",
    "            print(f\"tokens_target: {token_target.shape}\")\n",
    "            loss = self.criterion(tokens.transpose(-1, -2), token_target) \n",
    "            \n",
    "            epoch_loss += loss.item() \n",
    "            reporting_loss += loss.item()\n",
    "            printing_loss += loss.item()\n",
    "            \n",
    "            loss.backward() \n",
    "            self.optimizer.step()         \n",
    "        avg_epoch_loss = epoch_loss / self.num_batches\n",
    "        return avg_epoch_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [20] at entry 0 and [21] at entry 12",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m val_set \u001b[38;5;241m=\u001b[39m NCBIDataset(NCBI\u001b[38;5;241m.\u001b[39miloc[val_indices], vocabulary, max_length, mask_prob)\n\u001b[0;32m      9\u001b[0m test \u001b[38;5;241m=\u001b[39m BertTrainer(bert_test, train_set, val_set, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m0.001\u001b[39m, device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 38\u001b[0m, in \u001b[0;36mBertTrainer.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 38\u001b[0m avg_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 55\u001b[0m, in \u001b[0;36mBertTrainer.train\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     53\u001b[0m reporting_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     54\u001b[0m printing_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28minput\u001b[39m, token_target, attn_mask \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [20] at entry 0 and [21] at entry 12"
     ]
    }
   ],
   "source": [
    "max_length = 51\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob)\n",
    "\n",
    "test = BertTrainer(bert_test, train_set, val_set, 1, 32, 0.001, device, \"results_dir\")\n",
    "results = test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
