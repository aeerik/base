{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from itertools import chain \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lokalt\n",
    "data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\repo\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station√§r\n",
    "data_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saga\n",
    "data_dir = \"/home/aeerik/data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget config file\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "max_length = [51,81]\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 32\n",
    "drop_prob = 0.2\n",
    "\n",
    "#Encoder\n",
    "dim_emb = 128\n",
    "dim_hidden = 128\n",
    "attention_heads = 8 \n",
    "\n",
    "#BERT\n",
    "num_encoders = 2\n",
    "\n",
    "#trainer\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "stop_patience = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "include_pheno = False\n",
    "vocabulary = vocab_geno(NCBI, include_pheno)\n",
    "vocab = vocab_pheno(ab_df)\n",
    "print(len(vocabulary))\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from data_preprocessing import data_loader\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "\n",
    "max_length = [51,81]\n",
    "mask_prob = 0.25\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = deepcopy(NCBI['AST_phenotypes'].tolist())\n",
    "max_seq_len = [max_length[0],max_length[1]]\n",
    "list_idx = []\n",
    "list_SR = []\n",
    "for i in range(len(sequences)):\n",
    "    current_seq = sequences[i]\n",
    "    current_idxs = []\n",
    "    current_SRs = []\n",
    "    for j in range(len(current_seq)):\n",
    "        item = current_seq[j].split('=')\n",
    "        abs = item[0]   \n",
    "        sr = item[1]\n",
    "        current_idxs.append(vocabulary_pheno.lookup_indices([abs]))\n",
    "        for k in range(len(sr)):\n",
    "            if sr == 'R':\n",
    "                current_SRs.append(1)\n",
    "            else:\n",
    "                current_SRs.append(0)\n",
    "\n",
    "    if len(current_idxs) != len(current_SRs):\n",
    "        print(\"current sequence:\",current_seq, \"\\n\", \"with length:\", len(current_seq))\n",
    "        print(\"indexes:\",current_idxs, \"with length:\", len(current_idxs))\n",
    "        print(\"suceptability\",current_SRs, \"with length:\", len(current_SRs))\n",
    "        print('error at', j)\n",
    "        print(\"--------------------\")\n",
    "    current_idxs = [int(item[0]) for item in current_idxs]\n",
    "    #for i in range(0,max_length[1] - len(current_idxs)):\n",
    "    #    current_idxs.append(-1)\n",
    "    #for i in range(0,max_length[1] - len(current_SRs)):\n",
    "    #    current_SRs.append(-1)\n",
    "    list_idx.append(current_idxs)\n",
    "    list_SR.append(current_SRs)\n",
    "for i in range(len(list_idx)):\n",
    "    if len(list_idx[i]) != len(list_SR[i]):\n",
    "        print('error at', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_loss(logits, targets, pad_index=-1):\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "    \n",
    "    mask = (targets != pad_index).float()\n",
    "    masked_loss = loss * mask\n",
    "    \n",
    "    average_loss = masked_loss.sum() / mask.sum()\n",
    "    \n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4494, grad_fn=<DivBackward0>)\n",
      "tensor(0.6192, grad_fn=<DivBackward0>)\n",
      "tensor(0.4631, grad_fn=<DivBackward0>)\n",
      "tensor(0.8214, grad_fn=<DivBackward0>)\n",
      "tensor(0.8458, grad_fn=<DivBackward0>)\n",
      "tensor(0.6789, grad_fn=<DivBackward0>)\n",
      "tensor(0.7272, grad_fn=<DivBackward0>)\n",
      "tensor(0.6520, grad_fn=<DivBackward0>)\n",
      "tensor(0.7455, grad_fn=<DivBackward0>)\n",
      "tensor(0.6448, grad_fn=<DivBackward0>)\n",
      "tensor(0.7252, grad_fn=<DivBackward0>)\n",
      "tensor(0.7435, grad_fn=<DivBackward0>)\n",
      "tensor(0.6430, grad_fn=<DivBackward0>)\n",
      "tensor(0.6515, grad_fn=<DivBackward0>)\n",
      "tensor(0.6933, grad_fn=<DivBackward0>)\n",
      "tensor(0.6552, grad_fn=<DivBackward0>)\n",
      "pheno tensor(10.7591, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.6246, grad_fn=<DivBackward0>)\n",
      "tensor(0.5742, grad_fn=<DivBackward0>)\n",
      "tensor(0.7922, grad_fn=<DivBackward0>)\n",
      "tensor(0.5629, grad_fn=<DivBackward0>)\n",
      "tensor(0.6612, grad_fn=<DivBackward0>)\n",
      "tensor(0.6294, grad_fn=<DivBackward0>)\n",
      "tensor(0.6250, grad_fn=<DivBackward0>)\n",
      "tensor(0.5781, grad_fn=<DivBackward0>)\n",
      "tensor(0.7151, grad_fn=<DivBackward0>)\n",
      "tensor(0.7066, grad_fn=<DivBackward0>)\n",
      "tensor(0.6012, grad_fn=<DivBackward0>)\n",
      "tensor(0.5439, grad_fn=<DivBackward0>)\n",
      "tensor(0.5832, grad_fn=<DivBackward0>)\n",
      "tensor(0.5798, grad_fn=<DivBackward0>)\n",
      "tensor(0.6079, grad_fn=<DivBackward0>)\n",
      "tensor(0.5336, grad_fn=<DivBackward0>)\n",
      "pheno tensor(9.9191, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5629, grad_fn=<DivBackward0>)\n",
      "tensor(0.5458, grad_fn=<DivBackward0>)\n",
      "tensor(0.5672, grad_fn=<DivBackward0>)\n",
      "tensor(0.6077, grad_fn=<DivBackward0>)\n",
      "tensor(0.6073, grad_fn=<DivBackward0>)\n",
      "tensor(0.5151, grad_fn=<DivBackward0>)\n",
      "tensor(0.5960, grad_fn=<DivBackward0>)\n",
      "tensor(0.5461, grad_fn=<DivBackward0>)\n",
      "tensor(0.5753, grad_fn=<DivBackward0>)\n",
      "tensor(0.4369, grad_fn=<DivBackward0>)\n",
      "tensor(0.5634, grad_fn=<DivBackward0>)\n",
      "tensor(0.4985, grad_fn=<DivBackward0>)\n",
      "tensor(0.5628, grad_fn=<DivBackward0>)\n",
      "tensor(0.5415, grad_fn=<DivBackward0>)\n",
      "tensor(0.5369, grad_fn=<DivBackward0>)\n",
      "tensor(0.6386, grad_fn=<DivBackward0>)\n",
      "pheno tensor(8.9020, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5018, grad_fn=<DivBackward0>)\n",
      "tensor(0.6425, grad_fn=<DivBackward0>)\n",
      "tensor(0.3489, grad_fn=<DivBackward0>)\n",
      "tensor(0.4459, grad_fn=<DivBackward0>)\n",
      "tensor(0.5718, grad_fn=<DivBackward0>)\n",
      "tensor(0.3695, grad_fn=<DivBackward0>)\n",
      "tensor(0.4279, grad_fn=<DivBackward0>)\n",
      "tensor(0.5357, grad_fn=<DivBackward0>)\n",
      "tensor(0.4736, grad_fn=<DivBackward0>)\n",
      "tensor(0.4826, grad_fn=<DivBackward0>)\n",
      "tensor(0.7257, grad_fn=<DivBackward0>)\n",
      "tensor(0.5394, grad_fn=<DivBackward0>)\n",
      "tensor(0.4723, grad_fn=<DivBackward0>)\n",
      "tensor(0.6047, grad_fn=<DivBackward0>)\n",
      "tensor(0.5224, grad_fn=<DivBackward0>)\n",
      "tensor(0.5808, grad_fn=<DivBackward0>)\n",
      "pheno tensor(8.2456, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.4660, grad_fn=<DivBackward0>)\n",
      "tensor(0.4852, grad_fn=<DivBackward0>)\n",
      "tensor(0.6586, grad_fn=<DivBackward0>)\n",
      "tensor(0.5002, grad_fn=<DivBackward0>)\n",
      "tensor(0.3570, grad_fn=<DivBackward0>)\n",
      "tensor(0.3346, grad_fn=<DivBackward0>)\n",
      "tensor(0.7593, grad_fn=<DivBackward0>)\n",
      "tensor(0.3917, grad_fn=<DivBackward0>)\n",
      "tensor(0.3500, grad_fn=<DivBackward0>)\n",
      "tensor(0.4950, grad_fn=<DivBackward0>)\n",
      "tensor(0.5639, grad_fn=<DivBackward0>)\n",
      "tensor(0.3917, grad_fn=<DivBackward0>)\n",
      "tensor(0.3394, grad_fn=<DivBackward0>)\n",
      "tensor(0.4299, grad_fn=<DivBackward0>)\n",
      "tensor(0.3297, grad_fn=<DivBackward0>)\n",
      "tensor(0.5289, grad_fn=<DivBackward0>)\n",
      "pheno tensor(7.3811, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5784, grad_fn=<DivBackward0>)\n",
      "tensor(0.7858, grad_fn=<DivBackward0>)\n",
      "tensor(0.7450, grad_fn=<DivBackward0>)\n",
      "tensor(0.8342, grad_fn=<DivBackward0>)\n",
      "tensor(0.8003, grad_fn=<DivBackward0>)\n",
      "tensor(0.7615, grad_fn=<DivBackward0>)\n",
      "tensor(0.7754, grad_fn=<DivBackward0>)\n",
      "tensor(0.8403, grad_fn=<DivBackward0>)\n",
      "tensor(0.7923, grad_fn=<DivBackward0>)\n",
      "tensor(0.7989, grad_fn=<DivBackward0>)\n",
      "tensor(0.9176, grad_fn=<DivBackward0>)\n",
      "tensor(0.7700, grad_fn=<DivBackward0>)\n",
      "tensor(0.7818, grad_fn=<DivBackward0>)\n",
      "tensor(0.8853, grad_fn=<DivBackward0>)\n",
      "tensor(0.8046, grad_fn=<DivBackward0>)\n",
      "tensor(0.8255, grad_fn=<DivBackward0>)\n",
      "pheno tensor(12.6967, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.7293, grad_fn=<DivBackward0>)\n",
      "tensor(0.7857, grad_fn=<DivBackward0>)\n",
      "tensor(0.7156, grad_fn=<DivBackward0>)\n",
      "tensor(0.6079, grad_fn=<DivBackward0>)\n",
      "tensor(0.7837, grad_fn=<DivBackward0>)\n",
      "tensor(0.6396, grad_fn=<DivBackward0>)\n",
      "tensor(0.6884, grad_fn=<DivBackward0>)\n",
      "tensor(0.8099, grad_fn=<DivBackward0>)\n",
      "tensor(0.7502, grad_fn=<DivBackward0>)\n",
      "tensor(0.7205, grad_fn=<DivBackward0>)\n",
      "tensor(0.5605, grad_fn=<DivBackward0>)\n",
      "tensor(0.6232, grad_fn=<DivBackward0>)\n",
      "tensor(0.8054, grad_fn=<DivBackward0>)\n",
      "tensor(0.7512, grad_fn=<DivBackward0>)\n",
      "tensor(0.6784, grad_fn=<DivBackward0>)\n",
      "tensor(0.7003, grad_fn=<DivBackward0>)\n",
      "pheno tensor(11.3499, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5806, grad_fn=<DivBackward0>)\n",
      "tensor(0.5937, grad_fn=<DivBackward0>)\n",
      "tensor(0.5269, grad_fn=<DivBackward0>)\n",
      "tensor(0.6608, grad_fn=<DivBackward0>)\n",
      "tensor(0.5937, grad_fn=<DivBackward0>)\n",
      "tensor(0.6394, grad_fn=<DivBackward0>)\n",
      "tensor(0.5618, grad_fn=<DivBackward0>)\n",
      "tensor(0.5803, grad_fn=<DivBackward0>)\n",
      "tensor(0.7042, grad_fn=<DivBackward0>)\n",
      "tensor(0.6732, grad_fn=<DivBackward0>)\n",
      "tensor(0.6667, grad_fn=<DivBackward0>)\n",
      "tensor(0.5419, grad_fn=<DivBackward0>)\n",
      "tensor(0.5724, grad_fn=<DivBackward0>)\n",
      "tensor(0.5622, grad_fn=<DivBackward0>)\n",
      "tensor(0.5596, grad_fn=<DivBackward0>)\n",
      "tensor(0.5259, grad_fn=<DivBackward0>)\n",
      "pheno tensor(9.5435, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5112, grad_fn=<DivBackward0>)\n",
      "tensor(0.5430, grad_fn=<DivBackward0>)\n",
      "tensor(0.4644, grad_fn=<DivBackward0>)\n",
      "tensor(0.5875, grad_fn=<DivBackward0>)\n",
      "tensor(0.4972, grad_fn=<DivBackward0>)\n",
      "tensor(0.6459, grad_fn=<DivBackward0>)\n",
      "tensor(0.4852, grad_fn=<DivBackward0>)\n",
      "tensor(0.4855, grad_fn=<DivBackward0>)\n",
      "tensor(0.7060, grad_fn=<DivBackward0>)\n",
      "tensor(0.9851, grad_fn=<DivBackward0>)\n",
      "tensor(0.8329, grad_fn=<DivBackward0>)\n",
      "tensor(0.8666, grad_fn=<DivBackward0>)\n",
      "tensor(0.8465, grad_fn=<DivBackward0>)\n",
      "tensor(0.8306, grad_fn=<DivBackward0>)\n",
      "tensor(0.6029, grad_fn=<DivBackward0>)\n",
      "tensor(0.4709, grad_fn=<DivBackward0>)\n",
      "pheno tensor(10.3614, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "tensor(0.5169, grad_fn=<DivBackward0>)\n",
      "tensor(0.7717, grad_fn=<DivBackward0>)\n",
      "tensor(0.7607, grad_fn=<DivBackward0>)\n",
      "tensor(0.8253, grad_fn=<DivBackward0>)\n",
      "tensor(0.8822, grad_fn=<DivBackward0>)\n",
      "tensor(0.8139, grad_fn=<DivBackward0>)\n",
      "tensor(0.7908, grad_fn=<DivBackward0>)\n",
      "tensor(0.7070, grad_fn=<DivBackward0>)\n",
      "tensor(0.7512, grad_fn=<DivBackward0>)\n",
      "tensor(0.7885, grad_fn=<DivBackward0>)\n",
      "tensor(0.6836, grad_fn=<DivBackward0>)\n",
      "tensor(0.8017, grad_fn=<DivBackward0>)\n",
      "tensor(0.7629, grad_fn=<DivBackward0>)\n",
      "tensor(0.6187, grad_fn=<DivBackward0>)\n",
      "tensor(0.6952, grad_fn=<DivBackward0>)\n",
      "tensor(0.8308, grad_fn=<DivBackward0>)\n",
      "pheno tensor(12.0012, grad_fn=<AddBackward0>)\n",
      "Parameters were updated after optimization.\n",
      "------------------\n",
      "target tensor([ 0, -1,  1,  1,  1, -1,  1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1,\n",
      "        -1,  1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,\n",
      "        -1, -1,  0, -1, -1, -1,  0,  0, -1, -1,  1, -1, -1,  0, -1, -1, -1,  1,\n",
      "        -1, -1, -1,  0, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         1, -1, -1,  0,  0, -1,  1, -1, -1]) shape  torch.Size([81]) type <class 'torch.Tensor'>\n",
      "pred tensor([ 0.4786,  0.4658,  0.2647,  1.0500,  0.5575,  0.1078, -0.0405, -0.3351,\n",
      "        -0.6943, -0.5706, -1.3000, -0.0464,  0.1630,  0.0561,  1.1942, -0.7140,\n",
      "        -0.4864,  0.1857,  0.5022,  0.3904, -1.0160,  0.5669, -0.2253, -0.2588,\n",
      "        -0.5341, -0.0920, -0.8664,  0.3386,  0.3392, -0.5458, -0.3611, -0.9976,\n",
      "         0.6472, -0.2721, -1.0072, -0.2907,  0.6698, -0.0895,  0.9034,  0.2583,\n",
      "        -1.0019, -0.6530,  0.2442,  0.3329, -0.5051, -0.1699,  0.4667,  0.2443,\n",
      "        -0.2963, -0.2937, -0.6594, -0.5588,  0.5937,  1.1591, -0.1682, -0.3226,\n",
      "         0.8355,  0.2033,  0.1068, -0.2998,  0.3313, -0.1574,  0.1974, -1.0022,\n",
      "         0.7160,  0.5348, -0.6237,  0.7474, -0.1159, -0.4257, -0.6888, -0.4665,\n",
      "        -0.5394, -0.5916, -0.3950,  0.0524,  0.4405, -0.3407, -0.5185, -0.5657,\n",
      "        -0.3095]) shape  torch.Size([81]) type <class 'torch.Tensor'>\n",
      "tensor(0.5315)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from bert_builder import BERT_ft\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "bert_test = BERT_ft(len(vocabulary_geno), max_length, dim_emb, dim_hidden, attention_heads, num_encoders, drop_prob, len(vocabulary_pheno), device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(bert_test.parameters(), lr=0.001, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = -1).to(device)\n",
    "\n",
    "\n",
    "loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "for i, batch in enumerate(loader):\n",
    "    if i >= 10:\n",
    "        break \n",
    "    optimizer.zero_grad()\n",
    "    input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "    token_predictions, resistance_predictions = bert_test(input, attn_mask) \n",
    "    result_list = []\n",
    "    for j in range(len(AB_idx)):\n",
    "        result_tensor = torch.full((81,), -1)  # Create tensor filled with -1 values\n",
    "        for idx, value in enumerate(AB_idx[j]):\n",
    "            if value != -1:\n",
    "                result_tensor[value.item()] = SR_class[j][idx]\n",
    "                #print(AB_idx[j])\n",
    "                #print(SR_class[j])\n",
    "                #print(result_tensor)\n",
    "        result_list.append(result_tensor)\n",
    "    ab_loss = 0\n",
    "    pheno_loss = 0\n",
    "    for i, row in enumerate(resistance_predictions):\n",
    "        prediction = row\n",
    "        #print(\"pred\",prediction, \"shape \", prediction.shape,\"type\" ,type(prediction))\n",
    "        target = result_list[i]\n",
    "        #print(\"target\",target, \"shape \", target.shape, \"type\", type(target))\n",
    "        ab_loss = custom_loss(prediction, target.float()) \n",
    "        print(ab_loss)\n",
    "        pheno_loss += ab_loss\n",
    "    print(\"pheno\",pheno_loss)\n",
    "    params_before_optimization = copy.deepcopy(bert_test.state_dict())\n",
    "    pheno_loss.backward()\n",
    "    # Call optimizer.step() to perform optimization\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compare parameter values after optimization\n",
    "    params_after_optimization = bert_test.state_dict()\n",
    "\n",
    "    # Check if parameters were updated\n",
    "    parameters_updated = any((params_before_optimization[key] != params_after_optimization[key]).any() for key in params_before_optimization)\n",
    "\n",
    "    if parameters_updated:\n",
    "        print(\"Parameters were updated after optimization.\")\n",
    "    else:\n",
    "        print(\"Parameters were not updated after optimization.\")\n",
    "    \n",
    "\n",
    "target = torch.tensor([ 0, -1,  1,  1,  1, -1,  1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1,\n",
    "        -1,  1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,\n",
    "        -1, -1,  0, -1, -1, -1,  0,  0, -1, -1,  1, -1, -1,  0, -1, -1, -1,  1,\n",
    "        -1, -1, -1,  0, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "         1, -1, -1,  0,  0, -1,  1, -1, -1])\n",
    "prediction = torch.tensor([ 0.4786,  0.4658,  0.2647,  1.0500,  0.5575,  0.1078, -0.0405, -0.3351,\n",
    "        -0.6943, -0.5706, -1.3000, -0.0464,  0.1630,  0.0561,  1.1942, -0.7140,\n",
    "        -0.4864,  0.1857,  0.5022,  0.3904, -1.0160,  0.5669, -0.2253, -0.2588,\n",
    "        -0.5341, -0.0920, -0.8664,  0.3386,  0.3392, -0.5458, -0.3611, -0.9976,\n",
    "         0.6472, -0.2721, -1.0072, -0.2907,  0.6698, -0.0895,  0.9034,  0.2583,\n",
    "        -1.0019, -0.6530,  0.2442,  0.3329, -0.5051, -0.1699,  0.4667,  0.2443,\n",
    "        -0.2963, -0.2937, -0.6594, -0.5588,  0.5937,  1.1591, -0.1682, -0.3226,\n",
    "         0.8355,  0.2033,  0.1068, -0.2998,  0.3313, -0.1574,  0.1974, -1.0022,\n",
    "         0.7160,  0.5348, -0.6237,  0.7474, -0.1159, -0.4257, -0.6888, -0.4665,\n",
    "        -0.5394, -0.5916, -0.3950,  0.0524,  0.4405, -0.3407, -0.5185, -0.5657,\n",
    "        -0.3095])\n",
    "print(\"------------------\")\n",
    "print(\"target\",target, \"shape \", target.shape, \"type\", type(target))\n",
    "print(\"pred\",prediction, \"shape \", prediction.shape,\"type\" ,type(prediction))\n",
    "loss = loss_function(prediction, target.float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_AB_predictions = []\n",
    "    for i, row in enumerate(resistance_predictions):\n",
    "        AB_list = 0\n",
    "        AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "        current_abs = []\n",
    "        for ab in AB_list:\n",
    "            current_abs.append(row[ab].item())\n",
    "        current_abs = torch.tensor(current_abs, requires_grad=True)\n",
    "        list_AB_predictions.append(current_abs)\n",
    "    \n",
    "    processed_tensor = [row[row != -1] for row in SR_class]\n",
    "    ab_loss = 0\n",
    "    for i, row in enumerate(processed_tensor):\n",
    "        row = torch.tensor(row, dtype=torch.float32,requires_grad=True)\n",
    "        list_AB_predictions[i] = torch.tensor(list_AB_predictions[i], dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "        ab_loss += self.ab_criterion(list_AB_predictions[i], row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = res_pred[14]\n",
    "print(test)\n",
    "pred_res = torch.where(test > 0, torch.ones_like(test), torch.zeros_like(test))\n",
    "print(pred_res)\n",
    "indicies = [1,2,3,4]\n",
    "test = pred_res[indicies]\n",
    "print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from create_dataset import NCBIDataset\n",
    "def get_split_indices(size_to_split, val_share, random_state: int = 42):\n",
    "    indices = np.arange(size_to_split)\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_share = 1 - val_share\n",
    "    \n",
    "    train_size = int(train_share * size_to_split)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "max_length = 20\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BertTrainer_ft:\n",
    "    def __init__(self, model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, project_name, wandb_name):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.train_size = len(train_set)\n",
    "        self.val_set = val_set\n",
    "        self.epochs = epochs    \n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = self.train_size // self.batch_size\n",
    "        self.lr = lr\n",
    "        self.weight_decay = 0.1\n",
    "        self.current_epoch  = 0\n",
    "        self.early_stopping_counter = 0\t\n",
    "        self.patience = stop_patience\n",
    "        \n",
    "        self.wandb_mode = wandb_mode\n",
    "        self.project_name = project_name\n",
    "        self.wandb_name = wandb_name\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.token_criterion = nn.CrossEntropyLoss(ignore_index = -1).to(self.device)\n",
    "        self.ab_criterion = nn.BCEWithLogitsLoss().to(self.device)\n",
    "\n",
    "\n",
    "    def __call__(self):   \n",
    "        if self.wandb_mode:\n",
    "            self._init_wandb()   \n",
    "        self.val_set.prepare_dataset() \n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            #Training\n",
    "            self.model.train()\n",
    "            self.train_set.prepare_dataset()\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            avg_epoch_loss_geno, avg_epoch_loss_pheno = self.train(self.current_epoch)\n",
    "            self.train_losses_geno.append(avg_epoch_loss_geno) \n",
    "            self.train_losses_ab.append(avg_epoch_loss_pheno)  \n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min\")\n",
    "            \n",
    "            #Validation\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            val_results = self.evaluate(self.val_loader)\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            self.val_losses_geno.append(val_results[0])\n",
    "            self.val_losses_ab.append(val_results[1])\n",
    "            self.val_accs.append(val_results[2])\n",
    "            if self.wandb_mode:\n",
    "                self._report_epoch_results()\n",
    "            criterion = self.stop_early()\n",
    "            if criterion:\n",
    "                print(f\"Training interrupted at epoch: {self.current_epoch+1}\")\n",
    "                break\n",
    "        print(f\"-=Training completed=-\")\n",
    "        results = {\n",
    "            \"best_epoch\": self.best_epoch,\n",
    "            \"geno_train_losses\": self.train_losses_geno,\n",
    "            \"ab_train_losses\": self.train_losses_ab,\n",
    "            \"geno_val_losses\": self.val_losses_geno,\n",
    "            \"ab_val_losses\": self.val_losses_ab,\n",
    "            \"val_accs\": self.val_accs\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _init_result_lists(self):\n",
    "\n",
    "        self.train_losses_geno = []\n",
    "        self.train_losses_ab = []\n",
    "\n",
    "        self.val_losses_geno = []\n",
    "        self.val_losses_ab = []\n",
    "\n",
    "        self.val_accs = []\n",
    "    \n",
    "    def stop_early(self):\n",
    "        if self.val_losses_ab[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses_ab[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss_geno = 0\n",
    "        epoch_loss_pheno = 0\n",
    "\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "            \n",
    "            ABinclusion = torch.unique(AB_idx)\n",
    "            ABinclusion = ABinclusion[ABinclusion != -1]\n",
    "            ABinclusion = ABinclusion.tolist()\n",
    "            self.model.exclude_networks(ABinclusion)\n",
    "\n",
    "            self.optimizer.zero_grad() \n",
    "\n",
    "            token_predictions, resistance_predictions = self.model(input, attn_mask) \n",
    "            geno_loss = self.token_criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "            \n",
    "            result_list = []\n",
    "            for j in range(len(AB_idx)):\n",
    "                result_tensor = torch.full((81,), -1) \n",
    "                for idx, value in enumerate(AB_idx[j]):\n",
    "                    if value != -1:\n",
    "                        result_tensor[value.item()] = SR_class[j][idx]\n",
    "                result_list.append(result_tensor)\n",
    "            ab_loss = 0\n",
    "            pheno_loss = 0\n",
    "            for i, row in enumerate(resistance_predictions):\n",
    "                prediction = row\n",
    "                target = result_list[i]\n",
    "                ab_loss = custom_loss(prediction, target.float()) \n",
    "                pheno_loss += ab_loss\n",
    "            pheno_loss.backward() \n",
    "            epoch_loss_geno += geno_loss.item()\n",
    "            epoch_loss_pheno += pheno_loss.item()\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.model.reset_exclusion()   \n",
    "              \n",
    "\n",
    "        avg_epoch_loss_geno = epoch_loss_geno / self.num_batches\n",
    "        avg_epoch_loss_pheno = epoch_loss_pheno / self.num_batches\n",
    "\n",
    "        return avg_epoch_loss_geno, avg_epoch_loss_pheno\n",
    "\n",
    "    def custom_loss(logits, targets, pad_index=-1):\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        \n",
    "        mask = (targets != pad_index).float()\n",
    "        masked_loss = loss * mask\n",
    "        \n",
    "        average_loss = masked_loss.sum() / mask.sum()\n",
    "        \n",
    "        return average_loss\n",
    "\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        epoch_loss_geno = 0\n",
    "        epoch_loss_ab = 0\n",
    "        total_correct = 0\n",
    "        total_sum = 0\n",
    "  \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(loader):\n",
    "                input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "\n",
    "                token_predictions, resistance_predictions = self.model(input, attn_mask) \n",
    "                geno_loss = self.token_criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "                \n",
    "                result_list = []\n",
    "                for j in range(len(AB_idx)):\n",
    "                    result_tensor = torch.full((81,), -1)  # Create tensor filled with -1 values\n",
    "                    for idx, value in enumerate(AB_idx[j]):\n",
    "                        if value != -1:\n",
    "                            result_tensor[value.item()] = SR_class[j][idx]\n",
    "                    result_list.append(result_tensor)\n",
    "                ab_loss = 0\n",
    "                pheno_loss = 0\n",
    "                for i, row in enumerate(resistance_predictions):\n",
    "                    prediction = row\n",
    "                    target = result_list[i]\n",
    "                    ab_loss = custom_loss(prediction, target.float()) \n",
    "                    pheno_loss += ab_loss\n",
    "                epoch_loss_geno += geno_loss.item()\n",
    "                epoch_loss_ab += pheno_loss.item() \n",
    "                \n",
    "                list_AB_predictions = []\n",
    "                for i, row in enumerate(resistance_predictions):\n",
    "                    AB_list = 0\n",
    "                    AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                    current_abs = []\n",
    "                    for ab in AB_list:\n",
    "                        current_abs.append(row[ab].item())\n",
    "                    current_abs = torch.tensor(current_abs)\n",
    "                    current_abs = current_abs.type(torch.int16)\n",
    "                    list_AB_predictions.append(current_abs)\n",
    "                \n",
    "                    processed_tensor = [row[row != -1] for row in SR_class]\n",
    "                for i, row in enumerate(processed_tensor):\n",
    "                    total_correct += (row == list_AB_predictions[i]).sum().item()\n",
    "                    total_sum += len(row)\n",
    "\n",
    "        avg_epoch_loss_geno = epoch_loss_geno / self.num_batches\n",
    "        avg_epoch_loss_ab = epoch_loss_ab / self.num_batches\n",
    "\n",
    "        accuracy = total_correct / total_sum\n",
    "\n",
    "        return avg_epoch_loss_geno, avg_epoch_loss_ab, accuracy\n",
    "    \n",
    "    def _save_model(self, savepath: Path):\n",
    "        torch.save(self.best_model_state, savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        \n",
    "        \n",
    "    def _load_model(self, savepath: Path):\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "    def _init_wandb(self):\n",
    "        self.wandb_run = wandb.init(\n",
    "            project=self.project_name, # name of the project\n",
    "            name=self.wandb_name, # name of the run\n",
    "            \n",
    "            config={\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"num_heads\": self.model.attention_heads,\n",
    "                \"num_encoders\": self.model.num_encoders,\n",
    "                \"emb_dim\": self.model.dim_embedding,\n",
    "                'ff_dim': self.model.dim_embedding,\n",
    "                \"lr\": self.lr,\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "                \"max_seq_len\": self.model.max_length[0],\n",
    "                \"vocab_size\": len(self.train_set.vocab_geno),\n",
    "                \"num_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "            }\n",
    "        )\n",
    "        self.wandb_run.watch(self.model) # watch the model for gradients and parameters\n",
    "        self.wandb_run.define_metric(\"epoch\", hidden=True)\n",
    "        self.wandb_run.define_metric(\"batch\", hidden=True)\n",
    "\n",
    "        self.wandb_run.define_metric(\"GenoLosses/geno_train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"GenoLosses/geno_val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "        self.wandb_run.define_metric(\"AB_Losses/ab_train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"AB_Losses/ab_val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_acc\", summary=\"min\", step_metric=\"epoch\")\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/final_val_loss\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_acc\")\n",
    "        self.wandb_run.define_metric(\"final_epoch\")\n",
    "\n",
    "        return self.wandb_run\n",
    "    \n",
    "    def _report_epoch_results(self):\n",
    "        wandb_dict = {\n",
    "            \"epoch\": self.current_epoch+1,\n",
    "            \n",
    "            \"GenoLosses/geno_train_loss\": self.train_losses_geno[-1],\n",
    "            \"ABLosses/ab_train_loss\": self.train_losses_ab[-1],\n",
    "\n",
    "            \"GenoLosses/geno_val_loss\": self.val_losses_geno[-1],\n",
    "            \"ABLosses/ab_val_loss\": self.val_losses_ab[-1],\n",
    "            \n",
    "            \"Accuracies/val_acc\": self.val_accs[-1],\n",
    "        }\n",
    "        self.wandb_run.log(wandb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import data_loader\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from misc import get_split_indices\n",
    "from create_dataset import NCBIDataset\n",
    "include_pheno = False\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "\n",
    "\n",
    "include_pheno = False\n",
    "max_length = [51,81]\n",
    "mask_prob = 0.25\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_builder import BERT_pt\n",
    "from trainer import BertTrainer_pt\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BERT_pt(vocab_size=len(vocabulary_geno), max_length=51, dim_embedding = 128, attention_heads=8, num_encoders=2, dropout_prob=0.2)\n",
    "save_directory = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\savefiles'\n",
    "\n",
    "trainer = BertTrainer_pt(model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, save_directory)\n",
    "trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded, 4000 samples found\n",
      "length  of token vocabulary: 472\n"
     ]
    }
   ],
   "source": [
    "from bert_builder import BERT_ft\n",
    "from data_preprocessing import data_loader\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from misc import get_split_indices\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "include_pheno = True\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "reduced_samples = 4000\n",
    "NCBI = NCBI.head(reduced_samples)\n",
    "\n",
    "print(f\"Data loaded, {len(NCBI)} samples found\")\n",
    "print(f\"length  of token vocabulary:\",len(vocabulary_geno))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch completed in 0.4 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:30\n",
      "Epoch 2/10\n",
      "Epoch completed in 0.4 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:01:00\n",
      "Epoch 3/10\n",
      "Epoch completed in 0.4 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:01:31\n",
      "Epoch 4/10\n",
      "Epoch completed in 0.5 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:02:02\n",
      "Epoch 5/10\n",
      "Epoch completed in 0.5 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:02:34\n",
      "Epoch 6/10\n",
      "Epoch completed in 0.5 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:03:05\n",
      "Epoch 7/10\n",
      "Epoch completed in 0.5 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:03:38\n",
      "Epoch 8/10\n",
      "Epoch completed in 0.5 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:04:09\n",
      "Epoch 9/10\n",
      "Epoch completed in 0.5 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:04:41\n",
      "Epoch 10/10\n",
      "Epoch completed in 0.5 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:05:13\n",
      "-=Training completed=-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 6,\n",
       " 'geno_train_losses': [6.4125337886810305,\n",
       "  6.400923118591309,\n",
       "  6.403627672195435,\n",
       "  6.399268646240234,\n",
       "  6.391588034629822,\n",
       "  6.373066549301147,\n",
       "  6.374578204154968,\n",
       "  6.3650466299057005,\n",
       "  6.3600346994400025,\n",
       "  6.358304142951965],\n",
       " 'ab_train_losses': [12.261569795608521,\n",
       "  10.237569556236267,\n",
       "  9.69718879699707,\n",
       "  9.409794034957885,\n",
       "  9.283147954940796,\n",
       "  9.161010270118714,\n",
       "  9.122314696311951,\n",
       "  9.139444932937622,\n",
       "  8.925314598083496,\n",
       "  8.965279974937438],\n",
       " 'geno_val_losses': [1.5916257190704346,\n",
       "  1.6036368274688722,\n",
       "  1.601959490776062,\n",
       "  1.6028269672393798,\n",
       "  1.5962445974349975,\n",
       "  1.59734281539917,\n",
       "  1.598202199935913,\n",
       "  1.598109064102173,\n",
       "  1.5966084814071655,\n",
       "  1.5862339210510255],\n",
       " 'ab_val_losses': [2.397537546157837,\n",
       "  2.254580135345459,\n",
       "  2.178412051200867,\n",
       "  2.1575922203063964,\n",
       "  2.1707427406311037,\n",
       "  2.1885937070846557,\n",
       "  2.0935871601104736,\n",
       "  2.125500807762146,\n",
       "  2.1237565422058107,\n",
       "  2.145347490310669],\n",
       " 'val_accs': [0.22477352228899453,\n",
       "  0.20226029240290608,\n",
       "  0.20468203426316262,\n",
       "  0.20252937483182348,\n",
       "  0.21867432056686698,\n",
       "  0.21562471970580321,\n",
       "  0.21006368284151045,\n",
       "  0.18539779352408287,\n",
       "  0.21786707328011481,\n",
       "  0.20279845726074086]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.001\n",
    "bert_test = BERT_ft(len(vocabulary_geno), max_length, dim_emb, dim_hidden, attention_heads, num_encoders, drop_prob, len(vocabulary_pheno), device)\n",
    "\n",
    "test = BertTrainer_ft(bert_test, train_set, val_set, 10, batch_size, lr, device, stop_patience,  False, \"NCBI\", \"test\")\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "AB_idx = torch.tensor([49, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                       -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
    "\n",
    "SR_class = torch.tensor([1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "                         -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
    "\n",
    "# Create a new tensor filled with -1 values\n",
    "result_tensor = torch.full_like(AB_idx, -1)\n",
    "\n",
    "# Iterate over AB_idx and SR_class to place the values at correct indices\n",
    "for idx, value in enumerate(AB_idx):\n",
    "    if value != -1:\n",
    "        result_tensor[value] = SR_class[idx]\n",
    "\n",
    "print(result_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
