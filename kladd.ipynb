{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from itertools import chain \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lokalt\n",
    "data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saga\n",
    "data_dir = \"/home/aeerik/data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget config file\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "max_length = [88,51,37]\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 32\n",
    "drop_prob = 0.2\n",
    "\n",
    "#Encoder\n",
    "enc_dim_inp = 32 \n",
    "enc_dim_out = 32 \n",
    "attention_heads = 8 \n",
    "\n",
    "#BERT\n",
    "num_encoders = 2\n",
    "\n",
    "#trainer\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "stop_patience = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>location</th>\n",
       "      <th>genes</th>\n",
       "      <th>AST_phenotypes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>2013</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT, pmrB_Y358N=POINT, gyrA_D87N...</td>\n",
       "      <td>[meropenem=R]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>2013</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT, pmrB_Y358N=POINT, blaCTX-M-...</td>\n",
       "      <td>[meropenem=R]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>2013</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT, aac(3)-VIa, blaCMY-2, aadA1...</td>\n",
       "      <td>[meropenem=S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2818</th>\n",
       "      <td>2013</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT, mph(A), sul2, aadA5, sul1, ...</td>\n",
       "      <td>[amoxicillin-clavulanic acid=R, meropenem=S, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>2014</td>\n",
       "      <td>USA</td>\n",
       "      <td>[sul2, sul1, dfrA14, parE_I529L=POINT, parC_E8...</td>\n",
       "      <td>[amoxicillin-clavulanic acid=R, cefotaxime=R, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year location                                              genes  \\\n",
       "2256  2013      USA  [glpT_E448K=POINT, pmrB_Y358N=POINT, gyrA_D87N...   \n",
       "2259  2013      USA  [glpT_E448K=POINT, pmrB_Y358N=POINT, blaCTX-M-...   \n",
       "2260  2013      USA  [glpT_E448K=POINT, aac(3)-VIa, blaCMY-2, aadA1...   \n",
       "2818  2013      USA  [glpT_E448K=POINT, mph(A), sul2, aadA5, sul1, ...   \n",
       "2819  2014      USA  [sul2, sul1, dfrA14, parE_I529L=POINT, parC_E8...   \n",
       "\n",
       "                                         AST_phenotypes  \n",
       "2256                                      [meropenem=R]  \n",
       "2259                                      [meropenem=R]  \n",
       "2260                                      [meropenem=S]  \n",
       "2818  [amoxicillin-clavulanic acid=R, meropenem=S, a...  \n",
       "2819  [amoxicillin-clavulanic acid=R, cefotaxime=R, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_preprocessing import data_loader\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "NCBI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2013', '2013', '2013', '2013', '2014', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2011', '2011', '2011', '2012', '2011', '2011', '2011', '2012', '2012', '2011', '2012', '2012', '2012', '2012', '2011', '2011', '2011', '2011', '2012', '2012', '2011', '2011', '2011', '2011', '2012', '2012', '2012', '2011', '2011', '2011', '2011', '2011', '2012', '2012', '2012', '2011', '2012', '2012', '2012', '2011', '2012', '2012', '2012', '2011', '2012', '2011', '2011', '2011', '2011', '2011', '2011', '2011', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2011', '2011', '2011', '2011', '2011', '2015', '2014', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2016', '2016', '2016', '2016', '2016', '2015', '2015', '2015', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '[PAD]', '2016', '2016', '2016', '2016', '2016', '2015', '2015', '2015', '2014', '2014', '2015', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '[PAD]', '2016', '2016', '2014', '2014', '2014', '2016', '2016', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2015', '2016', '2014', '2014', '2014', '2016', '2016', '2016', '2016', '2012', '2012', '2012', '2012', '2013', '2013', '2013', '2013', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '[PAD]', '2016', '2016', '2016', '2016', '2015', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '[PAD]', '[PAD]', '2016', '2015', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2013', '2016', '2016', '2016', '2016', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2016', '2016', '2016', '2016', '2017', '2017', '2017', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2009', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2013', '2011', '2016', '2016', '2016', '2016', '2016', '2016', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2016', '2016', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2016', '2016', '2016', '2015', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2016', '2014', '2016', '2016', '2017', '2014', '2014', '2014', '2016', '2014', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2014', '2014', '2014', '2015', '2015', '2015', '2014', '2014', '2014', '2014', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2015', '2016', '2016', '2013', '2014', '2014', '2014', '2016', '2016', '2016', '2016', '2016', '2015', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2014', '2013', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2015', '2016', '2015', '2016', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2012', '2013', '2013', '2013', '2013', '2013', '2012', '2012', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2015', '2018', '2017', '2017', '2017', '2018', '2018', '2018', '2017', '2018', '2018', '2017', '2015', '[PAD]', '2016', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2018', '2018', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2017', '2017', '2018', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2018', '2017', '2017', '2017', '2017', '2017', '2018', '2018', '2017', '2018', '2017', '2018', '2017', '2017', '2018', '2017', '2018', '2017', '2018', '2017', '2017', '2017', '2018', '2018', '2018', '2018', '2017', '2018', '2017', '2018', '2017', '2018', '2017', '2017', '2018', '2017', '2018', '2018', '2018', '2017', '2017', '2018', '2018', '2017', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2017', '2018', '2018', '2018', '2018', '2018', '2017', '2017', '2017', '2018', '2017', '2017', '2018', '2018', '2018', '2018', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2019', '2019', '2019', '2019', '2019', '2019', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2018', '2019', '2019', '2019', '2018', '2018', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2017', '2017', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2017', '2017', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '[PAD]', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2016', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2018', '2018', '2018', '2018', '2017', '2018', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2018', '2018', '2018', '2018', '2018', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2014', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2014', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2017', '2017', '2017', '[PAD]', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '[PAD]', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '[PAD]', '2017', '2017', '2017', '2017', '2017', '2017', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2018', '2018', '2018', '2018', '[PAD]', '2018', '2018', '2017', '2017', '2017', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '[PAD]', '2017', '2017', '2017', '2017', '2017', '2016', '2016', '2016', '2016', '2016', '[PAD]', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '2016', '[PAD]', '2016', '2016', '2016', '2016', '2016', '2017', '[PAD]', '[PAD]', '[PAD]', '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2015', '2015', '2015', '2015', '2015', '[PAD]', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '2015', '[PAD]', '2014', '[PAD]', '2014', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2014', '2017', '2014', '2014', '2013', '2013', '2013', '2013', '2013', '2013', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2013', '2014', '2014', '2014', '2013', '2014', '2014', '2014', '2014', '2014', '2014', '2013', '2013', '2019', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014', '2021', '2021', '2021', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2013', '2019', '2019', '2019', '2019', '2020', '2019', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2019', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2019', '2020', '2019', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2019', '2019', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2019', '2019', '2019', '2019', '2020', '2019', '2019', '2019', '2019', '2019', '2020', '2020', '2019', '2019', '2019', '2019', '2019', '2019', '2020', '2019', '2019', '2019', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2019', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2019', '2019', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2019', '2019', '2020', '2020', '2019', '2019', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2019', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2019', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2019', '2020', '2019', '2019', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2019', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2019', '2019', '2020', '2020', '2019', '2019', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2019', '2019', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2019', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2019', '2019', '2019', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2019', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2019', '2019', '2019', '2020', '2019', '2020', '2019', '2019', '2019', '2019', '2019', '2020', '2019', '2020', '2019', '2020', '2019', '2020', '2019', '2019', '2020', '2019', '2019', '2019', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2019', '2020', '2019', '2019', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2019', '2020', '2020', '2019', '2019', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2019', '2019', '2020', '2020', '2019', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2019', '2019', '2019', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2022', '2013', '2013', '2013', '2013', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2012', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2019', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2019', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2020', '2020', '2019', '2019', '2019', '2020', '2019', '2019', '2020', '2020', '2019', '2020', '2020', '2020', '2019', '2020', '2020', '2020', '2020', '2020', '2019', '2020', '2020', '2019', '2020', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2021', '2019', '2021', '2018', '2017', '2018', '2017', '2017', '2020', '2023', '[PAD]', '2020', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '2019', '2016', '2023', '2022', '2023', '2023', '2023', '2022', '2022', '2022', '2022', '2022', '2023', '2023', '2022', '2023', '2022', '2023', '2022', '2023', '2022', '2022', '2023', '2023', '2022', '2022', '2023', '2023', '2022', '2022', '2022', '2022', '2023', '2023', '2022', '2022', '2022', '2023', '2023', '2022', '2022', '2023', '2023', '2022', '2023', '2023', '2023', '2022', '2022', '2023', '2022', '2022', '2022', '2022', '2023', '2022', '2023', '2023', '2022', '2022', '2022', '2023', '2023', '2022', '2023', '2023', '2022', '2023', '2022', '2022', '2022', '2023', '2023', '2023', '2023', '2022', '2022', '2022', '2022', '2022', '2023', '2022', '2023', '2023', '2023', '2023', '2022', '2022', '2022', '2023', '2022', '2023', '2023', '2022', '2023', '2023', '2022', '2022', '2022', '2023', '2022', '2022', '2022', '2022', '2022', '2022', '2022', '2023', '2023', '2023', '2022', '2022', '2022', '2022', '2023', '2023', '2022', '2023', '2022', '2023', '2022', '2022', '2023', '2022', '2023', '2022', '2023', '2023', '2023', '2022', '2022', '2023', '2023', '2022', '2022', '2022', '2023', '2023', '2023', '2022', '2023', '2023', '2022', '2023', '2022', '2022', '2022', '2022', '2022', '2022', '2022', '2022', '2022', '2022', '2022', '2022', '2022', '2023', '2023', '2023', '2023', '2022', '2023', '2022', '2022', '2022', '2023', '2022', '2022', '2022', '2022', '2022', '2023', '2023', '2022', '2022', '2023', '2022', '2022', '2023', '2022', '2022', '2022', '2022', '2022', '2022', '2022', '2023', '2023', '2023', '2022', '2023', '2022', '2022', '2023', '2023', '2023', '2022', '2023', '2023', '2023', '2023', '2022', '2022', '2022', '2022', '2022', '2022', '2022', '2023', '2023', '2022', '2022', '2022', '2023', '2022', '2023', '2023', '2023', '2022', '2022', '2022', '2022', '2022', '2022', '2023', '2022', '2023', '2022', '2023', '2023', '2022', '2023', '2022', '2022', '2023', '2023', '2022', '2022', '2023', '2022', '2022', '2022', '2022', '2023', '2022', '2023', '2023', '2023', '2022', '2022', '2023', '2023', '2022', '2023', '2022', '2022', '2023', '2022', '2022', '2022', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2022', '2023', '2023', '2023', '2022', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2022', '2022', '2023', '2023', '2023', '2022', '2022', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023', '2023']\n"
     ]
    }
   ],
   "source": [
    "ast_phenotypes_list = NCBI['year'].tolist()\n",
    "\n",
    "# Print the entire list\n",
    "print(ast_phenotypes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479\n"
     ]
    }
   ],
   "source": [
    "from build_vocabulary import make_vocabulary\n",
    "vocabulary = make_vocabulary(NCBI, include_pheno)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene: [1 8] Phenotype: [0]\n",
      "Gene: [5 6] Phenotype: []\n",
      "Gene: [3 6 7] Phenotype: []\n",
      "Gene: [ 2 17] Phenotype: [ 2  5 13 18 22]\n",
      "Gene: [ 2  7 16] Phenotype: [17 20]\n",
      "Gene: [3] Phenotype: [ 7 11 13]\n"
     ]
    }
   ],
   "source": [
    "sequences = [(gene, pheno) for gene, pheno in zip(NCBI['genes'].tolist(), NCBI['AST_phenotypes'].tolist())]\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    geno_len = len(seq[0])\n",
    "    pheno_len = len(seq[1])\n",
    "\n",
    "    masking_index_geno = np.random.rand(geno_len) < 0.15\n",
    "    masking_index_pheno = np.random.rand(pheno_len) < 0.15\n",
    "\n",
    "    target_indices_geno = np.array([-1]*geno_len)\n",
    "    target_indices_pheno = np.array([-1]*pheno_len)\n",
    "\n",
    "    indices_geno = masking_index_geno.nonzero()[0]\n",
    "    indices_pheno = masking_index_pheno.nonzero()[0]\n",
    "\n",
    "\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(f\"Gene: {indices_geno}\", f\"Phenotype: {indices_pheno}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MASK_PERCENTAGE = 0.15\n",
    "\n",
    "\n",
    "# data, vocabulary, max sequence length, mask probability, include sequences, some random state\n",
    "class NCBIDataset(Dataset):\n",
    "\n",
    "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
    "    TARGET_COLUMN = 'indices'\n",
    "    NSP_TARGET_COLUMN = 'is_next'\n",
    "    TOKEN_MASK_COLUMN = 'token_mask'\n",
    "\n",
    "    def __init__(self,\n",
    "                 data: pd.DataFrame,\n",
    "                 vocab: vocab,\n",
    "                 max_seq_len: list,\n",
    "                 mask_prob: float,\n",
    "                 include_pheno:bool,\n",
    "                 random_state: int = 23,\n",
    "                 ):\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        CLS = '[CLS]'\n",
    "        PAD = '[PAD]'\n",
    "        MASK = '[MASK]'\n",
    "        UNK = '[UNK]'\n",
    "\n",
    "        self.data = data.reset_index(drop=True) \n",
    "        self.num_samples = self.data.shape[0]\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.CLS = CLS \n",
    "        self.PAD = PAD\n",
    "        self.MASK = MASK\n",
    "        self.UNK = UNK\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN]\n",
    "        self.include_pheno = include_pheno\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        input = torch.Tensor(item[self.MASKED_INDICES_COLUMN],device=device).long()\n",
    "        token_mask  = torch.tensor(item[self.TARGET_COLUMN], device=device).long()\n",
    "        attention_mask = (input == self.vocab[self.PAD]).unsqueeze(0)\n",
    "\n",
    "        return input, token_mask , attention_mask\n",
    "\n",
    "    def _construct_masking_geno(self):\n",
    "        sequences = deepcopy(self.data['genes'].tolist())\n",
    "        masked_sequences = list()\n",
    "        target_indices_list = list()\n",
    "        seq_starts = [[self.CLS, self.data['year'].iloc[i], self.data['location'].iloc[i]] for i in range(self.data.shape[0])]\n",
    "\n",
    "        for i, geno_seq in enumerate(sequences):\n",
    "            seq_len = len(geno_seq)\n",
    "            masking_index = np.random.rand(seq_len) < self.mask_prob   \n",
    "            target_indices = np.array([-1]*seq_len)\n",
    "            indices = masking_index.nonzero()[0]\n",
    "            target_indices[indices] = self.vocab.lookup_indices([geno_seq[i] for i in indices])\n",
    "            for i in indices:\n",
    "                r = np.random.rand()\n",
    "                if r < 0.8:\n",
    "                    geno_seq[i] = self.MASK\n",
    "                elif r > 0.9:\n",
    "                    geno_seq[i] = self.vocab.lookup_token(np.random.randint(self.vocab_size))\n",
    "            geno_seq = seq_starts[i] + geno_seq\n",
    "            target_indices = [-1]*3 + target_indices.tolist() \n",
    "            masked_sequences.append(geno_seq)\n",
    "            target_indices_list.append(target_indices)\n",
    "        masked_sequences = [seq + [self.PAD]*(self.max_seq_len[0] - len(seq)) for seq in masked_sequences]\n",
    "        for i in range(len(target_indices_list)):\n",
    "            indices = target_indices_list[i]\n",
    "            padding = [-1] * (self.max_seq_len[0] - len(indices))\n",
    "            target_indices_list[i] = indices + padding\n",
    "        return masked_sequences, target_indices_list \n",
    "    \n",
    "    \n",
    "    def _construct_masking_pheno(self):\n",
    "        sequences = [(gene, pheno) for gene, pheno in zip(self.data['genes'].tolist(), self.data['AST_phenotypes'].tolist())]\n",
    "\n",
    "        # Deepcopy the list of tuples\n",
    "        sequences_deepcopy = deepcopy(sequences)\n",
    "        masked_sequences = list()\n",
    "        target_indices_list = list()\n",
    "        seq_starts = [[self.CLS, self.data['year'].iloc[i], self.data['location'].iloc[i]] for i in range(self.data.shape[0])]\n",
    "\n",
    "        for i, info_seq in enumerate(sequences_deepcopy):\n",
    "            geno_len = len(info_seq[0])\n",
    "            pheno_len = len(info_seq[1])\n",
    "\n",
    "            masking_index_geno = np.random.rand(geno_len) < self.mask_prob\n",
    "            masking_index_pheno = np.random.rand(pheno_len) < self.mask_prob\n",
    "\n",
    "            target_indices_geno = np.array([-1]*geno_len)\n",
    "            target_indices_pheno = np.array([-1]*pheno_len)\n",
    "\n",
    "            indices_geno = masking_index_geno.nonzero()[0]\n",
    "            indices_pheno = masking_index_pheno.nonzero()[0]\n",
    "\n",
    "            target_indices_geno[indices_geno] = self.vocab.lookup_indices([info_seq[0][i] for i in indices_geno])\n",
    "            target_indices_pheno[indices_pheno] = self.vocab.lookup_indices([info_seq[1][i] for i in indices_pheno])\n",
    "\n",
    "            for i in indices_geno:\n",
    "                r = np.random.rand()\n",
    "                if r < 0.8:\n",
    "                    info_seq[0][i] = self.MASK\n",
    "                elif r > 0.9:\n",
    "                    info_seq[0][i] = self.vocab.lookup_token(np.random.randint(self.vocab_size))\n",
    "            \n",
    "            for i in indices_pheno:\n",
    "                r = np.random.rand()\n",
    "                if r < 0.8:\n",
    "                    info_seq[1][i] = self.MASK\n",
    "                elif r > 0.9:\n",
    "                    info_seq[1][i] = self.vocab.lookup_token(np.random.randint(self.vocab_size))\n",
    "            geno_seq_pad = info_seq[0]+ [self.PAD]*(self.max_seq_len[1] - geno_len)\n",
    "            pheno_seq_pad = info_seq[1]+ [self.PAD]*(self.max_seq_len[2] - pheno_len)\n",
    "            seq = seq_starts[i] + geno_seq_pad +pheno_seq_pad\n",
    "\n",
    "            target_indices_geno =  target_indices_geno.tolist()\n",
    "            padding = [-1] * (self.max_seq_len[1] - geno_len)\n",
    "            target_indices_geno = target_indices_geno + padding\n",
    "\n",
    "            target_indices_pheno =  target_indices_pheno.tolist()\n",
    "            padding = [-1] * (self.max_seq_len[2] - pheno_len)\n",
    "            target_indices_pheno = target_indices_pheno + padding\n",
    "\n",
    "            target_indices = [-1]*3 + target_indices_geno + target_indices_pheno\n",
    "            \n",
    "            masked_sequences.append(seq)\n",
    "            target_indices_list.append(target_indices)\n",
    "        \n",
    "        return masked_sequences, target_indices_list \n",
    "        \n",
    "    def prepare_dataset(self):\n",
    "        if self.include_pheno:\n",
    "            masked_sequences, target_indices = self._construct_masking_pheno()\n",
    "            indices_masked = [self.vocab.lookup_indices(masked_seq) for masked_seq in masked_sequences]\n",
    "        else:\n",
    "            masked_sequences, target_indices = self._construct_masking_geno()\n",
    "            indices_masked = [self.vocab.lookup_indices(masked_seq) for masked_seq in masked_sequences]\n",
    "\n",
    "        rows = zip(indices_masked, target_indices)\n",
    "        self.df = pd.DataFrame(rows, columns=self.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import make_vocabulary\n",
    "from data_preprocessing import data_loader\n",
    "\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "\n",
    "max_length = [88,51,37]\n",
    "mask_prob = 0.25\n",
    "vocabulary = make_vocabulary(NCBI, include_pheno)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob,include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_pheno = True\n",
    "\n",
    "max_length = [91,51,37]\n",
    "mask_prob = 0.20\n",
    "vocabulary = make_vocabulary(NCBI, include_pheno)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob, include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,  60, 219, 214, 225,   2,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1])\n",
      "88\n",
      "tensor([ -1,  -1,  -1,  -1,  -1,  -1, 220,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "         -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "         -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "         -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "         -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "         -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "         -1,  -1,  -1,  -1])\n",
      "88\n",
      "tensor([[False,  True, False, False, False, False, False,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True]])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "i = 9\n",
    "\n",
    "print(test_set[i][0])\n",
    "print(len(test_set[i][0]))\n",
    "print(test_set[i][1])\n",
    "print(len(test_set[i][1]))\n",
    "print(test_set[i][2])\n",
    "print(len(test_set[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import NCBIDataset\n",
    "from build_vocabulary import make_vocabulary\n",
    "from data_preprocessing import data_loader\n",
    "\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "\n",
    "max_length = 51\n",
    "mask_prob = 0.50\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JointEmbedding(\n",
      "  (token_emb): Embedding(1208, 32)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from embedding import JointEmbedding\n",
    "embedding_dim = 32\n",
    "voca_size = len(vocabulary)\n",
    "max_length = [88,51,37]\n",
    "drop_prob = 0.2\n",
    "\n",
    "emb_test = JointEmbedding(embedding_dim, voca_size, max_length, drop_prob)\n",
    "emb_test.forward(test_set[1][0])\n",
    "print(emb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_builder import AttentionHead\n",
    "\n",
    "attention_test = AttentionHead(32, 32, 0.2)\n",
    "attention_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_builder import MultiHeadAttention\n",
    "multihead_test = MultiHeadAttention(num_heads=8, dim_inp=32, dim_out=32, drop_prob=0.2)\n",
    "multihead_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_builder import Encoder\n",
    "\n",
    "encoder_test = Encoder(32, 32, 8, 0.2)\n",
    "encoder_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_builder import BERT\n",
    "\n",
    "bert_test = BERT(vocab_size=len(vocabulary), max_length=51, dim_inp=32, dim_out=32, attention_heads=8, num_encoders=2, dropout_prob=0.2)\n",
    "bert_test.forward(test_set[1][0], test_set[1][2]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from create_dataset import NCBIDataset\n",
    "def get_split_indices(size_to_split, val_share, random_state: int = 42):\n",
    "    indices = np.arange(size_to_split)\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_share = 1 - val_share\n",
    "    \n",
    "    train_size = int(train_share * size_to_split)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "max_length = 20\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "class BertTrainer:\n",
    "    def __init__(self, model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, project_name, wandb_name):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.train_size = len(train_set)\n",
    "        self.val_set = val_set\n",
    "        self.epochs = epochs    \n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = self.train_size // self.batch_size\n",
    "        self.lr = lr\n",
    "        self.weight_decay = 0.01\n",
    "        self.current_epoch  = 0\n",
    "        self.early_stopping_counter = 0\t\n",
    "        self.patience = stop_patience\n",
    "        \n",
    "        \n",
    "        self.wandb_mode = wandb_mode\n",
    "        self.project_name = project_name\n",
    "        self.wandb_name = wandb_name\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index = -1).to(device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def __call__(self):   \n",
    "        if self.wandb_mode:\n",
    "            self._init_wandb()   \n",
    "        self.val_set.prepare_dataset() \n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            #Training\n",
    "            self.model.train()\n",
    "            self.train_set.prepare_dataset()\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            avg_epoch_loss = self.train(self.current_epoch)\n",
    "            self.train_losses.append(avg_epoch_loss) \n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min\")\n",
    "            \n",
    "            #Validation\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            val_results = self.evaluate(self.val_loader)\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            self.val_losses.append(val_results[0])  \n",
    "            self.val_accs.append(val_results[1])\n",
    "            if self.wandb_mode:\n",
    "                self._report_epoch_results()\n",
    "            self._report_epoch_results()\n",
    "            criterion = self.stop_early()\n",
    "            if criterion:\n",
    "                print(f\"Training interrupted at epoch: {self.current_epoch+1}\")\n",
    "                break\n",
    "\n",
    "        print(f\"-=Training completed=-\")\n",
    "        results = {\n",
    "            \"best_epoch\": self.current_epoch,\n",
    "            \"train_losses\": self.train_losses,\n",
    "            \"val_losses\": self.val_losses,\n",
    "            \"val_accs\": self.val_accs\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _init_result_lists(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accs = []\n",
    "    \n",
    "    def stop_early(self):\n",
    "        if self.val_losses[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        reporting_loss = 0\n",
    "        printing_loss = 0\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            input, token_target, attn_mask = batch\n",
    "            \n",
    "            self.optimizer.zero_grad() \n",
    "\n",
    "            tokens = self.model(input, attn_mask) \n",
    "            loss = self.criterion(tokens.transpose(-1, -2), token_target) \n",
    "            \n",
    "            epoch_loss += loss.item() \n",
    "            reporting_loss += loss.item()\n",
    "            printing_loss += loss.item()\n",
    "            \n",
    "            loss.backward() \n",
    "            self.optimizer.step()         \n",
    "        avg_epoch_loss = epoch_loss / self.num_batches\n",
    "        return avg_epoch_loss \n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0\n",
    "        total_correct = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(loader):\n",
    "                input, token_target, attn_mask = batch\n",
    "                tokens = self.model(input, attn_mask)\n",
    "                loss = self.criterion(tokens.transpose(-1, -2), token_target)\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                token_mask =  token_target != -1\n",
    "                predicted_tokens = tokens.argmax(dim=-1)\n",
    "                token_target = torch.masked_select(token_target, token_mask)\n",
    "                predicted_tokens = torch.masked_select(predicted_tokens, token_mask)\n",
    "                \n",
    "                correct = (predicted_tokens == token_target).sum().item()\n",
    "                total_correct += correct\n",
    "                total_tokens += token_target.numel() \n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(loader)\n",
    "        accuracy = total_correct / total_tokens\n",
    "\n",
    "        return avg_epoch_loss, accuracy\n",
    "    \n",
    "    def _save_model(self, savepath: Path):\n",
    "        torch.save(self.best_model_state, savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        \n",
    "        \n",
    "    def _load_model(self, savepath: Path):\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "    def _init_wandb(self):\n",
    "        self.wandb_run = wandb.init(\n",
    "            project=self.project_name, # name of the project\n",
    "            name=self.wandb_name, # name of the run\n",
    "            \n",
    "            config={\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"num_heads\": self.model.attention_heads,\n",
    "                \"num_encoders\": self.model.num_encoders,\n",
    "                \"emb_dim\": self.model.dim_inp,\n",
    "                'ff_dim': self.model.dim_out,\n",
    "                \"lr\": self.lr,\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "                \"max_seq_len\": self.model.max_length[0],\n",
    "                \"vocab_size\": len(self.train_set.vocab),\n",
    "                \"num_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "            }\n",
    "        )\n",
    "        self.wandb_run.watch(self.model) # watch the model for gradients and parameters\n",
    "        self.wandb_run.define_metric(\"epoch\", hidden=True)\n",
    "        self.wandb_run.define_metric(\"batch\", hidden=True)\n",
    "\n",
    "        self.wandb_run.define_metric(\"Losses/train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Losses/val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/final_val_loss\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_acc\")\n",
    "        self.wandb_run.define_metric(\"final_epoch\")\n",
    "\n",
    "        return self.wandb_run\n",
    "    \n",
    "    def _report_epoch_results(self):\n",
    "        wandb_dict = {\n",
    "            \"epoch\": self.current_epoch+1,\n",
    "            \"Losses/train_loss\": self.train_losses[-1],\n",
    "            \"Losses/val_loss\": self.val_losses[-1],\n",
    "            \"Accuracies/val_acc\": self.val_accs[-1],\n",
    "        }\n",
    "        self.wandb_run.log(wandb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded, 1000 samples found\n",
      "length  of vocabulary: 1208\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6acd82vc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd26e33968a74d24841fde3606e1961d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.015 MB uploaded\\r'), FloatProgress(value=0.059422213793539416, max=1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test</strong> at: <a href='https://wandb.ai/strompfel/NCBI/runs/6acd82vc' target=\"_blank\">https://wandb.ai/strompfel/NCBI/runs/6acd82vc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240212_163013-6acd82vc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6acd82vc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0ad681bf1042a5b8ef185b01e30240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\erika\\Desktop\\Exjobb\\data\\wandb\\run-20240212_163246-r6tal2xq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/strompfel/NCBI/runs/r6tal2xq' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/strompfel/NCBI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/strompfel/NCBI' target=\"_blank\">https://wandb.ai/strompfel/NCBI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/strompfel/NCBI/runs/r6tal2xq' target=\"_blank\">https://wandb.ai/strompfel/NCBI/runs/r6tal2xq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:06\n",
      "Epoch 2/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:12\n",
      "Epoch 3/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:18\n",
      "Epoch 4/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:24\n",
      "Epoch 5/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:31\n",
      "-=Training completed=-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 4,\n",
       " 'train_losses': [6.716255130767823,\n",
       "  5.805294799804687,\n",
       "  5.029448509216309,\n",
       "  4.463122625350952,\n",
       "  4.1071098518371585],\n",
       " 'val_losses': [6.11516353062221,\n",
       "  5.36531925201416,\n",
       "  4.712193965911865,\n",
       "  4.294982944216047,\n",
       "  4.067011458533151],\n",
       " 'val_accs': [0.24260355029585798,\n",
       "  0.23668639053254437,\n",
       "  0.1893491124260355,\n",
       "  0.1952662721893491,\n",
       "  0.2485207100591716]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import BERT\n",
    "from data_preprocessing import data_loader\n",
    "from build_vocabulary import make_vocabulary\n",
    "from misc import get_split_indices\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "include_pheno = False\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "vocabulary = make_vocabulary(NCBI, include_pheno)\n",
    "reduced_samples = 1000\n",
    "NCBI = NCBI.head(reduced_samples)\n",
    "print(f\"Data loaded, {len(NCBI)} samples found\")\n",
    "print(f\"length  of vocabulary:\",len(vocabulary))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob,include_pheno)\n",
    "\n",
    "bert_test = BERT(len(vocabulary), max_length, enc_dim_inp, enc_dim_out, attention_heads, num_encoders, drop_prob)\n",
    "\n",
    "test = BertTrainer(bert_test, train_set, val_set, epochs, batch_size, lr, device, stop_patience, True, \"NCBI\", \"test\")\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test._load_model('c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\test_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
