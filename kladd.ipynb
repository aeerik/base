{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from itertools import chain \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lokalt\n",
    "data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\repo\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station√§r\n",
    "data_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saga\n",
    "data_dir = \"/home/aeerik/data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget config file\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "max_length = [51,37]\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 32\n",
    "drop_prob = 0.2\n",
    "\n",
    "#Encoder\n",
    "dim_emb = 128\n",
    "dim_hidden = 128\n",
    "attention_heads = 8 \n",
    "\n",
    "#BERT\n",
    "num_encoders = 2\n",
    "\n",
    "#trainer\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "stop_patience = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "include_pheno = False\n",
    "vocabulary = vocab_geno(NCBI, include_pheno)\n",
    "vocab = vocab_pheno(ab_df)\n",
    "print(len(vocabulary))\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from data_preprocessing import data_loader\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "\n",
    "max_length = [51,37]\n",
    "mask_prob = 0.25\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = deepcopy(NCBI['AST_phenotypes'].tolist())\n",
    "max_seq_len = [max_length[0],max_length[1]]\n",
    "list_idx = []\n",
    "list_SR = []\n",
    "for i in range(len(sequences)):\n",
    "    current_seq = sequences[i]\n",
    "    current_idxs = []\n",
    "    current_SRs = []\n",
    "    for j in range(len(current_seq)):\n",
    "        item = current_seq[j].split('=')\n",
    "        abs = item[0]   \n",
    "        sr = item[1]\n",
    "        current_idxs.append(vocabulary_pheno.lookup_indices([abs]))\n",
    "        for k in range(len(sr)):\n",
    "            if sr == 'R':\n",
    "                current_SRs.append(1)\n",
    "            else:\n",
    "                current_SRs.append(0)\n",
    "\n",
    "    if len(current_idxs) != len(current_SRs):\n",
    "        print(\"current sequence:\",current_seq, \"\\n\", \"with length:\", len(current_seq))\n",
    "        print(\"indexes:\",current_idxs, \"with length:\", len(current_idxs))\n",
    "        print(\"suceptability\",current_SRs, \"with length:\", len(current_SRs))\n",
    "        print('error at', j)\n",
    "        print(\"--------------------\")\n",
    "    current_idxs = [int(item[0]) for item in current_idxs]\n",
    "    #for i in range(0,max_length[1] - len(current_idxs)):\n",
    "    #    current_idxs.append(-1)\n",
    "    #for i in range(0,max_length[1] - len(current_SRs)):\n",
    "    #    current_SRs.append(-1)\n",
    "    list_idx.append(current_idxs)\n",
    "    list_SR.append(current_SRs)\n",
    "for i in range(len(list_idx)):\n",
    "    if len(list_idx[i]) != len(list_SR[i]):\n",
    "        print('error at', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [49, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [49, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [53, 12,  3,  2, 30,  0, 25, 46, 72, 38, 42, 49, 19, 76, 43,  4,  2,  6,\n",
      "         57, 61, 75, 78,  4, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [53, 13, 26, 61,  3, 38, 30,  0, 43, 25, 46, 72, 38, 42, 49, 19, 76, 43,\n",
      "          4,  2,  6, 57, 75, 78, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 66, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [66, 72,  2, 42, 30, 69,  5,  3, 45, 54, 16, 78, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 72, 42, 16, 30, 22,  5, 69, 45, 54, 29, 78, 66,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 66, 69, 42, 16, 30, 22,  5, 45, 54, 29, 72, 78,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [66, 72,  2, 42, 30, 69,  5,  3, 45, 16, 54, 78, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 66,  2, 42, 69,  5, 30, 45, 16, 54, 72, 78, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 66, 72, 42, 16, 30, 22,  5, 69, 45, 54, 78, 29,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 66, 69, 42, 16, 30, 22,  5, 45, 54, 29, 72, 78,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1]])\n",
      "[0, 2, 3, 4, 5, 6, 11, 12, 13, 16, 19, 22, 25, 26, 29, 30, 38, 42, 43, 45, 46, 49, 53, 54, 57, 61, 66, 69, 72, 75, 76, 78]\n",
      "tensor([[72, 45,  2, 42, 30, 69,  5,  3, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 42, 72,  2, 30, 69,  5, 45, 54, 16, 29, 78, 66, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 66, 69, 42, 16, 30, 22,  5, 45, 54, 29, 72, 78,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 66, 69, 42, 16, 30, 22,  5, 45, 54, 29, 72, 78,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 16, 54, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 16, 54, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 66, 72, 42, 16, 30, 22,  5, 69, 45, 54, 78, 29,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 66, 69, 42, 16, 30, 22,  5, 45, 54, 29, 72, 78,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 66, 72, 45,  2, 42, 30, 69,  5, 54, 16, 29, 78, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72, 45,  2, 42, 30, 69,  5, 54, 16, 29, 78, 66, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 72, 42, 16, 30, 22,  5, 69, 45, 54, 78, 66, 29,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 16, 54, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 16, 54, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 66, 72, 42, 16, 30, 22,  5, 69, 45, 54, 29, 78,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1]])\n",
      "[2, 3, 5, 16, 22, 25, 29, 30, 42, 45, 54, 66, 69, 72, 78]\n",
      "tensor([[ 3, 25, 66, 69, 42, 16, 30, 22,  5, 45, 54, 29, 72, 78,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 66, 72,  2, 42, 30, 69,  5, 45, 16, 54, 78, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 72, 42, 16, 30, 22,  5, 69, 45, 54, 78, 66, 29,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72, 45,  2, 42, 30, 69,  5, 54, 16, 29, 78, 66, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72, 45,  2, 42, 30, 69,  5,  3, 16, 29, 54, 78, 66, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72, 45,  2, 42, 30, 69,  5, 54, 16, 29, 78, 66, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 66, 69, 42, 30, 22,  5, 45, 54, 16, 29, 72, 78,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 69,  5, 30, 45, 16, 54, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72, 42, 16, 30, 69,  5, 45, 54, 78,  2, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 72, 42, 16, 30, 22,  5, 69, 45, 54, 78, 66, 29,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 72, 42, 16, 30, 69,  5, 45, 54, 78,  2, 66, 29, 22, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [66, 72,  2, 42, 30, 69,  5,  3, 45, 16, 54, 78, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72, 45,  2, 42, 30, 69,  5,  3, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 72, 45, 42, 16, 30, 22,  5, 69, 29, 78, 54, 66,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1]])\n",
      "[2, 3, 5, 16, 22, 25, 29, 30, 42, 45, 54, 66, 69, 72, 78]\n",
      "tensor([[ 3, 25, 72, 42, 16, 30, 22,  5, 69, 45, 54, 78, 66, 29,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 66, 72,  2, 42, 69,  5, 30, 45, 16, 78, 54, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 66, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 16, 29, 78, 54, 66, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 66, 69, 42, 16, 30, 22,  5, 45, 54, 29, 72, 78,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 66, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 16, 54, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [66, 72,  2, 42, 69,  5,  3, 30, 45, 16, 78, 54, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 66, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 69,  5, 30, 45, 16, 78, 54, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 16, 54, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 72, 42, 16, 22,  5, 30, 45, 69, 54, 78, 66, 29,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1]])\n",
      "[2, 3, 5, 16, 22, 25, 29, 30, 42, 45, 54, 66, 69, 72, 78]\n",
      "tensor([[ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 29, 78, 66, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72, 45,  2, 42, 30, 69,  5,  3, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 72, 42, 16, 30, 22,  5, 69, 45, 54, 78, 66, 29,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72, 42, 30, 69,  5, 45, 54, 16, 29, 78,  2, 66, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 42, 72, 45, 16, 30, 22,  5, 69, 54, 78, 66, 29,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 66, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 72, 42, 16, 30, 22,  5, 69, 45, 54, 78, 66, 29,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 42, 72,  2, 30, 69, 45, 54, 16, 29, 78,  5, 66, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [72,  2, 42, 30, 69,  5,  3, 45, 16, 54, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 72,  2, 42, 30, 69,  5, 45, 54, 16, 78, 66, 29, 22, 25, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1],\n",
      "        [ 3, 25, 72, 42, 16, 30, 22,  5, 69, 45, 54, 78, 66, 29,  2, -1, -1, -1,\n",
      "         -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "         -1]])\n",
      "[2, 3, 5, 16, 22, 25, 29, 30, 42, 45, 54, 66, 69, 72, 78]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from bert_builder import BERT_ft\n",
    "\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "for i, batch in enumerate(loader):\n",
    "    if i >= 5:\n",
    "        break \n",
    "    input, token_target, attn_mask, AB_idx, SR_class  = batch\n",
    "    ABinclusion = torch.unique(AB_idx)\n",
    "    ABinclusion = ABinclusion[ABinclusion != -1]\n",
    "    ABinclusion = ABinclusion.tolist()\n",
    "\n",
    "    print(AB_idx)\n",
    "    print(ABinclusion)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = res_pred[14]\n",
    "print(test)\n",
    "pred_res = torch.where(test > 0, torch.ones_like(test), torch.zeros_like(test))\n",
    "print(pred_res)\n",
    "indicies = [1,2,3,4]\n",
    "test = pred_res[indicies]\n",
    "print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from create_dataset import NCBIDataset\n",
    "def get_split_indices(size_to_split, val_share, random_state: int = 42):\n",
    "    indices = np.arange(size_to_split)\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_share = 1 - val_share\n",
    "    \n",
    "    train_size = int(train_share * size_to_split)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "max_length = 20\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "class BertTrainer_ft:\n",
    "    def __init__(self, model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, project_name, wandb_name):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.train_size = len(train_set)\n",
    "        self.val_set = val_set\n",
    "        self.epochs = epochs    \n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = self.train_size // self.batch_size\n",
    "        self.lr = lr\n",
    "        self.weight_decay = 0.01\n",
    "        self.current_epoch  = 0\n",
    "        self.early_stopping_counter = 0\t\n",
    "        self.patience = stop_patience\n",
    "        \n",
    "        self.wandb_mode = wandb_mode\n",
    "        self.project_name = project_name\n",
    "        self.wandb_name = wandb_name\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adadelta(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.token_criterion = nn.CrossEntropyLoss(ignore_index = -1).to(self.device)\n",
    "        self.ab_criterion = nn.BCEWithLogitsLoss().to(self.device)\n",
    "\n",
    "\n",
    "    def __call__(self):   \n",
    "        if self.wandb_mode:\n",
    "            self._init_wandb()   \n",
    "        self.val_set.prepare_dataset() \n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            #Training\n",
    "            self.model.train()\n",
    "            self.train_set.prepare_dataset()\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            avg_epoch_loss_geno, avg_epoch_loss_ab = self.train(self.current_epoch)\n",
    "            self.train_losses_geno.append(avg_epoch_loss_geno) \n",
    "            self.train_losses_ab.append(avg_epoch_loss_ab)  \n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min\")\n",
    "            \n",
    "            #Validation\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            val_results = self.evaluate(self.val_loader)\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            self.val_losses_geno.append(val_results[0])\n",
    "            self.val_losses_ab.append(val_results[1])\n",
    "            self.val_accs.append(val_results[2])\n",
    "            if self.wandb_mode:\n",
    "                self._report_epoch_results()\n",
    "            criterion = self.stop_early()\n",
    "            if criterion:\n",
    "                print(f\"Training interrupted at epoch: {self.current_epoch+1}\")\n",
    "                break\n",
    "        print(f\"-=Training completed=-\")\n",
    "        results = {\n",
    "            \"best_epoch\": self.best_epoch,\n",
    "            \"geno_train_losses\": self.train_losses_geno,\n",
    "            \"ab_train_losses\": self.train_losses_ab,\n",
    "            \"geno_val_losses\": self.val_losses_geno,\n",
    "            \"ab_val_losses\": self.val_losses_ab,\n",
    "            \"val_accs\": self.val_accs\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _init_result_lists(self):\n",
    "\n",
    "        self.train_losses_geno = []\n",
    "        self.train_losses_ab = []\n",
    "\n",
    "        self.val_losses_geno = []\n",
    "        self.val_losses_ab = []\n",
    "\n",
    "        self.val_accs = []\n",
    "    \n",
    "    def stop_early(self):\n",
    "        if self.val_losses_ab[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses_ab[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss_geno = 0\n",
    "        epoch_loss_ab = 0\n",
    "\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "            \n",
    "            ABinclusion = torch.unique(AB_idx)\n",
    "            ABinclusion = ABinclusion[ABinclusion != -1]\n",
    "            ABinclusion = ABinclusion.tolist()\n",
    "            #self.model.exclude_networks(ABinclusion)\n",
    "\n",
    "            self.optimizer.zero_grad() \n",
    "\n",
    "            token_predictions, resistance_predictions = self.model(input, attn_mask) \n",
    "            geno_loss = self.token_criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "            list_AB_predictions = []\n",
    "            for i, row in enumerate(resistance_predictions):\n",
    "                AB_list = 0\n",
    "                AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                current_abs = []\n",
    "                for ab in AB_list:\n",
    "                    current_abs.append(row[ab].item())\n",
    "                current_abs = torch.tensor(current_abs, requires_grad=True)\n",
    "                list_AB_predictions.append(current_abs)\n",
    "            \n",
    "            processed_tensor = [row[row != -1] for row in SR_class]\n",
    "            ab_loss = 0\n",
    "            for i, row in enumerate(processed_tensor):\n",
    "                row = torch.tensor(row, dtype=torch.float32,requires_grad=True)\n",
    "                list_AB_predictions[i] = torch.tensor(list_AB_predictions[i], dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "                ab_loss += self.ab_criterion(list_AB_predictions[i], row)\n",
    "\n",
    "            epoch_loss_geno += geno_loss.item()\n",
    "            epoch_loss_ab += ab_loss.item()\n",
    "            \n",
    "            ab_loss.backward() \n",
    "\n",
    "            self.optimizer.step()\n",
    "            #self.model.reset_exclusion()   \n",
    "              \n",
    "        avg_epoch_loss_geno = epoch_loss_geno / self.num_batches\n",
    "        avg_epoch_loss_ab = epoch_loss_ab / self.num_batches\n",
    "\n",
    "        return avg_epoch_loss_geno, avg_epoch_loss_ab\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        epoch_loss_geno = 0\n",
    "        epoch_loss_ab = 0\n",
    "        total_correct = 0\n",
    "        total_sum = 0\n",
    "  \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(loader):\n",
    "                input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "\n",
    "                token_predictions, resistance_predictions = self.model(input, attn_mask) \n",
    "                geno_loss = self.token_criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "                \n",
    "                list_AB_predictions = []\n",
    "                for i, row in enumerate(resistance_predictions):\n",
    "                    AB_list = 0\n",
    "                    AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                    current_abs = []\n",
    "                    for ab in AB_list:\n",
    "                        current_abs.append(row[ab].item())\n",
    "                    current_abs = torch.tensor(current_abs)\n",
    "                    list_AB_predictions.append(current_abs)\n",
    "                \n",
    "                processed_tensor = [row[row != -1] for row in SR_class]\n",
    "                ab_loss = 0\n",
    "                for i, row in enumerate(processed_tensor):\n",
    "                    row = row.type(torch.float32)\n",
    "                    ab_loss += self.ab_criterion(list_AB_predictions[i], row)\n",
    "                    print(ab_loss)\n",
    "                epoch_loss_geno += geno_loss.item()\n",
    "                epoch_loss_ab += ab_loss.item() \n",
    "                \n",
    "                list_AB_predictions = []\n",
    "                for i, row in enumerate(resistance_predictions):\n",
    "                    AB_list = 0\n",
    "                    AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                    current_abs = []\n",
    "                    for ab in AB_list:\n",
    "                        current_abs.append(row[ab].item())\n",
    "                    current_abs = torch.tensor(current_abs)\n",
    "                    current_abs = current_abs.type(torch.int16)\n",
    "                    list_AB_predictions.append(current_abs)\n",
    "                \n",
    "                    processed_tensor = [row[row != -1] for row in SR_class]\n",
    "                for i, row in enumerate(processed_tensor):\n",
    "                    total_correct += (row == list_AB_predictions[i]).sum().item()\n",
    "                    total_sum += len(row)\n",
    "\n",
    "        avg_epoch_loss_geno = epoch_loss_geno / self.num_batches\n",
    "        avg_epoch_loss_ab = epoch_loss_ab / self.num_batches\n",
    "\n",
    "        accuracy = total_correct / total_sum\n",
    "\n",
    "        return avg_epoch_loss_geno, avg_epoch_loss_ab, accuracy\n",
    "    \n",
    "    def _save_model(self, savepath: Path):\n",
    "        torch.save(self.best_model_state, savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        \n",
    "        \n",
    "    def _load_model(self, savepath: Path):\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "    def _init_wandb(self):\n",
    "        self.wandb_run = wandb.init(\n",
    "            project=self.project_name, # name of the project\n",
    "            name=self.wandb_name, # name of the run\n",
    "            \n",
    "            config={\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"num_heads\": self.model.attention_heads,\n",
    "                \"num_encoders\": self.model.num_encoders,\n",
    "                \"emb_dim\": self.model.dim_embedding,\n",
    "                'ff_dim': self.model.dim_embedding,\n",
    "                \"lr\": self.lr,\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "                \"max_seq_len\": self.model.max_length[0],\n",
    "                \"vocab_size\": len(self.train_set.vocab_geno),\n",
    "                \"num_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "            }\n",
    "        )\n",
    "        self.wandb_run.watch(self.model) # watch the model for gradients and parameters\n",
    "        self.wandb_run.define_metric(\"epoch\", hidden=True)\n",
    "        self.wandb_run.define_metric(\"batch\", hidden=True)\n",
    "\n",
    "        self.wandb_run.define_metric(\"GenoLosses/geno_train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"GenoLosses/geno_val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "        self.wandb_run.define_metric(\"AB_Losses/ab_train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"AB_Losses/ab_val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_acc\", summary=\"min\", step_metric=\"epoch\")\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/final_val_loss\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_acc\")\n",
    "        self.wandb_run.define_metric(\"final_epoch\")\n",
    "\n",
    "        return self.wandb_run\n",
    "    \n",
    "    def _report_epoch_results(self):\n",
    "        wandb_dict = {\n",
    "            \"epoch\": self.current_epoch+1,\n",
    "            \n",
    "            \"GenoLosses/geno_train_loss\": self.train_losses_geno[-1],\n",
    "            \"ABLosses/ab_train_loss\": self.train_losses_ab[-1],\n",
    "\n",
    "            \"GenoLosses/geno_val_loss\": self.val_losses_geno[-1],\n",
    "            \"ABLosses/ab_val_loss\": self.val_losses_ab[-1],\n",
    "            \n",
    "            \"Accuracies/val_acc\": self.val_accs[-1],\n",
    "        }\n",
    "        self.wandb_run.log(wandb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import data_loader\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from misc import get_split_indices\n",
    "from create_dataset import NCBIDataset\n",
    "include_pheno = False\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "\n",
    "\n",
    "include_pheno = False\n",
    "max_length = [51,37]\n",
    "mask_prob = 0.25\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_builder import BERT_pt\n",
    "from trainer import BertTrainer_pt\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BERT_pt(vocab_size=len(vocabulary_geno), max_length=51, dim_embedding = 128, attention_heads=8, num_encoders=2, dropout_prob=0.2)\n",
    "save_directory = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\savefiles'\n",
    "\n",
    "trainer = BertTrainer_pt(model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, save_directory)\n",
    "trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_builder import BERT_ft\n",
    "from data_preprocessing import data_loader\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from misc import get_split_indices\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "include_pheno = True\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "reduced_samples = 1000\n",
    "NCBI = NCBI.head(reduced_samples)\n",
    "\n",
    "print(f\"Data loaded, {len(NCBI)} samples found\")\n",
    "print(f\"length  of token vocabulary:\",len(vocabulary_geno))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09748435020446777\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09421432763338089\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1671677827835083\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9765563011169434\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10920542478561401\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.17830917239189148\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.27350154519081116\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1235535517334938\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.014424520544707775\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.13338075578212738\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 9.941765188870022e-10\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3267219364643097\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.0824337974190712\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.11798210442066193\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.010717851109802723\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.12543465197086334\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.3059624492939292e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.31647375226020813\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07376474887132645\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1762172281742096\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.02180596999824047\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.19085854291915894\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 2.5818280846579e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.37293338775634766\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.08869310468435287\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.15478448569774628\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.008118825033307076\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.1528213620185852\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.2492736845004515e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3482435345649719\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07738395035266876\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.10862497240304947\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.011898862197995186\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10193701833486557\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 8.746074975363172e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3645579218864441\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07850047200918198\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.12897562980651855\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.012628776021301746\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.12622536718845367\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 9.137713918860868e-10\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.31265121698379517\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.07206371426582336\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.11149734258651733\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.008922543376684189\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.1114145815372467\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 9.137889334098759e-10\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3368303179740906\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07581660896539688\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.12810733914375305\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.009099765680730343\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.12432592362165451\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 9.85353798554911e-10\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3492327928543091\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07884015142917633\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.8271982669830322\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3934636116027832\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04627172276377678\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06442999094724655\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3528277277946472\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04120738431811333\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3717103898525238\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.13737404346466064\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9422110915184021\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09955915808677673\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.17186066508293152\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.23190397024154663\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08556617051362991\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.008426319807767868\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.09389325976371765\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.0984564369209693e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.30298081040382385\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05996575206518173\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09040528535842896\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.009958918206393719\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09464967995882034\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.182592853816061e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.31207332015037537\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06453366577625275\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07251039147377014\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006080774590373039\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06639355421066284\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.1765132201801975e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3367067873477936\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.0641392394900322\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.07238345593214035\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007139344234019518\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.06554011255502701\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.5384735707613117e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.30303001403808594\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06175929307937622\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08050505071878433\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.0058524892665445805\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.07141251862049103\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.2952555694667467e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2929331064224243\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.05795188993215561\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.1127191036939621\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.012402266263961792\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.12232561409473419\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.4453995778040962e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.32521918416023254\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.070896677672863\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.0692860558629036\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007632536347955465\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.06841615587472916\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 6.660605955666199e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3130422532558441\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06557612866163254\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.08579444140195847\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.007718558888882399\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07612548023462296\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.665025919007974e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.3158285617828369\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06835468858480453\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.623474359512329\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.31237930059432983\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.048332419246435165\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05845281481742859\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3659692108631134\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03873028978705406\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3494456112384796\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11928829550743103\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.9517053365707397\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.21703027188777924\n",
      "ab_loss: 23.519859313964844\n",
      "Gradient of AB Loss for row 0: tensor([-0.0206, -0.0322,  0.0284,  0.0100,  0.0382, -0.0399,  0.0435,  0.0311,\n",
      "         0.0338,  0.0434,  0.0215, -0.0227, -0.0181,  0.0283,  0.0340])\n",
      "Gradient of AB Loss for row 1: tensor([ 0.0283, -0.0271,  0.0452,  0.0138,  0.0423,  0.0493,  0.0406,  0.0329,\n",
      "         0.0402,  0.0352,  0.0247,  0.0317,  0.0289,  0.0275])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0318, -0.0483, -0.0306, -0.0482, -0.0378,  0.0319, -0.0234, -0.0397,\n",
      "         0.0278,  0.0492, -0.0289,  0.0368, -0.0355, -0.0350, -0.0251])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0327, -0.0366,  0.0189,  0.0386,  0.0172,  0.0294, -0.0484,  0.0347,\n",
      "         0.0288,  0.0409,  0.0240,  0.0432,  0.0403,  0.0363, -0.0266])\n",
      "Gradient of AB Loss for row 4: tensor([ 0.0412, -0.0466,  0.0518,  0.0162,  0.0223,  0.0314,  0.0389,  0.0293,\n",
      "         0.0459,  0.0448,  0.0461,  0.0390,  0.0326,  0.0340])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0273,  0.0381,  0.0219,  0.0439, -0.0350,  0.0327,  0.0316,  0.0304,\n",
      "         0.0223,  0.0534,  0.0215, -0.0295, -0.0382,  0.0339,  0.0204])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0536, -0.0461,  0.0437,  0.0260,  0.0418,  0.0286,  0.0278,  0.0369,\n",
      "         0.0487,  0.0379,  0.0364,  0.0358, -0.0403,  0.0348])\n",
      "Gradient of AB Loss for row 7: tensor([ 0.1944, -0.1380,  0.1594])\n",
      "Gradient of AB Loss for row 8: tensor([0.0365, 0.0519, 0.0260, 0.0246, 0.0472, 0.0276, 0.0276, 0.0308, 0.0415,\n",
      "        0.0420, 0.0403, 0.0384, 0.0360, 0.0413])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0258, -0.0341,  0.0360,  0.0195, -0.0214,  0.0313,  0.0419,  0.0399,\n",
      "         0.0206,  0.0391,  0.0402,  0.0274,  0.0331, -0.0302,  0.0222,  0.0139,\n",
      "        -0.0193])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0230, -0.0230,  0.0215, -0.0158, -0.0254, -0.0322, -0.0205,  0.0241,\n",
      "         0.0364, -0.0233, -0.0153,  0.0216,  0.0303,  0.0284, -0.0197,  0.0289,\n",
      "        -0.0180, -0.0185,  0.0265, -0.0277,  0.0276])\n",
      "Gradient of AB Loss for row 11: tensor([ 0.0335,  0.0542,  0.0247,  0.0386,  0.0414,  0.0340, -0.0343,  0.0357,\n",
      "         0.0352,  0.0219, -0.0259,  0.0361,  0.0431])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0146, -0.0119, -0.0149, -0.0143,  0.0183, -0.0126, -0.0178, -0.0195,\n",
      "        -0.0173, -0.0224, -0.0152, -0.0116, -0.0196, -0.0180, -0.0120, -0.0134,\n",
      "        -0.0190, -0.0174, -0.0129,  0.0109, -0.0145,  0.0192, -0.0158, -0.0122,\n",
      "        -0.0168, -0.0092, -0.0084, -0.0137, -0.0189, -0.0147,  0.0159, -0.0173,\n",
      "        -0.0135])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0235, -0.0299, -0.0371,  0.0090, -0.0338,  0.0360, -0.0416,  0.0362,\n",
      "        -0.0441,  0.0410,  0.0347,  0.0284, -0.0263, -0.0214, -0.0286])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0389, -0.0604, -0.0502,  0.0332, -0.0436,  0.0479,  0.0461,  0.0535,\n",
      "         0.0439,  0.0518,  0.0298, -0.0447,  0.0392])\n",
      "Gradient of AB Loss for row 15: tensor([ 0.0338, -0.0473,  0.0507,  0.0160,  0.0447, -0.0354,  0.0270,  0.0285,\n",
      "         0.0513,  0.0390,  0.0382,  0.0273,  0.0228,  0.0296])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0321, -0.0364, -0.0481, -0.0459, -0.0326,  0.0420, -0.0262,  0.0441,\n",
      "         0.0367,  0.0437,  0.0440, -0.0378, -0.0524, -0.0343])\n",
      "Gradient of AB Loss for row 17: tensor([0.0357, 0.0496, 0.0226, 0.0253, 0.0349, 0.0352, 0.0264, 0.0418, 0.0434,\n",
      "        0.0472, 0.0329, 0.0303, 0.0342, 0.0277])\n",
      "Gradient of AB Loss for row 18: tensor([0.0298, 0.0531, 0.0147, 0.0303, 0.0448, 0.0472, 0.0337, 0.0393, 0.0448,\n",
      "        0.0496, 0.0373, 0.0457, 0.0287, 0.0294])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0397, -0.0288, -0.0367, -0.0508, -0.0342,  0.0454, -0.0275,  0.0385,\n",
      "         0.0405,  0.0370,  0.0312, -0.0286, -0.0417, -0.0280])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0354, -0.0340, -0.0356, -0.0405,  0.0282,  0.0161, -0.0156, -0.0332,\n",
      "         0.0254, -0.0273,  0.0320, -0.0454,  0.0252, -0.0267,  0.0239, -0.0216])\n",
      "Gradient of AB Loss for row 21: tensor([-0.4396])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0455,  0.0430, -0.0461,  0.0147,  0.0306, -0.0479,  0.0311,  0.0350,\n",
      "         0.0522,  0.0523,  0.0472,  0.0438,  0.0424])\n",
      "Gradient of AB Loss for row 23: tensor([0.0271, 0.0483, 0.0108, 0.0301, 0.0418, 0.0320, 0.0356, 0.0354, 0.0522,\n",
      "        0.0368, 0.0258, 0.0221, 0.0375, 0.0348])\n",
      "Gradient of AB Loss for row 24: tensor([0.1159, 0.1174, 0.1628, 0.1556])\n",
      "Gradient of AB Loss for row 25: tensor([ 0.0369, -0.0412,  0.0487,  0.0345,  0.0222,  0.0311,  0.0415,  0.0278,\n",
      "         0.0521,  0.0545,  0.0301,  0.0363,  0.0320,  0.0218])\n",
      "Gradient of AB Loss for row 26: tensor([0.0330, 0.0487, 0.0318, 0.0240, 0.0380, 0.0214, 0.0360, 0.0329, 0.0567,\n",
      "        0.0225, 0.0353, 0.0354, 0.0294, 0.0267])\n",
      "Gradient of AB Loss for row 27: tensor([0.0980, 0.1188, 0.1485, 0.1546])\n",
      "Gradient of AB Loss for row 28: tensor([0.0391, 0.0412, 0.0210, 0.0289, 0.0365, 0.0365, 0.0310, 0.0420, 0.0526,\n",
      "        0.0345, 0.0514, 0.0392, 0.0256, 0.0443])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0437, -0.0557, -0.0440, -0.0286,  0.0424, -0.0419,  0.0375,  0.0352,\n",
      "         0.0354,  0.0322,  0.0324, -0.0420, -0.0223,  0.0380])\n",
      "Gradient of AB Loss for row 30: tensor([ 0.0437, -0.0370,  0.0437,  0.0265,  0.0204,  0.0261,  0.0447,  0.0390,\n",
      "         0.0428,  0.0488,  0.0280,  0.0397,  0.0295,  0.0372])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0455, -0.0347,  0.0382,  0.0161,  0.0329,  0.0292,  0.0370,  0.0338,\n",
      "         0.0402,  0.0438,  0.0284, -0.0462,  0.0256,  0.0258, -0.0323, -0.0303])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.1157444640994072\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.11583899706602097\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1957048624753952\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.078540325164795\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.12360629439353943\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.2118663191795349\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.3031131327152252\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.142170250415802\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.014295412227511406\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.16647249460220337\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.0678745665515521e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.2967905104160309\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07397371530532837\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.166733518242836\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.015886766836047173\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.19745059311389923\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.219509049299461e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3146277070045471\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.0808766558766365\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1650906503200531\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.016093293204903603\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.19180184602737427\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 9.03271846208753e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3412165343761444\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.08157871663570404\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.1094374880194664\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.012883458286523819\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.10588214546442032\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.149060402383384e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.34163346886634827\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07313207536935806\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11271225661039352\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.009175063110888004\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.11522242426872253\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 7.881591490566109e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.36258888244628906\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.0781889483332634\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.15628264844417572\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.01957404799759388\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.15025117993354797\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.2501755186633545e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3461471498012543\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08181177824735641\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.15792228281497955\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.012091190554201603\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.17976748943328857\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.4623674493563499e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3688879907131195\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08275679498910904\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.159865602850914\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.017459489405155182\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.1739668846130371\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.285988648902503e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3730607032775879\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.08101949095726013\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.8226420879364014\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3942785859107971\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04612711817026138\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06497907638549805\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.4198647141456604\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.05124901607632637\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.4339664876461029\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.15265539288520813\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0475600957870483\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.10701172053813934\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.2369576394557953\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2469412386417389\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.10065288841724396\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.009967046789824963\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.10575074702501297\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 7.331324436421482e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.32182255387306213\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06280708312988281\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.07494200021028519\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006734522525221109\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.0852944478392601\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 9.117651633694379e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.35388049483299255\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07324452698230743\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08350560069084167\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006896377541124821\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08185376971960068\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.4114833746248223e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.41758114099502563\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.08051545172929764\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.0806037038564682\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007739486638456583\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.0784493088722229\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 7.575605143195219e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.33238688111305237\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06724448502063751\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.09364061802625656\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.008868771605193615\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.09006286412477493\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 7.790473821600585e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.33964499831199646\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06428427994251251\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.1084655374288559\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.010034698061645031\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.11889547109603882\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.6021644011487979e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3382483422756195\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07285834848880768\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.13615112006664276\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.013272947631776333\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.13383519649505615\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 8.227444281416751e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.35931965708732605\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07427281886339188\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.11183944344520569\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.012058188207447529\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.10316646099090576\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.306511589672994e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.3125593662261963\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.061959750950336456\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.9255127906799316\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.34984147548675537\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05227651447057724\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06409762799739838\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.4116992950439453\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.04218899458646774\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.379966676235199\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.1202206164598465\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.227076768875122\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.23963335156440735\n",
      "ab_loss: 24.686893463134766\n",
      "Gradient of AB Loss for row 0: tensor([-0.0226, -0.0417,  0.0397,  0.0385,  0.0181,  0.0278,  0.0382,  0.0350,\n",
      "         0.0334,  0.0364,  0.0345,  0.0210,  0.0259, -0.0414,  0.0191, -0.0357,\n",
      "        -0.0296,  0.0158])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0559,  0.0482,  0.0543, -0.0419,  0.0323,  0.0407,  0.0333,  0.0242,\n",
      "         0.0528,  0.0366,  0.0289,  0.0375, -0.0417,  0.0266])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0393, -0.0533, -0.0400,  0.0463,  0.0577, -0.0433,  0.0379,  0.0512,\n",
      "         0.0519,  0.0438,  0.0417,  0.0315, -0.0366,  0.0284])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0550,  0.0456,  0.0375, -0.0489,  0.0387,  0.0411,  0.0376,  0.0213,\n",
      "         0.0476,  0.0419,  0.0381,  0.0407, -0.0327,  0.0397])\n",
      "Gradient of AB Loss for row 4: tensor([ 0.0411, -0.0472,  0.0447,  0.0402,  0.0283,  0.0475,  0.0285,  0.0367,\n",
      "         0.0338,  0.0505,  0.0387,  0.0283,  0.0301,  0.0278])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0359, -0.0316, -0.0215, -0.0390, -0.0402,  0.0348,  0.0174, -0.0302,\n",
      "         0.0360,  0.0295,  0.0429, -0.0269,  0.0330, -0.0380, -0.0309, -0.0199])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0289, -0.0378, -0.0369,  0.0148, -0.0195,  0.0399, -0.0418,  0.0373,\n",
      "         0.0327,  0.0222,  0.0276, -0.0194, -0.0424, -0.0119,  0.0236])\n",
      "Gradient of AB Loss for row 7: tensor([0.0390, 0.0405, 0.0177, 0.0308, 0.0373, 0.0420, 0.0477, 0.0423, 0.0498,\n",
      "        0.0314, 0.0417, 0.0275, 0.0333, 0.0286])\n",
      "Gradient of AB Loss for row 8: tensor([ 0.0358, -0.0294,  0.0482,  0.0146,  0.0468, -0.0398,  0.0408,  0.0258,\n",
      "         0.0338,  0.0457,  0.0373,  0.0400, -0.0363,  0.0239])\n",
      "Gradient of AB Loss for row 9: tensor([ 0.0413, -0.0454,  0.0445,  0.0278,  0.0258,  0.0435,  0.0338,  0.0470,\n",
      "         0.0343,  0.0538,  0.0460,  0.0219,  0.0303,  0.0350])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0274, -0.0282, -0.0201,  0.0251, -0.0165, -0.0182, -0.0265, -0.0383,\n",
      "        -0.0220, -0.0174, -0.0215, -0.0296, -0.0188, -0.0240, -0.0221, -0.0141,\n",
      "        -0.0200, -0.0155,  0.0164, -0.0272, -0.0187, -0.0235])\n",
      "Gradient of AB Loss for row 11: tensor([0.1549, 0.1283, 0.1399, 0.1202])\n",
      "Gradient of AB Loss for row 12: tensor([0.0280, 0.0466, 0.0246, 0.0213, 0.0489, 0.0258, 0.0434, 0.0290, 0.0388,\n",
      "        0.0354, 0.0359, 0.0355, 0.0336, 0.0334])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0125, -0.0427,  0.0497,  0.0404,  0.0323, -0.0296,  0.0477,  0.0375,\n",
      "         0.0403,  0.0379,  0.0235, -0.0343, -0.0140,  0.0336])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0399, -0.0482, -0.0255, -0.0477, -0.0326,  0.0348, -0.0196,  0.0339,\n",
      "         0.0409,  0.0385,  0.0258, -0.0382, -0.0297, -0.0309, -0.0236,  0.0349])\n",
      "Gradient of AB Loss for row 15: tensor([-0.5402])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0339,  0.0266, -0.0382,  0.0465,  0.0264,  0.0229,  0.0440,  0.0273,\n",
      "         0.0315,  0.0380,  0.0320,  0.0369,  0.0224,  0.0319])\n",
      "Gradient of AB Loss for row 17: tensor([0.0456, 0.0526, 0.0357, 0.0464, 0.0362, 0.0276, 0.0405, 0.0542, 0.0360,\n",
      "        0.0435, 0.0353, 0.0454, 0.0279])\n",
      "Gradient of AB Loss for row 18: tensor([ 0.0309, -0.0489,  0.0361,  0.0146,  0.0326,  0.0318,  0.0319,  0.0377,\n",
      "         0.0359,  0.0463,  0.0491,  0.0270,  0.0369,  0.0274])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0536, -0.0397,  0.0426,  0.0355, -0.0332,  0.0389,  0.0407,  0.0358,\n",
      "         0.0516,  0.0369,  0.0294,  0.0251, -0.0355,  0.0322])\n",
      "Gradient of AB Loss for row 20: tensor([0.0307, 0.0451, 0.0247, 0.0256, 0.0330, 0.0316, 0.0408, 0.0274, 0.0509,\n",
      "        0.0414, 0.0393, 0.0323, 0.0411, 0.0336])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0446,  0.0415,  0.0430,  0.0228,  0.0257,  0.0364,  0.0349,  0.0352,\n",
      "         0.0450,  0.0524,  0.0482,  0.0281,  0.0343,  0.0341])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0360, -0.0490, -0.0344,  0.0543,  0.0338, -0.0364,  0.0342,  0.0284,\n",
      "         0.0440,  0.0413,  0.0269, -0.0373, -0.0289,  0.0307,  0.0241])\n",
      "Gradient of AB Loss for row 23: tensor([0.0288, 0.0464, 0.0441, 0.0302, 0.0357, 0.0300, 0.0439, 0.0442, 0.0505,\n",
      "        0.0447, 0.0475, 0.0345, 0.0304, 0.0248])\n",
      "Gradient of AB Loss for row 24: tensor([0.0494, 0.0525, 0.0275, 0.0405, 0.0498, 0.0368, 0.0379, 0.0303, 0.0587,\n",
      "        0.0375, 0.0281, 0.0340, 0.0257, 0.0293])\n",
      "Gradient of AB Loss for row 25: tensor([ 0.0361, -0.0336,  0.0469,  0.0282,  0.0260,  0.0376,  0.0442,  0.0288,\n",
      "         0.0449,  0.0521,  0.0355,  0.0428,  0.0336,  0.0301])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0477,  0.0434,  0.0184,  0.0276, -0.0484,  0.0412,  0.0351,  0.0469,\n",
      "         0.0422,  0.0379,  0.0415,  0.0288, -0.0282,  0.0286])\n",
      "Gradient of AB Loss for row 27: tensor([0.1706, 0.1346, 0.1627, 0.1557])\n",
      "Gradient of AB Loss for row 28: tensor([-0.4306])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0529, -0.0425,  0.0557,  0.0339, -0.0421,  0.0201,  0.0209,  0.0406,\n",
      "         0.0528,  0.0471,  0.0403,  0.0355, -0.0400,  0.0190])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0306, -0.0218, -0.0426, -0.0368, -0.0344, -0.0327,  0.0302, -0.0406,\n",
      "         0.0283,  0.0320,  0.0257,  0.0256, -0.0268,  0.0285, -0.0258, -0.0129])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0337, -0.0583, -0.0428,  0.0479,  0.0339, -0.0452,  0.0398,  0.0250,\n",
      "         0.0476,  0.0526,  0.0402,  0.0267, -0.0272,  0.0277])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.1033160462975502\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09973239153623581\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1648385226726532\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9787103533744812\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10608430206775665\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.177307590842247\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2616535425186157\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1411868780851364\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.012765258550643921\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.15226462483406067\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 2.637350116074799e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3038996458053589\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07063578069210052\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.12765273451805115\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.011155858635902405\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.14564058184623718\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.1148807432803665e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.30609363317489624\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07502447068691254\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.21946263313293457\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.0237133651971817\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.23045389354228973\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.1501979368944149e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3211281895637512\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.07848886400461197\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.22016771137714386\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.021328387781977654\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.20861251652240753\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 3.0483062740671585e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3285573720932007\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.06962142884731293\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.12171730399131775\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.011796548962593079\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.1252150982618332\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.4333808584510166e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3491283059120178\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07590639591217041\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.136788010597229\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.011721390299499035\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.1398390382528305\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 2.0146333490345114e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.33341264724731445\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.07242909073829651\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.152066171169281\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.018329789862036705\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.16229617595672607\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.1085515838615834e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3199596107006073\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.06913348287343979\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.17737707495689392\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.013628010638058186\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.17681433260440826\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.254068626721505e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.32139867544174194\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07265211641788483\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.616471767425537\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.35238972306251526\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04715059697628021\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.05587806552648544\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3816329836845398\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04673749953508377\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3789766728878021\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1302446573972702\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9735983610153198\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.0980055034160614\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1807718425989151\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.23058919608592987\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.10161592066287994\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.009138539433479309\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.09999626129865646\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 8.271663909376059e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.32245326042175293\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06335976719856262\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.08575979620218277\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.00775342108681798\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.0925399512052536\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.0004035377875198e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3044339120388031\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.061589404940605164\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08313348144292831\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.009050985798239708\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07999266684055328\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.125374238242216e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.36677467823028564\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.07149369269609451\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.08392687141895294\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007620321586728096\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.08826587349176407\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.8256824940721117e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.31662294268608093\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.0682205855846405\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08622165024280548\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.007726544048637152\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.07698085904121399\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 6.774825700439635e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.30190029740333557\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.05659499391913414\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08789332956075668\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.008229506202042103\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09932239353656769\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.5548347054306078e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2712649405002594\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.05698859319090843\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08208795636892319\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.006371849682182074\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08443781733512878\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 8.353395197779889e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.30279454588890076\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06681504100561142\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.08297836035490036\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.008889849297702312\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.08190764486789703\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 9.56299817111983e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.302266925573349\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06559120118618011\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.6246302127838135\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.31246206164360046\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04667065665125847\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.056991495192050934\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.4022916555404663\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.04096269607543945\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.375051349401474\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11772868037223816\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.0338449478149414\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2167276293039322\n",
      "ab_loss: 24.34201431274414\n",
      "Gradient of AB Loss for row 0: tensor([-0.0285, -0.0256, -0.0286, -0.0399,  0.0464,  0.0414,  0.0391,  0.0084,\n",
      "        -0.0190,  0.0258,  0.0457,  0.0432,  0.0407,  0.0320,  0.0353, -0.0349,\n",
      "         0.0146])\n",
      "Gradient of AB Loss for row 1: tensor([0.0297, 0.0533, 0.0147, 0.0226, 0.0516, 0.0279, 0.0356, 0.0306, 0.0400,\n",
      "        0.0452, 0.0431, 0.0386, 0.0304, 0.0326])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0399, -0.0430, -0.0603, -0.0366, -0.0202,  0.0308, -0.0346,  0.0323,\n",
      "         0.0254,  0.0476,  0.0305,  0.0288, -0.0357, -0.0225])\n",
      "Gradient of AB Loss for row 3: tensor([0.1456, 0.1323, 0.1233, 0.1190])\n",
      "Gradient of AB Loss for row 4: tensor([ 0.0163, -0.0149, -0.0217,  0.0206, -0.0198,  0.0241,  0.0161,  0.0307,\n",
      "         0.0221,  0.0276,  0.0115,  0.0251, -0.0184, -0.0105, -0.0183,  0.0335,\n",
      "         0.0339,  0.0362, -0.0168,  0.0259,  0.0275])\n",
      "Gradient of AB Loss for row 5: tensor([-0.5898])\n",
      "Gradient of AB Loss for row 6: tensor([0.0320, 0.0577, 0.0272, 0.0248, 0.0477, 0.0276, 0.0241, 0.0451, 0.0431,\n",
      "        0.0419, 0.0371, 0.0374, 0.0342, 0.0382])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0380, -0.0394, -0.0366,  0.0463,  0.0422, -0.0402,  0.0373,  0.0249,\n",
      "         0.0556,  0.0329,  0.0398,  0.0336, -0.0314,  0.0287])\n",
      "Gradient of AB Loss for row 8: tensor([0.0263, 0.0405, 0.0108, 0.0269, 0.0407, 0.0310, 0.0371, 0.0329, 0.0574,\n",
      "        0.0364, 0.0400, 0.0337, 0.0370, 0.0235])\n",
      "Gradient of AB Loss for row 9: tensor([0.1162, 0.1590, 0.1626, 0.1365])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0398, -0.0473,  0.0435,  0.0370,  0.0339,  0.0434,  0.0337,  0.0484,\n",
      "         0.0449,  0.0438,  0.0386, -0.0437,  0.0395])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0292, -0.0396, -0.0370, -0.0404,  0.0293,  0.0280,  0.0267, -0.0301,\n",
      "         0.0345,  0.0504,  0.0390,  0.0447,  0.0478,  0.0385, -0.0350])\n",
      "Gradient of AB Loss for row 12: tensor([ 0.1318,  0.1477, -0.0995,  0.1156])\n",
      "Gradient of AB Loss for row 13: tensor([-0.1379,  0.2184,  0.1753])\n",
      "Gradient of AB Loss for row 14: tensor([-0.5898])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0399,  0.0494,  0.0195,  0.0497, -0.0429,  0.0479,  0.0335,  0.0349,\n",
      "         0.0452,  0.0387,  0.0298,  0.0343, -0.0347,  0.0361])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0278, -0.0399,  0.0467,  0.0290,  0.0256,  0.0383,  0.0305,  0.0353,\n",
      "         0.0468,  0.0371,  0.0301,  0.0235, -0.0372,  0.0253])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0503,  0.0467,  0.0333, -0.0435,  0.0335,  0.0384,  0.0256,  0.0449,\n",
      "         0.0303,  0.0489,  0.0403,  0.0319, -0.0462,  0.0517])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0421, -0.0222, -0.0193, -0.0339, -0.0289, -0.0295, -0.0325, -0.0328,\n",
      "        -0.0198, -0.0364, -0.0252, -0.0248, -0.0273,  0.0384, -0.0335, -0.0264,\n",
      "         0.0328])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0296, -0.0358, -0.0423, -0.0493,  0.0362,  0.0197, -0.0317, -0.0382,\n",
      "         0.0418, -0.0374,  0.0396, -0.0374,  0.0267, -0.0387, -0.0315])\n",
      "Gradient of AB Loss for row 20: tensor([0.1505, 0.1482, 0.1050, 0.1489])\n",
      "Gradient of AB Loss for row 21: tensor([0.0325, 0.0511, 0.0304, 0.0345, 0.0335, 0.0283, 0.0279, 0.0420, 0.0406,\n",
      "        0.0422, 0.0269, 0.0287, 0.0414, 0.0156])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0188, -0.0257, -0.0177,  0.0208, -0.0160, -0.0217, -0.0281, -0.0144,\n",
      "         0.0176, -0.0274, -0.0278, -0.0264,  0.0181, -0.0308, -0.0284, -0.0138,\n",
      "        -0.0179, -0.0321, -0.0260, -0.0170])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0269, -0.0332,  0.0326, -0.0336,  0.0085, -0.0277,  0.0412, -0.0444,\n",
      "         0.0404, -0.0443,  0.0352,  0.0419,  0.0404, -0.0421, -0.0237])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0231,  0.0310, -0.0400,  0.0353,  0.0143,  0.0333, -0.0389,  0.0397,\n",
      "         0.0309,  0.0360,  0.0472,  0.0289, -0.0272,  0.0217,  0.0444])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0404, -0.0326, -0.0438,  0.0384,  0.0315, -0.0486,  0.0305,  0.0409,\n",
      "         0.0487,  0.0315,  0.0323,  0.0305, -0.0309,  0.0332])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0440,  0.0400,  0.0265,  0.0161,  0.0514,  0.0423,  0.0387,  0.0413,\n",
      "         0.0453,  0.0479,  0.0427,  0.0283, -0.0235,  0.0268])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0287,  0.0262, -0.0329,  0.0413,  0.0130,  0.0333, -0.0425,  0.0380,\n",
      "         0.0333,  0.0368,  0.0458,  0.0246, -0.0366,  0.0189,  0.0338])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0441,  0.0547,  0.0405,  0.0416,  0.0304,  0.0336,  0.0416,  0.0487,\n",
      "         0.0423,  0.0462,  0.0266, -0.0538,  0.0260])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0247, -0.0521, -0.0448,  0.0552, -0.0490,  0.0383,  0.0389,  0.0528,\n",
      "         0.0560,  0.0288,  0.0221, -0.0368,  0.0262])\n",
      "Gradient of AB Loss for row 30: tensor([0.0353, 0.0385, 0.0126, 0.0321, 0.0467, 0.0324, 0.0228, 0.0336, 0.0430,\n",
      "        0.0337, 0.0379, 0.0437, 0.0286, 0.0238])\n",
      "Gradient of AB Loss for row 31: tensor([0.0288, 0.0257, 0.0368, 0.0333, 0.0349, 0.0417, 0.0490, 0.0343, 0.0480,\n",
      "        0.0407, 0.0401, 0.0489, 0.0301, 0.0189])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.11016524583101273\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.106571264564991\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.17083202302455902\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0293551683425903\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11876025050878525\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.19458356499671936\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.30007681250572205\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1414388120174408\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.014990889467298985\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.15110325813293457\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 2.334017645821973e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3274594247341156\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07915974408388138\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.1417730152606964\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.014284730888903141\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.1635448932647705\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 2.6199722391595515e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3443364202976227\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.0860593169927597\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1759250909090042\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.019387319684028625\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.1954377293586731\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.0659441107563339e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3736437261104584\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09420981258153915\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.14656558632850647\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.01400974951684475\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.1474122405052185\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.2213339228850373e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3758383095264435\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07993712276220322\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.1548260599374771\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.019020212814211845\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.14941798150539398\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.4651539981258566e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.37845221161842346\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08495371043682098\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.1457541137933731\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.010670267976820469\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.14572224020957947\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.1134332345008602e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3614077866077423\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08556503057479858\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.17544609308242798\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.016921648755669594\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.18940003216266632\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.2074796718053449e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3580380380153656\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08509046584367752\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.14080792665481567\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.010111786425113678\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.13607098162174225\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.1836657209940427e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3638267517089844\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.08207445591688156\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.9682862758636475\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4272301495075226\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04755692929029465\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06771395355463028\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.4454289972782135\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.05301334336400032\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.45629221200942993\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.16006599366664886\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0365300178527832\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.11141133308410645\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.18524082005023956\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2531066834926605\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06678085774183273\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.005957550834864378\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.06660372763872147\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 8.69724070540201e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.3259362578392029\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.0646090880036354\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09884513169527054\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.008294615894556046\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.10330914705991745\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 6.576941213864984e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3181544542312622\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06557008624076843\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.10463464260101318\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.008156404830515385\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.0951177254319191\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.253833370462587e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.4006463587284088\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.07450050115585327\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.08183316886425018\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007835044525563717\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.08366101235151291\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.21238830086412e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.312743604183197\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06591275334358215\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.06951108574867249\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.005547617562115192\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.06727154552936554\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.0800123018128716e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.31735166907310486\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06240295618772507\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.07792528718709946\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.005790631752461195\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.08471859246492386\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 7.698947035450487e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3292278051376343\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07458620518445969\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08478877693414688\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.006831591948866844\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08531631529331207\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.3663218334514227e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.31598854064941406\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07132300734519958\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07223980128765106\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.00612283032387495\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07045748829841614\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.862897638017841e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.3341235816478729\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.07089840620756149\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.8342349529266357\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.34044238924980164\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05433790385723114\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06347564607858658\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.38858598470687866\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03995697572827339\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3942651152610779\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.12804095447063446\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.2585935592651367\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.24364157021045685\n",
      "ab_loss: 23.422019958496094\n",
      "Gradient of AB Loss for row 0: tensor([-0.3356])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0219, -0.0317, -0.0203, -0.0383, -0.0297, -0.0250,  0.0226,  0.0414,\n",
      "        -0.0299, -0.0289, -0.0433,  0.0305,  0.0242, -0.0310, -0.0230,  0.0258,\n",
      "        -0.0327, -0.0237])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0388, -0.0482, -0.0418,  0.0525,  0.0431, -0.0376,  0.0342,  0.0380,\n",
      "         0.0479,  0.0430,  0.0329,  0.0378, -0.0358,  0.0343])\n",
      "Gradient of AB Loss for row 3: tensor([0.0279, 0.0461, 0.0196, 0.0272, 0.0378, 0.0293, 0.0387, 0.0338, 0.0469,\n",
      "        0.0398, 0.0364, 0.0248, 0.0260, 0.0395])\n",
      "Gradient of AB Loss for row 4: tensor([0.0388, 0.0349, 0.0350, 0.0251, 0.0457, 0.0399, 0.0405, 0.0446, 0.0436,\n",
      "        0.0350, 0.0372, 0.0393, 0.0370, 0.0362])\n",
      "Gradient of AB Loss for row 5: tensor([0.0450, 0.0441, 0.0217, 0.0293, 0.0314, 0.0405, 0.0337, 0.0404, 0.0329,\n",
      "        0.0418, 0.0435, 0.0340, 0.0419, 0.0227])\n",
      "Gradient of AB Loss for row 6: tensor([0.0388, 0.0338, 0.0352, 0.0357, 0.0433, 0.0348, 0.0367, 0.0612, 0.0550,\n",
      "        0.0462, 0.0360])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0341, -0.0273, -0.0306,  0.0300,  0.0209,  0.0144, -0.0288,  0.0309,\n",
      "         0.0444,  0.0332,  0.0283,  0.0367,  0.0232,  0.0309,  0.0239, -0.0317,\n",
      "         0.0295])\n",
      "Gradient of AB Loss for row 8: tensor([0.0264, 0.0335, 0.0113, 0.0384, 0.0309, 0.0351, 0.0334, 0.0346, 0.0265,\n",
      "        0.0448, 0.0430, 0.0214, 0.0261, 0.0350])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0385, -0.0289, -0.0316,  0.0401,  0.0436,  0.0351,  0.0175, -0.0285,\n",
      "         0.0309,  0.0385,  0.0218,  0.0448,  0.0255,  0.0382,  0.0225, -0.0253,\n",
      "         0.0292])\n",
      "Gradient of AB Loss for row 10: tensor([-0.4642])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0217,  0.0179, -0.0160, -0.0296,  0.0304, -0.0208,  0.0223,  0.0279,\n",
      "        -0.0317, -0.0237,  0.0249,  0.0146,  0.0293,  0.0249, -0.0245,  0.0154,\n",
      "         0.0238, -0.0187,  0.0224,  0.0147, -0.0246, -0.0273])\n",
      "Gradient of AB Loss for row 12: tensor([ 0.0288, -0.0475,  0.0506,  0.0196,  0.0307,  0.0350,  0.0362,  0.0371,\n",
      "         0.0387,  0.0566,  0.0357,  0.0306,  0.0300,  0.0162])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0510, -0.0233,  0.0353,  0.0475, -0.0384,  0.0391,  0.0347,  0.0310,\n",
      "         0.0394,  0.0505,  0.0383,  0.0234, -0.0454,  0.0353])\n",
      "Gradient of AB Loss for row 14: tensor([0.0399, 0.0511, 0.0223, 0.0274, 0.0471, 0.0461, 0.0385, 0.0380, 0.0564,\n",
      "        0.0372, 0.0301, 0.0438, 0.0295, 0.0318])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0298, -0.0501,  0.0492,  0.0325, -0.0501,  0.0515,  0.0429,  0.0556,\n",
      "         0.0398,  0.0555,  0.0429, -0.0385,  0.0383])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0409,  0.0446,  0.0212,  0.0477, -0.0345,  0.0416,  0.0432,  0.0419,\n",
      "         0.0406,  0.0407,  0.0310,  0.0280, -0.0421,  0.0300])\n",
      "Gradient of AB Loss for row 17: tensor([0.0419, 0.0493, 0.0122, 0.0376, 0.0472, 0.0369, 0.0378, 0.0392, 0.0435,\n",
      "        0.0622, 0.0328, 0.0367, 0.0469])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0509, -0.0633, -0.0466, -0.0468,  0.0373, -0.0481,  0.0577,  0.0396,\n",
      "         0.0351, -0.0438,  0.0360, -0.0365,  0.0433])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0439, -0.0306,  0.0479,  0.0091,  0.0327, -0.0319,  0.0259,  0.0403,\n",
      "         0.0402,  0.0398,  0.0278, -0.0372, -0.0151,  0.0165,  0.0457])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0255, -0.0591, -0.0449,  0.0428, -0.0419,  0.0481,  0.0391,  0.0497,\n",
      "         0.0511,  0.0344,  0.0410, -0.0426,  0.0276])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0368, -0.0291, -0.0259, -0.0258,  0.0179,  0.0163, -0.0122, -0.0201,\n",
      "         0.0254,  0.0360,  0.0385,  0.0418, -0.0284, -0.0399, -0.0399,  0.0260,\n",
      "        -0.0166])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0296, -0.0288, -0.0346, -0.0409,  0.0332,  0.0334,  0.0125, -0.0361,\n",
      "         0.0421,  0.0346,  0.0345,  0.0286, -0.0305, -0.0456, -0.0217,  0.0314])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0190, -0.0343,  0.0345,  0.0170,  0.0398, -0.0265,  0.0372,  0.0292,\n",
      "         0.0426,  0.0413,  0.0245, -0.0393, -0.0332,  0.0193,  0.0271])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0324,  0.0313,  0.0301,  0.0340,  0.0358,  0.0415,  0.0468,  0.0275,\n",
      "         0.0427,  0.0443,  0.0338,  0.0415,  0.0212,  0.0225])\n",
      "Gradient of AB Loss for row 25: tensor([0.0246, 0.0490, 0.0300, 0.0284, 0.0502, 0.0370, 0.0299, 0.0438, 0.0478,\n",
      "        0.0430, 0.0440, 0.0399, 0.0241, 0.0248])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0295, -0.0190,  0.0279,  0.0396,  0.0093, -0.0165, -0.0306,  0.0249,\n",
      "         0.0253,  0.0299,  0.0363,  0.0378,  0.0266,  0.0202, -0.0285,  0.0251,\n",
      "         0.0287])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0356, -0.0437,  0.0312,  0.0203,  0.0387,  0.0361,  0.0455,  0.0491,\n",
      "         0.0360,  0.0449,  0.0369, -0.0304,  0.0248])\n",
      "Gradient of AB Loss for row 28: tensor([0.0340, 0.0481, 0.0207, 0.0426, 0.0412, 0.0260, 0.0284, 0.0430, 0.0402,\n",
      "        0.0387, 0.0336, 0.0401, 0.0386, 0.0303])\n",
      "Gradient of AB Loss for row 29: tensor([0.0399, 0.0510, 0.0207, 0.0295, 0.0259, 0.0336, 0.0365, 0.0368, 0.0418,\n",
      "        0.0407, 0.0373, 0.0232, 0.0265, 0.0440])\n",
      "Gradient of AB Loss for row 30: tensor([-0.5531])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0268, -0.0253, -0.0249, -0.0435, -0.0417,  0.0306,  0.0240, -0.0257,\n",
      "         0.0278,  0.0443,  0.0375,  0.0398,  0.0494, -0.0244,  0.0163, -0.0147])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.12415915727615356\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.11998432129621506\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.20344607532024384\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.1291331052780151\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.13571955263614655\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.19744059443473816\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.34210845828056335\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1497737020254135\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.016383424401283264\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1676570177078247\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.2568673879442827e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3498237729072571\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.08593890070915222\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.16465817391872406\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.017881566658616066\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.18634635210037231\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.2189573794785247e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.36457544565200806\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.09149648249149323\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1597125381231308\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01933101750910282\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.17278823256492615\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.2619444378358935e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3809953033924103\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.0964791476726532\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.23311057686805725\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.021276166662573814\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.2346176654100418\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.2240963798149096e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3990370035171509\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.09274058789014816\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11380435526371002\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.01144987903535366\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.11021555960178375\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.1273059152827614e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.38993823528289795\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.09326543658971786\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.18777945637702942\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.013198676519095898\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.194883331656456\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.6399422930746255e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.35910800099372864\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09035904705524445\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.18452507257461548\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01853967271745205\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.19573403894901276\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.4746297516410323e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3765861690044403\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08837642520666122\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.12054374068975449\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.010125335305929184\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.13120914995670319\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.2214569355961657e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.37911882996559143\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.08671760559082031\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.9875802993774414\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4279511868953705\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.05435408651828766\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.07121504843235016\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.44147413969039917\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.054472822695970535\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.47299858927726746\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1730208545923233\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.1597081422805786\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.12778493762016296\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.21144329011440277\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2903847098350525\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.0905299186706543\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.007023102603852749\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.09187115728855133\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.8236889776090948e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.32999441027641296\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06897184252738953\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.0893966406583786\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.009369795210659504\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09544135630130768\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 9.003747747371449e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3491680920124054\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07484418153762817\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07864315807819366\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005508544389158487\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08211783319711685\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.0127354510558462e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.42341187596321106\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.083545982837677\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.0821995660662651\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007098938338458538\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.0802786573767662\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.5640075901046657e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3587090075016022\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07751855999231339\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08892933279275894\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.007344639394432306\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.0869210734963417\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.6104834132946166e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.367694228887558\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.0769449919462204\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.10050991177558899\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.009483156725764275\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.10747722536325455\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.038250707630084e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.35569676756858826\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07951915264129639\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.1046450212597847\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.010772449895739555\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.10702308267354965\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.2876913979553706e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3584781587123871\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.08132304251194\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.1050647497177124\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.010560376569628716\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.09914896637201309\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.817852559239725e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.33352893590927124\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.07341635227203369\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 3.0813252925872803\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.38880375027656555\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.056392595171928406\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.07410939782857895\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3987885117530823\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.04141169786453247\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.43338140845298767\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.14552247524261475\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.382216453552246\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2708543837070465\n",
      "ab_loss: 23.450796127319336\n",
      "Gradient of AB Loss for row 0: tensor([-0.0421,  0.0352,  0.0157,  0.0351, -0.0429,  0.0435,  0.0313,  0.0436,\n",
      "         0.0484,  0.0483,  0.0329, -0.0345, -0.0224,  0.0385])\n",
      "Gradient of AB Loss for row 1: tensor([-0.6333])\n",
      "Gradient of AB Loss for row 2: tensor([0.0302, 0.0502, 0.0185, 0.0340, 0.0509, 0.0308, 0.0346, 0.0408, 0.0439,\n",
      "        0.0359, 0.0294, 0.0315, 0.0275, 0.0382])\n",
      "Gradient of AB Loss for row 3: tensor([0.0321, 0.0446, 0.0136, 0.0383, 0.0513, 0.0401, 0.0302, 0.0395, 0.0497,\n",
      "        0.0439, 0.0366, 0.0306, 0.0292, 0.0239])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0367, -0.0360,  0.0478,  0.0096,  0.0449, -0.0421,  0.0301,  0.0282,\n",
      "         0.0431,  0.0480,  0.0428,  0.0344, -0.0337,  0.0273])\n",
      "Gradient of AB Loss for row 5: tensor([0.0263, 0.0445, 0.0158, 0.0328, 0.0381, 0.0408, 0.0304, 0.0428, 0.0352,\n",
      "        0.0455, 0.0405, 0.0262, 0.0322, 0.0345])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0424, -0.0343, -0.0406,  0.0194, -0.0118,  0.0384, -0.0352,  0.0329,\n",
      "        -0.0360,  0.0334,  0.0302,  0.0479,  0.0264, -0.0392, -0.0130])\n",
      "Gradient of AB Loss for row 7: tensor([0.0446, 0.0526, 0.0309, 0.0271, 0.0429, 0.0372, 0.0336, 0.0517, 0.0550,\n",
      "        0.0399, 0.0356, 0.0367, 0.0357, 0.0240])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0336, -0.0284,  0.0137,  0.0415,  0.0308,  0.0373, -0.0200, -0.0275,\n",
      "         0.0355,  0.0438,  0.0424,  0.0379,  0.0328, -0.0369,  0.0216,  0.0181])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0145,  0.0257,  0.0291, -0.0140, -0.0179, -0.0233, -0.0124, -0.0141,\n",
      "         0.0221,  0.0182, -0.0245, -0.0236, -0.0166, -0.0185,  0.0105, -0.0224,\n",
      "        -0.0147,  0.0257, -0.0172, -0.0099, -0.0183, -0.0191, -0.0218, -0.0122,\n",
      "        -0.0202])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0350, -0.0427,  0.0399,  0.0331,  0.0367, -0.0431,  0.0331,  0.0401,\n",
      "         0.0461,  0.0571,  0.0259, -0.0331, -0.0440,  0.0262,  0.0182])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0245, -0.0412, -0.0305,  0.0179, -0.0149,  0.0498, -0.0462,  0.0406,\n",
      "        -0.0453,  0.0439,  0.0364,  0.0243, -0.0330, -0.0210, -0.0194])\n",
      "Gradient of AB Loss for row 12: tensor([ 0.0356, -0.0306,  0.0474,  0.0373,  0.0300,  0.0526,  0.0299,  0.0368,\n",
      "         0.0348,  0.0476,  0.0262,  0.0331,  0.0270,  0.0186])\n",
      "Gradient of AB Loss for row 13: tensor([ 0.0367, -0.0331,  0.0538,  0.0414,  0.0320,  0.0462,  0.0348,  0.0201,\n",
      "         0.0385,  0.0458,  0.0302,  0.0323,  0.0238,  0.0277])\n",
      "Gradient of AB Loss for row 14: tensor([0.0412, 0.0448, 0.0322, 0.0255, 0.0478, 0.0391, 0.0282, 0.0322, 0.0504,\n",
      "        0.0441, 0.0384, 0.0399, 0.0305, 0.0262])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0241, -0.0463, -0.0448,  0.0397,  0.0384, -0.0414,  0.0286,  0.0357,\n",
      "         0.0568,  0.0376,  0.0331,  0.0278, -0.0378,  0.0410])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0321, -0.0348, -0.0284, -0.0320, -0.0394,  0.0370,  0.0140, -0.0138,\n",
      "         0.0225,  0.0418, -0.0338,  0.0259, -0.0348, -0.0241, -0.0238, -0.0200,\n",
      "        -0.0171])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0330, -0.0349, -0.0250,  0.0306,  0.0207, -0.0323,  0.0379,  0.0301,\n",
      "         0.0311,  0.0458,  0.0434,  0.0331, -0.0269, -0.0334, -0.0226, -0.0189])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0317, -0.0151, -0.0213, -0.0349, -0.0349, -0.0188,  0.0279, -0.0243,\n",
      "        -0.0218, -0.0284,  0.0249,  0.0143, -0.0227, -0.0149, -0.0163,  0.0277,\n",
      "        -0.0310, -0.0268, -0.0252])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0405,  0.0363,  0.0122,  0.0456, -0.0347,  0.0329,  0.0345,  0.0429,\n",
      "         0.0419,  0.0405,  0.0300,  0.0310, -0.0318,  0.0340])\n",
      "Gradient of AB Loss for row 20: tensor([-0.4630])\n",
      "Gradient of AB Loss for row 21: tensor([0.0283, 0.0260, 0.0156, 0.0258, 0.0240, 0.0370, 0.0440, 0.0302, 0.0380,\n",
      "        0.0457, 0.0388, 0.0193, 0.0333, 0.0343])\n",
      "Gradient of AB Loss for row 22: tensor([0.0299, 0.0405, 0.0120, 0.0271, 0.0281, 0.0427, 0.0257, 0.0496, 0.0483,\n",
      "        0.0463, 0.0479, 0.0346, 0.0260, 0.0527])\n",
      "Gradient of AB Loss for row 23: tensor([0.0321, 0.0266, 0.0256, 0.0268, 0.0323, 0.0331, 0.0481, 0.0305, 0.0333,\n",
      "        0.0332, 0.0360, 0.0427, 0.0368])\n",
      "Gradient of AB Loss for row 24: tensor([0.0275, 0.0440, 0.0284, 0.0215, 0.0381, 0.0309, 0.0357, 0.0351, 0.0409,\n",
      "        0.0445, 0.0318, 0.0317, 0.0295, 0.0265])\n",
      "Gradient of AB Loss for row 25: tensor([-0.5175])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0220, -0.0336,  0.0384,  0.0204,  0.0094,  0.0389,  0.0365,  0.0337,\n",
      "         0.0210,  0.0255,  0.0411,  0.0258, -0.0233,  0.0230, -0.0421,  0.0160,\n",
      "        -0.0223,  0.0228])\n",
      "Gradient of AB Loss for row 27: tensor([0.1481, 0.1097, 0.1191, 0.1511])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0230, -0.0623, -0.0558,  0.0450, -0.0503,  0.0354,  0.0353,  0.0577,\n",
      "         0.0438,  0.0472,  0.0201, -0.0422,  0.0287])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0369,  0.0382, -0.0377,  0.0362,  0.0205,  0.0251,  0.0359,  0.0422,\n",
      "         0.0372,  0.0590,  0.0344,  0.0406,  0.0268,  0.0288])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0374, -0.0306, -0.0500, -0.0339, -0.0454,  0.0404, -0.0262,  0.0418,\n",
      "         0.0502,  0.0387,  0.0230, -0.0294,  0.0346,  0.0233, -0.0274])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0415, -0.0390,  0.0409,  0.0211, -0.0261,  0.0469,  0.0343,  0.0169,\n",
      "         0.0356,  0.0367,  0.0316, -0.0352,  0.0191, -0.0214,  0.0153, -0.0104])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10662450641393661\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.1166008859872818\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.18305188417434692\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0800367593765259\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11320777982473373\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.19361747801303864\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2747180461883545\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.19083906710147858\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.01778372749686241\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.20864905416965485\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 2.764946716027339e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3605320453643799\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.09574509412050247\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.14036111533641815\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.01538900751620531\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.16578038036823273\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.06350483974893e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3444008529186249\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.09418387711048126\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.12963679432868958\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.014779512770473957\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.13265912234783173\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.4320177266213818e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.33745867013931274\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09252758324146271\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.1429489105939865\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.012969397008419037\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.15260131657123566\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.370325075633616e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3775802254676819\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.09506569057703018\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.15114574134349823\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.02364506386220455\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.14966072142124176\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.389036663468346e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.379783570766449\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.10256808251142502\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.15838918089866638\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.016550838947296143\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.16127601265907288\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.450178976902805e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.37502992153167725\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09856178611516953\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.16629327833652496\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.012354783713817596\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.17177312076091766\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.2310766850376353e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.37106460332870483\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08982237428426743\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.13139554858207703\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.018274933099746704\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.1438940316438675\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.4646852619648598e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3821336328983307\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.08473024517297745\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.956671714782715\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4562631845474243\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.05159926041960716\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06446246057748795\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.4036277234554291\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04403708875179291\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.4224212169647217\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1399253010749817\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.036396861076355\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.1026482954621315\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1887047290802002\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2301749736070633\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08795543015003204\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.006969931069761515\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.09045401215553284\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.3900854911597094e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.31889283657073975\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06366697698831558\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.10260133445262909\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.010139839723706245\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.10300280153751373\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 9.002906753430295e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3447211682796478\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07824934273958206\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.09894799441099167\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.010151687078177929\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.09274265170097351\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.7373639193962731e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.389780730009079\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.08008456230163574\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09502118825912476\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.009307126514613628\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.10259240120649338\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.0986976883842203e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3266410529613495\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.0679231807589531\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08472263813018799\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.007176732178777456\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.08676932007074356\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 8.158417830195219e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3309091329574585\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06567685306072235\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.09611158818006516\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.009990713559091091\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.10208264738321304\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 9.59513246634458e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.32765018939971924\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.0691109448671341\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.10110601782798767\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.008321207948029041\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.10470087826251984\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.5482316539916496e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.35705429315567017\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07490669935941696\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.09831701964139938\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.009261732921004295\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.09242034703493118\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 2.268561338780728e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.33188560605049133\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.07361910492181778\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.885162353515625\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3586817979812622\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05273821949958801\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.059627775102853775\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.4251291751861572\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.040981680154800415\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3850913345813751\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.12071499973535538\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.2507283687591553\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.22694522142410278\n",
      "ab_loss: 23.053791046142578\n",
      "Gradient of AB Loss for row 0: tensor([-0.0224, -0.0355, -0.0286, -0.0380,  0.0287,  0.0192, -0.0198, -0.0214,\n",
      "         0.0222, -0.0255,  0.0345, -0.0316,  0.0336, -0.0170,  0.0287, -0.0197])\n",
      "Gradient of AB Loss for row 1: tensor([0.0175, 0.0395, 0.0174, 0.0292, 0.0387, 0.0384, 0.0326, 0.0367, 0.0509,\n",
      "        0.0331, 0.0395, 0.0368, 0.0322, 0.0330])\n",
      "Gradient of AB Loss for row 2: tensor([0.0394, 0.0356, 0.0203, 0.0315, 0.0377, 0.0416, 0.0353, 0.0526, 0.0380,\n",
      "        0.0220, 0.0354, 0.0382, 0.0360, 0.0313])\n",
      "Gradient of AB Loss for row 3: tensor([0.1671, 0.1391, 0.1139, 0.1252])\n",
      "Gradient of AB Loss for row 4: tensor([0.0289, 0.0367, 0.0278, 0.0205, 0.0464, 0.0404, 0.0449, 0.0394, 0.0449,\n",
      "        0.0341, 0.0442, 0.0284, 0.0266, 0.0370])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0382, -0.0424, -0.0320,  0.0106, -0.0170, -0.0430,  0.0326,  0.0479,\n",
      "        -0.0396,  0.0242, -0.0332, -0.0215, -0.0388])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0275, -0.0313, -0.0374,  0.0358,  0.0304,  0.0334,  0.0135,  0.0299,\n",
      "         0.0423,  0.0241,  0.0410,  0.0357,  0.0302,  0.0230,  0.0234, -0.0278,\n",
      "        -0.0311,  0.0177])\n",
      "Gradient of AB Loss for row 7: tensor([0.0400, 0.0485, 0.0309, 0.0330, 0.0428, 0.0294, 0.0422, 0.0451, 0.0419,\n",
      "        0.0408, 0.0339, 0.0342, 0.0271, 0.0255])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0244, -0.0168, -0.0412, -0.0462, -0.0328,  0.0421, -0.0348,  0.0260,\n",
      "         0.0418,  0.0442,  0.0379, -0.0434, -0.0332, -0.0285, -0.0339])\n",
      "Gradient of AB Loss for row 9: tensor([0.4939])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0162, -0.0253, -0.0176, -0.0173, -0.0126, -0.0394, -0.0311, -0.0260,\n",
      "         0.0318, -0.0203, -0.0218, -0.0232, -0.0269,  0.0240, -0.0274, -0.0091,\n",
      "        -0.0257, -0.0146,  0.0241, -0.0280, -0.0198])\n",
      "Gradient of AB Loss for row 11: tensor([0.0234, 0.0494, 0.0242, 0.0337, 0.0551, 0.0207, 0.0228, 0.0318, 0.0427,\n",
      "        0.0425, 0.0395, 0.0331, 0.0279, 0.0254])\n",
      "Gradient of AB Loss for row 12: tensor([0.0290, 0.0416, 0.0162, 0.0278, 0.0341, 0.0303, 0.0216, 0.0323, 0.0446,\n",
      "        0.0262, 0.0385, 0.0435, 0.0299, 0.0245])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0480,  0.0537,  0.0343, -0.0436, -0.0320,  0.0491,  0.0531,  0.0521,\n",
      "         0.0588, -0.0506, -0.0316,  0.0556])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0393, -0.0401, -0.0378, -0.0208, -0.0363,  0.0470,  0.0378,  0.0273,\n",
      "        -0.0479, -0.0337,  0.0504,  0.0302, -0.0433, -0.0297])\n",
      "Gradient of AB Loss for row 15: tensor([0.2177, 0.1978, 0.1937])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0135, -0.0203, -0.0196,  0.0091, -0.0158, -0.0229, -0.0208, -0.0163,\n",
      "         0.0226, -0.0200,  0.0245, -0.0212, -0.0184,  0.0177, -0.0170, -0.0275,\n",
      "         0.0112, -0.0178, -0.0203, -0.0158,  0.0282, -0.0250,  0.0144])\n",
      "Gradient of AB Loss for row 17: tensor([0.0357, 0.0455, 0.0181, 0.0239, 0.0429, 0.0373, 0.0414, 0.0488, 0.0585,\n",
      "        0.0436, 0.0416, 0.0422, 0.0190, 0.0296])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0290, -0.0197, -0.0198, -0.0343, -0.0262, -0.0211,  0.0215, -0.0208,\n",
      "        -0.0231, -0.0275,  0.0175,  0.0286,  0.0208, -0.0327,  0.0312, -0.0144,\n",
      "        -0.0153,  0.0241,  0.0240, -0.0281, -0.0328])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0453, -0.0414,  0.0184, -0.0279,  0.0404, -0.0397,  0.0361,  0.0498,\n",
      "         0.0528,  0.0258,  0.0297, -0.0338, -0.0209,  0.0376])\n",
      "Gradient of AB Loss for row 20: tensor([0.0457, 0.0424, 0.0229, 0.0299, 0.0456, 0.0412, 0.0346, 0.0438, 0.0441,\n",
      "        0.0374, 0.0376, 0.0345, 0.0246, 0.0197])\n",
      "Gradient of AB Loss for row 21: tensor([-0.4691])\n",
      "Gradient of AB Loss for row 22: tensor([ 0.0328, -0.0421,  0.0429,  0.0321,  0.0276,  0.0464,  0.0424,  0.0387,\n",
      "         0.0444,  0.0353,  0.0421,  0.0339,  0.0274,  0.0334])\n",
      "Gradient of AB Loss for row 23: tensor([0.0759, 0.1374, 0.1316, 0.1201])\n",
      "Gradient of AB Loss for row 24: tensor([0.0233, 0.0203, 0.0233, 0.0208, 0.0242, 0.0161, 0.0321, 0.0307, 0.0198,\n",
      "        0.0179, 0.0218, 0.0087, 0.0257, 0.0186, 0.0215, 0.0178, 0.0251, 0.0264,\n",
      "        0.0285, 0.0246, 0.0248, 0.0185, 0.0140])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0274, -0.0333,  0.0181,  0.0383, -0.0421,  0.0427,  0.0382,  0.0480,\n",
      "         0.0413,  0.0311,  0.0270, -0.0198, -0.0236,  0.0229,  0.0212])\n",
      "Gradient of AB Loss for row 26: tensor([0.0223, 0.0359, 0.0222, 0.0299, 0.0409, 0.0405, 0.0279, 0.0336, 0.0495,\n",
      "        0.0373, 0.0334, 0.0293, 0.0186, 0.0411])\n",
      "Gradient of AB Loss for row 27: tensor([0.0307, 0.0419, 0.0173, 0.0250, 0.0364, 0.0356, 0.0440, 0.0406, 0.0427,\n",
      "        0.0437, 0.0344, 0.0317, 0.0276, 0.0271])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0407, -0.0346, -0.0432, -0.0471, -0.0302,  0.0446, -0.0330,  0.0399,\n",
      "         0.0390,  0.0400,  0.0406, -0.0453, -0.0416, -0.0223])\n",
      "Gradient of AB Loss for row 29: tensor([0.0395, 0.0436, 0.0102, 0.0289, 0.0379, 0.0522, 0.0432, 0.0358, 0.0566,\n",
      "        0.0421, 0.0425, 0.0327, 0.0291, 0.0351])\n",
      "Gradient of AB Loss for row 30: tensor([0.0339, 0.0440, 0.0172, 0.0327, 0.0470, 0.0387, 0.0467, 0.0387, 0.0476,\n",
      "        0.0367, 0.0323, 0.0384, 0.0202, 0.0306])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0262, -0.0306, -0.0218, -0.0208, -0.0195, -0.0305, -0.0238, -0.0202,\n",
      "         0.0229, -0.0220, -0.0177, -0.0209, -0.0252,  0.0218, -0.0212, -0.0266,\n",
      "        -0.0140, -0.0118, -0.0179,  0.0331, -0.0266, -0.0219])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08799314498901367\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.08582133799791336\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.15485186874866486\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.8940281867980957\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.0980416014790535\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.15627731382846832\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.24798241257667542\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.13889984786510468\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.013481294736266136\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.14717723429203033\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 2.0250385812659033e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.26361900568008423\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07290217280387878\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.10367359220981598\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.010276386514306068\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.11445881426334381\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 9.63067514625493e-10\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2766624689102173\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08021384477615356\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.10172250121831894\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01387194823473692\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.11108563840389252\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.0775090819592492e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3005349338054657\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.08022911846637726\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.10134739428758621\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.008038965053856373\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.10054375976324081\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.6817355286136149e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.30860838294029236\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07471106946468353\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11016169935464859\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.008740942925214767\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10988917201757431\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.4695999972502705e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.29993152618408203\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07155885547399521\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.1081872507929802\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.011860822327435017\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.10761858522891998\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 2.3935098347749317e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.2997685968875885\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.0780147910118103\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.13603192567825317\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.015287287533283234\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.14245566725730896\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.3686330957440873e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.27513954043388367\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07207641750574112\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.16553319990634918\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.01002871710807085\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.15896880626678467\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.6050362150465958e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.31855475902557373\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07581467181444168\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.449204444885254\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3669911324977875\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.03658895567059517\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.05220402404665947\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.34430116415023804\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.039010707288980484\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3504774272441864\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.12450564652681351\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.962384045124054\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09924230724573135\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.15539342164993286\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.20696991682052612\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08145851641893387\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.007207606919109821\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.08323343098163605\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 8.639803872334539e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.28339117765426636\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05961243808269501\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.08315949141979218\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.00857655517756939\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.08838137984275818\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.5393892827120226e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.30665406584739685\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06575530022382736\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07336127012968063\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006474561523646116\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06861457973718643\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 8.998038425467314e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3325318396091461\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06485210359096527\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.06848430633544922\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.006255808752030134\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.06452611833810806\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.077285816108997e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3005315959453583\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06329628825187683\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.06451212614774704\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.005149420816451311\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.06187307462096214\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 9.92269821864511e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.30632346868515015\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06211312487721443\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08146774768829346\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.008021443150937557\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09162695705890656\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.1521814613502102e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.29499801993370056\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06645465642213821\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.06365039944648743\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.004748588427901268\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.06758284568786621\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 9.259231714686678e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.28416723012924194\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06367561221122742\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.0726025179028511\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.006382387597113848\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07218717038631439\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.0061634858615776e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.3016244173049927\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06538455933332443\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.671882152557373\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.32658451795578003\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05350577086210251\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.058270640671253204\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3212178945541382\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03172760456800461\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.33161279559135437\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.10268353670835495\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.8414515256881714\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.19180016219615936\n",
      "ab_loss: 24.062082290649414\n",
      "Gradient of AB Loss for row 0: tensor([0.1424, 0.1551, 0.0773, 0.1437])\n",
      "Gradient of AB Loss for row 1: tensor([0.0440, 0.0544, 0.0325, 0.0263, 0.0342, 0.0420, 0.0434, 0.0460, 0.0413,\n",
      "        0.0257, 0.0422, 0.0376, 0.0263, 0.0229])\n",
      "Gradient of AB Loss for row 2: tensor([ 0.0350, -0.0420,  0.0471,  0.0226, -0.0454,  0.0418,  0.0304, -0.0246,\n",
      "         0.0506,  0.0352, -0.0308,  0.0375, -0.0188,  0.0260])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0308, -0.0379, -0.0458, -0.0243, -0.0103, -0.0252, -0.0392, -0.0287,\n",
      "        -0.0308, -0.0181, -0.0343, -0.0340, -0.0356, -0.0340, -0.0249, -0.0183,\n",
      "        -0.0230])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0222, -0.0193, -0.0228, -0.0233, -0.0258, -0.0239, -0.0286, -0.0217,\n",
      "        -0.0222, -0.0107, -0.0215, -0.0207, -0.0221, -0.0113, -0.0102, -0.0190,\n",
      "        -0.0076, -0.0142, -0.0085,  0.0260, -0.0205, -0.0148, -0.0248])\n",
      "Gradient of AB Loss for row 5: tensor([0.0389, 0.0435, 0.0208, 0.0273, 0.0417, 0.0493, 0.0349, 0.0365, 0.0621,\n",
      "        0.0447, 0.0360, 0.0360, 0.0323, 0.0263])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0244, -0.0583, -0.0418,  0.0462,  0.0329, -0.0440,  0.0318,  0.0415,\n",
      "         0.0539,  0.0447,  0.0389,  0.0231, -0.0229,  0.0400])\n",
      "Gradient of AB Loss for row 7: tensor([0.0406, 0.0498, 0.0154, 0.0302, 0.0437, 0.0413, 0.0430, 0.0463, 0.0526,\n",
      "        0.0456, 0.0405, 0.0357, 0.0299, 0.0267])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0231,  0.0225, -0.0211, -0.0175,  0.0314, -0.0263, -0.0161, -0.0260,\n",
      "         0.0297,  0.0130,  0.0289,  0.0274, -0.0215,  0.0237, -0.0233, -0.0211,\n",
      "        -0.0156,  0.0278,  0.0272, -0.0287, -0.0198])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0283, -0.0416, -0.0527, -0.0493, -0.0341,  0.0400, -0.0509,  0.0365,\n",
      "         0.0324,  0.0404,  0.0323,  0.0333, -0.0218, -0.0300])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0349,  0.0317, -0.0316, -0.0502, -0.0400,  0.0290, -0.0216,  0.0412,\n",
      "         0.0240,  0.0236, -0.0410,  0.0284, -0.0427, -0.0214,  0.0244, -0.0216])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0550, -0.0357,  0.0435,  0.0432, -0.0355,  0.0388,  0.0307,  0.0447,\n",
      "         0.0394,  0.0502,  0.0316,  0.0255, -0.0351,  0.0273])\n",
      "Gradient of AB Loss for row 12: tensor([-0.2100, -0.2056])\n",
      "Gradient of AB Loss for row 13: tensor([0.1252, 0.1401, 0.1466, 0.1218])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0289, -0.0258, -0.0241,  0.0348,  0.0390,  0.0237,  0.0135,  0.0357,\n",
      "         0.0426,  0.0168,  0.0401,  0.0333,  0.0370,  0.0210,  0.0326, -0.0254,\n",
      "        -0.0229,  0.0173])\n",
      "Gradient of AB Loss for row 15: tensor([-0.5124])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0352,  0.0412,  0.0450,  0.0213, -0.0453,  0.0370, -0.0410,  0.0253,\n",
      "         0.0464, -0.0340,  0.0373,  0.0244, -0.0406,  0.0296,  0.0355])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0228,  0.0396,  0.0338,  0.0274,  0.0361,  0.0333,  0.0379,  0.0249,\n",
      "         0.0452,  0.0452,  0.0420,  0.0225,  0.0220,  0.0192, -0.0212,  0.0291])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0352, -0.0368,  0.0323,  0.0232,  0.0375,  0.0304,  0.0388,  0.0269,\n",
      "         0.0412,  0.0298,  0.0508, -0.0255,  0.0413,  0.0296, -0.0162, -0.0151])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0287, -0.0227, -0.0335, -0.0497, -0.0328,  0.0302, -0.0387,  0.0364,\n",
      "         0.0350,  0.0456,  0.0334, -0.0386, -0.0432, -0.0241, -0.0364])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0208, -0.0502, -0.0415,  0.0304,  0.0215,  0.0328,  0.0434,  0.0365,\n",
      "         0.0390,  0.0523,  0.0350,  0.0346, -0.0320,  0.0277])\n",
      "Gradient of AB Loss for row 21: tensor([0.0378, 0.0383, 0.0298, 0.0195, 0.0436, 0.0394, 0.0389, 0.0397, 0.0466,\n",
      "        0.0445, 0.0498, 0.0316, 0.0316, 0.0364])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0403, -0.0429,  0.0199,  0.0496, -0.0479,  0.0417,  0.0449,  0.0540,\n",
      "         0.0378,  0.0467,  0.0331, -0.0334,  0.0461])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0325,  0.0390, -0.0376,  0.0540,  0.0173,  0.0239,  0.0379,  0.0400,\n",
      "         0.0387,  0.0564,  0.0284,  0.0431,  0.0364,  0.0468])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0426, -0.0389, -0.0451, -0.0301, -0.0364, -0.0425,  0.0405, -0.0328,\n",
      "         0.0282,  0.0438,  0.0394,  0.0445, -0.0353, -0.0246])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0517,  0.0505,  0.0412, -0.0443,  0.0212,  0.0307,  0.0474,  0.0273,\n",
      "         0.0445,  0.0388,  0.0462,  0.0351, -0.0476,  0.0248])\n",
      "Gradient of AB Loss for row 26: tensor([0.0352, 0.0482, 0.0242, 0.0319, 0.0475, 0.0258, 0.0342, 0.0382, 0.0530,\n",
      "        0.0329, 0.0486, 0.0418, 0.0144, 0.0296])\n",
      "Gradient of AB Loss for row 27: tensor([0.0405, 0.0385, 0.0205, 0.0236, 0.0412, 0.0343, 0.0289, 0.0294, 0.0426,\n",
      "        0.0226, 0.0358, 0.0356, 0.0295, 0.0358])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0304, -0.0160, -0.0207, -0.0335, -0.0340, -0.0296, -0.0138, -0.0274,\n",
      "         0.0235, -0.0192, -0.0219, -0.0330,  0.0181,  0.0179,  0.0283, -0.0253,\n",
      "        -0.0217,  0.0290, -0.0291, -0.0319])\n",
      "Gradient of AB Loss for row 29: tensor([ 0.0317, -0.0467,  0.0426,  0.0160,  0.0318,  0.0349,  0.0346,  0.0220,\n",
      "         0.0295,  0.0437,  0.0465,  0.0357,  0.0241,  0.0277])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0385, -0.0409,  0.0403, -0.0375,  0.0210,  0.0450, -0.0181,  0.0386,\n",
      "         0.0418,  0.0311,  0.0275,  0.0453,  0.0298, -0.0329])\n",
      "Gradient of AB Loss for row 31: tensor([-0.5930])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10523345321416855\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09141483157873154\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1876065880060196\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.055516004562378\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11394649744033813\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.16398459672927856\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2719986140727997\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.16385620832443237\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.014745952561497688\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1844272017478943\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 2.3061668130708313e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.33073747158050537\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.08360984176397324\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.19835197925567627\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.0192726943641901\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.23076729476451874\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.4563066308426187e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3200813829898834\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08770007640123367\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.16687752306461334\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01568729616701603\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.18564096093177795\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.7515261463429965e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3502857983112335\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.08725834637880325\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.1539801061153412\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.019779110327363014\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.15899138152599335\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.3086557393293674e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3716224730014801\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.09479181468486786\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.1353244036436081\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.015087684616446495\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.13700810074806213\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 2.017170430690385e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.371793270111084\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08898811042308807\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.13841885328292847\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.01382899284362793\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.14334140717983246\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.8198061946250732e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.33441829681396484\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08292245119810104\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.18109147250652313\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.019256995990872383\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.1978001445531845\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 2.380713404193102e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.35882192850112915\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08239124715328217\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.1605370193719864\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.021031267940998077\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.1816403716802597\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.8388257583268341e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.38800907135009766\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.09367188066244125\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.897008180618286\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4356694519519806\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.049627482891082764\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06192825734615326\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3573463559150696\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04185029864311218\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.40814369916915894\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1362280696630478\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0089607238769531\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09827649593353271\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.16823139786720276\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.21183539927005768\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.0886392667889595\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.008345327340066433\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.08860839903354645\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.504620317227534e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.33019912242889404\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06654577702283859\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09264091402292252\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.008647807873785496\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09787839651107788\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.0592319243940551e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.32837551832199097\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07373599708080292\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08403939753770828\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006357791367918253\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08130752295255661\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.043694797253636e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3615880608558655\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.07286346703767776\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.0921752005815506\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007146513555198908\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.09275004267692566\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.1794102361406544e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3355734050273895\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07188701629638672\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.10510861873626709\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.00854543037712574\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.09762288630008698\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.82684245508824e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.32593822479248047\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06979339569807053\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.0894080176949501\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.008669535629451275\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.10143053531646729\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.555134576669559e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.30991649627685547\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07174351811408997\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.09952737390995026\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.00941710826009512\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.09042919427156448\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.565428453531581e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.2975049614906311\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06469601392745972\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.09855661541223526\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.009777185507118702\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.0966850221157074\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 2.58618237936048e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.30590224266052246\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06988467276096344\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.819096565246582\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.35402289032936096\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05091894790530205\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.060329604893922806\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.4022708237171173\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.038877736777067184\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.36627572774887085\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.10931989550590515\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.9988127946853638\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.20006787776947021\n",
      "ab_loss: 24.048587799072266\n",
      "Gradient of AB Loss for row 0: tensor([-0.0220, -0.0306, -0.0279, -0.0359,  0.0337,  0.0289,  0.0125, -0.0236,\n",
      "         0.0273,  0.0411,  0.0401,  0.0336,  0.0342,  0.0342, -0.0274, -0.0359,\n",
      "        -0.0267,  0.0171])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0338, -0.0404,  0.0286,  0.0262,  0.0197, -0.0268,  0.0363, -0.0412,\n",
      "         0.0450,  0.0421,  0.0402,  0.0268,  0.0271,  0.0166, -0.0361])\n",
      "Gradient of AB Loss for row 2: tensor([0.1183, 0.1392, 0.1562, 0.1409])\n",
      "Gradient of AB Loss for row 3: tensor([ 0.0309, -0.0376,  0.0368,  0.0264,  0.0271,  0.0424,  0.0318,  0.0465,\n",
      "         0.0322,  0.0393,  0.0340,  0.0247,  0.0297,  0.0263])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0263, -0.0200, -0.0236, -0.0269, -0.0159,  0.0246, -0.0218, -0.0244,\n",
      "        -0.0297,  0.0238,  0.0280,  0.0256, -0.0195,  0.0345, -0.0115,  0.0305,\n",
      "         0.0272, -0.0333, -0.0294])\n",
      "Gradient of AB Loss for row 5: tensor([ 0.0328, -0.0238,  0.0362,  0.0385,  0.0255,  0.0122,  0.0182,  0.0142,\n",
      "         0.0118,  0.0406,  0.0239,  0.0251,  0.0248, -0.0152,  0.0321,  0.0319,\n",
      "         0.0221,  0.0174,  0.0206])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0400,  0.0422, -0.0363,  0.0473,  0.0181, -0.0476,  0.0427,  0.0484,\n",
      "         0.0496,  0.0440,  0.0353,  0.0228,  0.0206])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0370, -0.0345, -0.0374,  0.0251,  0.0309,  0.0327,  0.0110,  0.0323,\n",
      "         0.0431,  0.0273,  0.0368,  0.0245,  0.0351,  0.0243,  0.0217, -0.0244,\n",
      "         0.0263,  0.0255])\n",
      "Gradient of AB Loss for row 8: tensor([0.0336, 0.0438, 0.0204, 0.0299, 0.0365, 0.0302, 0.0284, 0.0343, 0.0445,\n",
      "        0.0442, 0.0287, 0.0353, 0.0310, 0.0239])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0217, -0.0251, -0.0223, -0.0131, -0.0249, -0.0301, -0.0194,  0.0274,\n",
      "         0.0259, -0.0203, -0.0156, -0.0224,  0.0104, -0.0265,  0.0243, -0.0246,\n",
      "        -0.0169, -0.0163, -0.0160,  0.0225, -0.0267, -0.0179])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0388,  0.0350,  0.0175,  0.0239,  0.0427,  0.0349,  0.0291,  0.0357,\n",
      "         0.0557,  0.0482,  0.0337,  0.0382, -0.0376,  0.0468])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0484,  0.0481,  0.0402, -0.0411,  0.0319,  0.0360,  0.0357,  0.0227,\n",
      "         0.0484,  0.0235,  0.0327,  0.0337, -0.0329,  0.0387])\n",
      "Gradient of AB Loss for row 12: tensor([0.1741, 0.1431, 0.1601, 0.1259])\n",
      "Gradient of AB Loss for row 13: tensor([0.0447, 0.0452, 0.0291, 0.0224, 0.0334, 0.0428, 0.0344, 0.0400, 0.0387,\n",
      "        0.0602, 0.0399, 0.0402, 0.0356])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0250, -0.0126, -0.0209, -0.0371, -0.0329, -0.0237, -0.0196, -0.0226,\n",
      "        -0.0218, -0.0248, -0.0279, -0.0236, -0.0230, -0.0208, -0.0214, -0.0170,\n",
      "        -0.0168,  0.0277, -0.0311, -0.0276, -0.0307])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0397,  0.0496,  0.0436,  0.0313,  0.0294,  0.0433,  0.0400,  0.0377,\n",
      "         0.0561,  0.0447,  0.0404,  0.0242,  0.0351,  0.0360])\n",
      "Gradient of AB Loss for row 16: tensor([0.0235, 0.0466, 0.0215, 0.0278, 0.0522, 0.0274, 0.0516, 0.0406, 0.0578,\n",
      "        0.0399, 0.0411, 0.0263, 0.0328])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0288, -0.0302, -0.0257, -0.0409,  0.0340,  0.0143,  0.0229,  0.0167,\n",
      "        -0.0176, -0.0371,  0.0179,  0.0455,  0.0378,  0.0240,  0.0220,  0.0250,\n",
      "         0.0239])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0308, -0.0251, -0.0336,  0.0407,  0.0332,  0.0072,  0.0271,  0.0415,\n",
      "         0.0359,  0.0301,  0.0386,  0.0293, -0.0230,  0.0360, -0.0419, -0.0215,\n",
      "         0.0288])\n",
      "Gradient of AB Loss for row 19: tensor([0.0289, 0.0520, 0.0315, 0.0312, 0.0451, 0.0411, 0.0345, 0.0443, 0.0489,\n",
      "        0.0276, 0.0346, 0.0357, 0.0307, 0.0256])\n",
      "Gradient of AB Loss for row 20: tensor([ 0.0344, -0.0415,  0.0470,  0.0136,  0.0223,  0.0383,  0.0303,  0.0261,\n",
      "         0.0330,  0.0398,  0.0490,  0.0417,  0.0368,  0.0455])\n",
      "Gradient of AB Loss for row 21: tensor([0.1472, 0.1472, 0.1082, 0.1059])\n",
      "Gradient of AB Loss for row 22: tensor([-0.3989])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0511,  0.0514,  0.0320,  0.0423,  0.0477,  0.0300,  0.0418,  0.0546,\n",
      "         0.0450,  0.0300,  0.0297, -0.0470,  0.0462])\n",
      "Gradient of AB Loss for row 24: tensor([0.0259, 0.0433, 0.0313, 0.0268, 0.0444, 0.0384, 0.0367, 0.0408, 0.0569,\n",
      "        0.0258, 0.0362, 0.0380, 0.0291, 0.0225])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0399, -0.0403, -0.0461, -0.0268, -0.0439,  0.0347, -0.0268,  0.0362,\n",
      "         0.0530,  0.0395,  0.0510,  0.0403,  0.0309, -0.0419, -0.0262])\n",
      "Gradient of AB Loss for row 26: tensor([0.0363, 0.0461, 0.0310, 0.0243, 0.0423, 0.0423, 0.0375, 0.0531, 0.0393,\n",
      "        0.0414, 0.0354, 0.0390, 0.0264, 0.0304])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0489, -0.0488,  0.0464,  0.0279,  0.0358,  0.0302,  0.0304,  0.0469,\n",
      "         0.0467,  0.0575,  0.0492,  0.0336, -0.0386,  0.0404])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0254, -0.0428,  0.0371,  0.0374,  0.0367,  0.0463,  0.0282,  0.0502,\n",
      "         0.0485,  0.0360,  0.0175, -0.0403,  0.0438])\n",
      "Gradient of AB Loss for row 29: tensor([ 0.0389, -0.0411,  0.0485,  0.0178,  0.0480, -0.0435,  0.0466,  0.0384,\n",
      "         0.0608,  0.0410,  0.0466, -0.0354, -0.0264,  0.0342])\n",
      "Gradient of AB Loss for row 30: tensor([0.0280, 0.0379, 0.0209, 0.0377, 0.0492, 0.0298, 0.0454, 0.0386, 0.0524,\n",
      "        0.0369, 0.0231, 0.0335, 0.0259, 0.0277])\n",
      "Gradient of AB Loss for row 31: tensor([0.0339, 0.0503, 0.0199, 0.0351, 0.0459, 0.0325, 0.0385, 0.0430, 0.0580,\n",
      "        0.0283, 0.0483, 0.0377, 0.0339, 0.0321])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.12228454649448395\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.14208610355854034\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.17890000343322754\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0927120447158813\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11430414766073227\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.24691489338874817\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2822483479976654\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1507081538438797\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.012642946094274521\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1650964468717575\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.0413714335300028e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.32325509190559387\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.06903310865163803\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.14908096194267273\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.013247041031718254\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.1730635166168213\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.02319441808163e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.356936514377594\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07828278839588165\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.165727436542511\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01697433926165104\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.17500193417072296\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 9.71913216574194e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.35554152727127075\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.07848497480154037\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.2724570035934448\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.01844649389386177\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.2714332044124603\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.8933468126647313e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3742887079715729\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07441986352205276\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11106093227863312\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.012797311879694462\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.11094235628843307\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.6023020688038514e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3674623966217041\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07293173670768738\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.1242373064160347\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.00969089474529028\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.12314075976610184\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.349009015605418e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3871391713619232\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.07490164786577225\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.17545367777347565\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.018435994163155556\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.183843195438385\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.1300550495363382e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3414560556411743\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.06934432685375214\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.17191022634506226\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.014502150937914848\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.18368668854236603\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.1221125140181698e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3830001950263977\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07477850466966629\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.943185806274414\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3611375689506531\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.05559375509619713\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.05913291126489639\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.44522154331207275\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.0517912320792675\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.4562065601348877\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.14214180409908295\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.1200058460235596\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.1174989640712738\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.21154311299324036\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2742331326007843\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.07417222112417221\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.006427810061722994\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.07876542955636978\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.5670638120468539e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.3246748447418213\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.0582999587059021\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.098880834877491\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.011371361091732979\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.10485070198774338\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.876793661765191e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.33207783102989197\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06589069962501526\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.09425351768732071\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.008135318756103516\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.0809570774435997\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.0044344245230263e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3660527169704437\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.065848708152771\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09307784587144852\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.008203787729144096\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.09207361936569214\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 9.463856365243828e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.33276498317718506\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06809952855110168\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08325953036546707\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.00697026401758194\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.07875019311904907\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.3098830908830905e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3265169560909271\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.056921638548374176\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08292314410209656\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.007961108349263668\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.08419682830572128\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 9.219443541930161e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.30985739827156067\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.061891525983810425\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08722639083862305\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.008367535658180714\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08760234713554382\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.212354661106474e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3235417604446411\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.062243957072496414\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07423845678567886\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.006027841009199619\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07147213816642761\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.1189965620772568e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.29038265347480774\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.05697982385754585\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.7986812591552734\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.31490615010261536\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05762592703104019\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.060622844845056534\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.4218136668205261\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.042236972600221634\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.42011523246765137\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.13618159294128418\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.3581595420837402\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2472548633813858\n",
      "ab_loss: 23.64088249206543\n",
      "Gradient of AB Loss for row 0: tensor([0.0401, 0.0436, 0.0254, 0.0282, 0.0292, 0.0418, 0.0318, 0.0522, 0.0412,\n",
      "        0.0375, 0.0474, 0.0332, 0.0296, 0.0374])\n",
      "Gradient of AB Loss for row 1: tensor([0.0369, 0.0425, 0.0137, 0.0266, 0.0396, 0.0306, 0.0276, 0.0508, 0.0435,\n",
      "        0.0557, 0.0390, 0.0240, 0.0306, 0.0278])\n",
      "Gradient of AB Loss for row 2: tensor([ 0.0203, -0.0269,  0.0329, -0.0274,  0.0281,  0.0283,  0.0242, -0.0220,\n",
      "         0.0321,  0.0205,  0.0181,  0.0292,  0.0166,  0.0151,  0.0251,  0.0315,\n",
      "         0.0279,  0.0282, -0.0163,  0.0124,  0.0231])\n",
      "Gradient of AB Loss for row 3: tensor([0.0251, 0.0161, 0.0208, 0.0290, 0.0298, 0.0189, 0.0275, 0.0287, 0.0124,\n",
      "        0.0320, 0.0152, 0.0250, 0.0335, 0.0195, 0.0189, 0.0141, 0.0230, 0.0266,\n",
      "        0.0296, 0.0205, 0.0156, 0.0132])\n",
      "Gradient of AB Loss for row 4: tensor([ 0.0338, -0.0418,  0.0466,  0.0292,  0.0190,  0.0412,  0.0388,  0.0345,\n",
      "         0.0437,  0.0558,  0.0226,  0.0382,  0.0174,  0.0348])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0286,  0.0351,  0.0158,  0.0352,  0.0530,  0.0406,  0.0329,  0.0449,\n",
      "         0.0391,  0.0499,  0.0308,  0.0364, -0.0368,  0.0290])\n",
      "Gradient of AB Loss for row 6: tensor([0.1487, 0.1186, 0.1327, 0.1777])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0276,  0.0335, -0.0430, -0.0436, -0.0350, -0.0339, -0.0142,  0.0433,\n",
      "         0.0371,  0.0321,  0.0307,  0.0380, -0.0440,  0.0339, -0.0274])\n",
      "Gradient of AB Loss for row 8: tensor([0.1550, 0.1280, 0.1300, 0.1686])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0382, -0.0406, -0.0328,  0.0456,  0.0225,  0.0461,  0.0382,  0.0306,\n",
      "         0.0487,  0.0254,  0.0363,  0.0289, -0.0264,  0.0258])\n",
      "Gradient of AB Loss for row 10: tensor([0.0321, 0.0390, 0.0219, 0.0365, 0.0419, 0.0277, 0.0344, 0.0398, 0.0401,\n",
      "        0.0300, 0.0263, 0.0239, 0.0313, 0.0354])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0276, -0.0445, -0.0412, -0.0458,  0.0518,  0.0499, -0.0466,  0.0358,\n",
      "         0.0375,  0.0500,  0.0371, -0.0391, -0.0344, -0.0338])\n",
      "Gradient of AB Loss for row 12: tensor([ 0.0278,  0.0560,  0.0195,  0.0329,  0.0353,  0.0372,  0.0412,  0.0318,\n",
      "         0.0309, -0.0410,  0.0404,  0.0235,  0.0327])\n",
      "Gradient of AB Loss for row 13: tensor([-0.1046, -0.1220, -0.1185, -0.1641])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0384, -0.0647, -0.0315,  0.0489, -0.0453,  0.0324,  0.0456,  0.0450,\n",
      "         0.0531,  0.0469,  0.0396, -0.0427,  0.0353])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0315, -0.0353,  0.0447,  0.0193,  0.0320, -0.0469,  0.0274,  0.0380,\n",
      "         0.0270,  0.0474,  0.0272, -0.0365, -0.0331,  0.0227,  0.0220])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0417, -0.0330, -0.0180, -0.0360,  0.0124, -0.0222, -0.0250,  0.0418,\n",
      "         0.0284,  0.0402,  0.0343,  0.0424,  0.0345, -0.0282,  0.0306])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0299, -0.0375, -0.0466, -0.0297, -0.0342,  0.0362,  0.0361, -0.0369,\n",
      "         0.0288,  0.0456,  0.0436,  0.0288,  0.0334,  0.0236, -0.0297,  0.0158])\n",
      "Gradient of AB Loss for row 18: tensor([0.1629, 0.1214, 0.0881, 0.1018])\n",
      "Gradient of AB Loss for row 19: tensor([ 0.0300, -0.0379,  0.0406,  0.0201,  0.0306,  0.0436,  0.0398,  0.0344,\n",
      "         0.0417,  0.0460,  0.0338,  0.0246,  0.0336,  0.0212])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0214, -0.0223, -0.0176, -0.0158, -0.0262, -0.0226,  0.0256,  0.0177,\n",
      "        -0.0237, -0.0180, -0.0246,  0.0071, -0.0300,  0.0153, -0.0244,  0.0219,\n",
      "        -0.0194, -0.0279, -0.0210,  0.0236, -0.0310, -0.0344])\n",
      "Gradient of AB Loss for row 21: tensor([0.0317, 0.0271, 0.0168, 0.0340, 0.0465, 0.0361, 0.0524, 0.0353, 0.0477,\n",
      "        0.0387, 0.0314, 0.0297, 0.0252, 0.0303])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0512, -0.0380,  0.0392,  0.0314, -0.0511,  0.0401,  0.0460,  0.0341,\n",
      "         0.0522,  0.0470,  0.0414,  0.0304, -0.0294,  0.0304])\n",
      "Gradient of AB Loss for row 23: tensor([ 0.0253, -0.0266,  0.0145,  0.0295, -0.0164,  0.0259,  0.0301,  0.0226,\n",
      "        -0.0235,  0.0233,  0.0096,  0.0306,  0.0263, -0.0131,  0.0174,  0.0276,\n",
      "         0.0288,  0.0268,  0.0170, -0.0317,  0.0210,  0.0189])\n",
      "Gradient of AB Loss for row 24: tensor([0.0311, 0.0416, 0.0103, 0.0357, 0.0343, 0.0427, 0.0377, 0.0394, 0.0447,\n",
      "        0.0388, 0.0368, 0.0229, 0.0309, 0.0344])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0396, -0.0167, -0.0400, -0.0559, -0.0369,  0.0334, -0.0442,  0.0373,\n",
      "         0.0418,  0.0375, -0.0410, -0.0364, -0.0284, -0.0212])\n",
      "Gradient of AB Loss for row 26: tensor([0.0344, 0.0379, 0.0361, 0.0286, 0.0429, 0.0432, 0.0402, 0.0354, 0.0397,\n",
      "        0.0302, 0.0425, 0.0362, 0.0301, 0.0235])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0470, -0.0490,  0.0440,  0.0353, -0.0303,  0.0417,  0.0351,  0.0454,\n",
      "         0.0467,  0.0414,  0.0348,  0.0359, -0.0283,  0.0251])\n",
      "Gradient of AB Loss for row 28: tensor([0.0354, 0.0540, 0.0193, 0.0277, 0.0460, 0.0313, 0.0285, 0.0373, 0.0396,\n",
      "        0.0507, 0.0428, 0.0263, 0.0279, 0.0317])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0323, -0.0402,  0.0371,  0.0438,  0.0309,  0.0376,  0.0467,  0.0295,\n",
      "         0.0543,  0.0396,  0.0364,  0.0297,  0.0281,  0.0275])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0360, -0.0179, -0.0399, -0.0285,  0.0227,  0.0259, -0.0244,  0.0380,\n",
      "         0.0421,  0.0279, -0.0209,  0.0331, -0.0361,  0.0212,  0.0400])\n",
      "Gradient of AB Loss for row 31: tensor([0.0363, 0.0330, 0.0195, 0.0371, 0.0352, 0.0305, 0.0249, 0.0462, 0.0473,\n",
      "        0.0439, 0.0402, 0.0272, 0.0256, 0.0232])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09518624097108841\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09836961328983307\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1365354061126709\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.921446681022644\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10484011471271515\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.17892295122146606\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2636330723762512\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1293139010667801\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.011904344893991947\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.14777326583862305\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.7956330866653047e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.2689589262008667\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.06987064331769943\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.10853338241577148\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.011055628769099712\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.12544488906860352\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 7.700561299728292e-10\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2612260580062866\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.06782691925764084\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.11982939392328262\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.0149407759308815\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.12726882100105286\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 2.1045343245873482e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.27642542123794556\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.06943000108003616\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.10402325540781021\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.012490731664001942\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.10302523523569107\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 9.439232728780667e-10\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3161143660545349\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07383634150028229\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.12136506289243698\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.010963240638375282\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.1129818856716156\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 9.029031966534262e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3064914643764496\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.06927025318145752\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.11036042869091034\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.010299723595380783\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.10861687362194061\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.1842893332669746e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.29688844084739685\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.06291639804840088\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.09772855788469315\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.009069743566215038\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.10370219498872757\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.0956381357729583e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.31011849641799927\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07151796668767929\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.15832388401031494\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.011017251759767532\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.16356484591960907\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.167705043769729e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3254653215408325\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07482409477233887\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.443451404571533\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3431929647922516\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04482106491923332\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.05603017285466194\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3508865237236023\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04055127874016762\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3776836097240448\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.12929843366146088\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9439798593521118\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.10378377884626389\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.17972609400749207\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2474139779806137\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06360599398612976\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.005451831500977278\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.06664799898862839\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.319464648652513e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.27117428183555603\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05267355218529701\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.07344356924295425\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006648876704275608\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.08003416657447815\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.951616587398291e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.28105390071868896\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.05933022499084473\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.0765446349978447\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.0075887334533035755\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07445906102657318\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 6.641080463332116e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3402700126171112\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06414288282394409\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.08228643238544464\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.009209067560732365\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.08069737255573273\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.4675702875166508e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.30608272552490234\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06342682242393494\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.04784190654754639\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.003928382880985737\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.047230251133441925\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 8.652095706551677e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2794240117073059\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.05082345008850098\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.0749136209487915\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.007806742563843727\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.07894396781921387\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 5.494774080183618e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2764255404472351\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06013990566134453\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.07451698184013367\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.006426404695957899\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07126225531101227\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.027440851459232e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.2812312841415405\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06036875769495964\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.05945928394794464\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.004809378646314144\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.0586930550634861\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.4617500543323558e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.27291855216026306\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.05748782306909561\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.496074676513672\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.29967793822288513\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05510786920785904\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05620425567030907\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3606468141078949\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03895992040634155\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.35571157932281494\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.12494247406721115\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.9356071949005127\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.22454789280891418\n",
      "ab_loss: 24.568126678466797\n",
      "Gradient of AB Loss for row 0: tensor([-0.6119])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0282, -0.0539, -0.0565,  0.0501,  0.0219, -0.0394,  0.0396,  0.0366,\n",
      "         0.0471,  0.0477,  0.0418,  0.0297, -0.0275,  0.0328])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0171, -0.0378,  0.0405,  0.0314,  0.0178, -0.0267,  0.0354, -0.0490,\n",
      "         0.0451,  0.0398,  0.0449,  0.0473,  0.0306,  0.0239, -0.0310])\n",
      "Gradient of AB Loss for row 3: tensor([0.0256, 0.0490, 0.0207, 0.0262, 0.0233, 0.0268, 0.0250, 0.0358, 0.0458,\n",
      "        0.0449, 0.0401, 0.0226, 0.0441, 0.0336])\n",
      "Gradient of AB Loss for row 4: tensor([0.0356, 0.0531, 0.0336, 0.0330, 0.0439, 0.0344, 0.0325, 0.0359, 0.0338,\n",
      "        0.0315, 0.0307, 0.0321, 0.0296, 0.0271])\n",
      "Gradient of AB Loss for row 5: tensor([0.0280, 0.0419, 0.0142, 0.0404, 0.0279, 0.0355, 0.0320, 0.0383, 0.0434,\n",
      "        0.0473, 0.0423, 0.0245, 0.0237, 0.0457])\n",
      "Gradient of AB Loss for row 6: tensor([0.0148, 0.0417, 0.0250, 0.0390, 0.0176, 0.0380, 0.0361, 0.0327, 0.0346,\n",
      "        0.0404, 0.0370, 0.0337, 0.0270, 0.0457])\n",
      "Gradient of AB Loss for row 7: tensor([0.0289, 0.0452, 0.0190, 0.0376, 0.0481, 0.0401, 0.0251, 0.0332, 0.0414,\n",
      "        0.0568, 0.0452, 0.0245, 0.0346, 0.0440])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0446,  0.0422,  0.0349,  0.0252,  0.0336,  0.0442,  0.0443,  0.0402,\n",
      "         0.0492,  0.0348,  0.0368,  0.0293, -0.0308,  0.0383])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0253,  0.0330,  0.0381,  0.0345,  0.0310,  0.0120,  0.0365,  0.0324,\n",
      "         0.0388,  0.0352,  0.0362,  0.0357,  0.0379, -0.0334,  0.0320,  0.0202,\n",
      "         0.0208])\n",
      "Gradient of AB Loss for row 10: tensor([0.0229, 0.0374, 0.0167, 0.0298, 0.0194, 0.0451, 0.0401, 0.0395, 0.0404,\n",
      "        0.0452, 0.0379, 0.0221, 0.0302, 0.0314])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0385, -0.0305,  0.0257,  0.0196,  0.0233, -0.0289,  0.0320, -0.0481,\n",
      "         0.0381,  0.0350,  0.0405,  0.0524,  0.0272,  0.0274, -0.0214])\n",
      "Gradient of AB Loss for row 12: tensor([-0.1302, -0.1263, -0.1015, -0.1294])\n",
      "Gradient of AB Loss for row 13: tensor([0.0382, 0.0392, 0.0404, 0.0260, 0.0359, 0.0358, 0.0433, 0.0349, 0.0510,\n",
      "        0.0483, 0.0376, 0.0351, 0.0383, 0.0354])\n",
      "Gradient of AB Loss for row 14: tensor([-0.5733])\n",
      "Gradient of AB Loss for row 15: tensor([-0.6682])\n",
      "Gradient of AB Loss for row 16: tensor([0.1188, 0.1314, 0.1054, 0.1361])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0570,  0.0517,  0.0515, -0.0524,  0.0319,  0.0383,  0.0320,  0.0499,\n",
      "         0.0352,  0.0327, -0.0453,  0.0278, -0.0252,  0.0427])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0385, -0.0243, -0.0400, -0.0471, -0.0347,  0.0436, -0.0278,  0.0267,\n",
      "         0.0311,  0.0434,  0.0357, -0.0450, -0.0414, -0.0388, -0.0152])\n",
      "Gradient of AB Loss for row 19: tensor([0.0293, 0.0504, 0.0340, 0.0439, 0.0323, 0.0283, 0.0423, 0.0396, 0.0562,\n",
      "        0.0392, 0.0285, 0.0296, 0.0377])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0562,  0.0476,  0.0294, -0.0392,  0.0456,  0.0430,  0.0468,  0.0306,\n",
      "         0.0449,  0.0505,  0.0429,  0.0372, -0.0373,  0.0329])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0312, -0.0270, -0.0186, -0.0223, -0.0375, -0.0310, -0.0134,  0.0161,\n",
      "        -0.0186, -0.0265, -0.0294, -0.0289, -0.0281, -0.0237, -0.0196, -0.0150,\n",
      "        -0.0267,  0.0257, -0.0299, -0.0168])\n",
      "Gradient of AB Loss for row 22: tensor([-0.4061])\n",
      "Gradient of AB Loss for row 23: tensor([-0.1842, -0.1360,  0.1525])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0233, -0.0273, -0.0336,  0.0371,  0.0250,  0.0460, -0.0385,  0.0382,\n",
      "         0.0417,  0.0432,  0.0286,  0.0317, -0.0380,  0.0269,  0.0206])\n",
      "Gradient of AB Loss for row 25: tensor([0.0394, 0.0563, 0.0302, 0.0285, 0.0474, 0.0337, 0.0316, 0.0305, 0.0574,\n",
      "        0.0360, 0.0465, 0.0267, 0.0275, 0.0275])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0300, -0.0443, -0.0321, -0.0388, -0.0269,  0.0401,  0.0401,  0.0138,\n",
      "        -0.0242,  0.0236,  0.0424,  0.0399,  0.0389,  0.0393, -0.0306, -0.0238])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0204, -0.0127, -0.0208,  0.0309, -0.0202,  0.0182, -0.0247, -0.0220,\n",
      "        -0.0266,  0.0245,  0.0123,  0.0250, -0.0164,  0.0121,  0.0243, -0.0211,\n",
      "        -0.0107, -0.0149,  0.0305,  0.0191,  0.0193, -0.0268,  0.0206, -0.0238])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0301, -0.0390, -0.0478, -0.0202, -0.0326,  0.0461,  0.0251, -0.0277,\n",
      "         0.0409,  0.0366,  0.0485,  0.0234,  0.0344, -0.0342, -0.0424, -0.0216,\n",
      "         0.0181])\n",
      "Gradient of AB Loss for row 29: tensor([0.0342, 0.0584, 0.0310, 0.0239, 0.0549, 0.0224, 0.0270, 0.0366, 0.0508,\n",
      "        0.0365, 0.0419, 0.0305, 0.0437])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0333, -0.0482, -0.0532, -0.0292, -0.0456,  0.0408, -0.0279,  0.0442,\n",
      "         0.0357,  0.0581, -0.0252,  0.0499,  0.0362, -0.0242])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0209, -0.0333, -0.0272, -0.0397,  0.0391,  0.0323,  0.0310,  0.0097,\n",
      "        -0.0312,  0.0215,  0.0432,  0.0337,  0.0441,  0.0250,  0.0283,  0.0180,\n",
      "         0.0257])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08743809908628464\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.08838041126728058\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.17594876885414124\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9452764391899109\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.1075439527630806\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.15399636328220367\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2769213318824768\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1381976455450058\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.013739027082920074\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.15004929900169373\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.295813012447411e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.29153621196746826\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07920363545417786\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.13589008152484894\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.013934416696429253\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.15797534584999084\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.5389696184087143e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.31122174859046936\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08707992732524872\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.13406948745250702\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.014557532966136932\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.1431758552789688\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.0473421019341345e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.326930433511734\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09188677370548248\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.12179037183523178\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.010562882758677006\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.1267489343881607\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.6094859889292934e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3169631063938141\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.08352304995059967\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.13389204442501068\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.017958547919988632\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.12962982058525085\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.4254640801070195e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3470394015312195\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08579441159963608\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.1247825026512146\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.013906792737543583\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.12356376647949219\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.6208087094682355e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.31808561086654663\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08354814350605011\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.13595478236675262\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01330140233039856\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.13321946561336517\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.1454868165117205e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.34164518117904663\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.09767070412635803\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.18151934444904327\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.012485506944358349\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.18387781083583832\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.1537624189372764e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.32970255613327026\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.0867478996515274\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.6479592323303223\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4293462038040161\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.052058860659599304\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06117132306098938\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.33384788036346436\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04360387846827507\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3503197729587555\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1375150829553604\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9343204498291016\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09547184407711029\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1732115000486374\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2144995778799057\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.07656186819076538\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.006766820326447487\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.07830993086099625\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 9.134492051643406e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.28169625997543335\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.062147848308086395\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09567338973283768\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.010099219158291817\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09195933490991592\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.167885343988928e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.2941531836986542\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06611219793558121\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.0867968499660492\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.009389402344822884\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08381088823080063\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.904179258715999e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3556438386440277\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.0798707976937294\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.06558812409639359\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.005647051613777876\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.06784737855195999\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 7.282654479467965e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3043677508831024\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07010842859745026\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08045785874128342\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.006878035143017769\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.07782775908708572\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.312228659067216e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.28528645634651184\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.060548171401023865\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.07420146465301514\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.006736262235790491\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.08284401148557663\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 8.227789005665898e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3177814781665802\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07541412115097046\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08642515540122986\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.00866703037172556\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08438730239868164\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.364452936364785e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3167177140712738\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07475412636995316\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.09006790071725845\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.009850330650806427\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.08135420083999634\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.1163742152930922e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2928027808666229\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06517158448696136\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.5638492107391357\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.33442869782447815\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04408397525548935\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05776365473866463\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.37081843614578247\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.04073552414774895\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3444404900074005\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11326978355646133\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.7182296514511108\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.18690240383148193\n",
      "ab_loss: 24.160120010375977\n",
      "Gradient of AB Loss for row 0: tensor([0.1119, 0.1690, 0.1271, 0.1484])\n",
      "Gradient of AB Loss for row 1: tensor([0.0461, 0.0436, 0.0270, 0.0484, 0.0534, 0.0486, 0.0458, 0.0519, 0.0252,\n",
      "        0.0384, 0.0371, 0.0304, 0.0325])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0253, -0.0412, -0.0276, -0.0326,  0.0377,  0.0238,  0.0219, -0.0362,\n",
      "         0.0248,  0.0437,  0.0382,  0.0348,  0.0269, -0.0393, -0.0367, -0.0344,\n",
      "         0.0232])\n",
      "Gradient of AB Loss for row 3: tensor([0.0280, 0.0489, 0.0107, 0.0368, 0.0251, 0.0311, 0.0308, 0.0340, 0.0488,\n",
      "        0.0382, 0.0452, 0.0377, 0.0226, 0.0380])\n",
      "Gradient of AB Loss for row 4: tensor([0.0356, 0.0501, 0.0300, 0.0284, 0.0447, 0.0374, 0.0377, 0.0447, 0.0485,\n",
      "        0.0361, 0.0353, 0.0229, 0.0254, 0.0311])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0220, -0.0292, -0.0216,  0.0218,  0.0158,  0.0159, -0.0363, -0.0267,\n",
      "         0.0218,  0.0190,  0.0324,  0.0255,  0.0180,  0.0235,  0.0336, -0.0305,\n",
      "         0.0226])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0262, -0.0262, -0.0222, -0.0301, -0.0334,  0.0348,  0.0277,  0.0133,\n",
      "        -0.0360,  0.0262,  0.0408,  0.0325,  0.0338,  0.0287, -0.0285, -0.0303,\n",
      "        -0.0252])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0251, -0.0287, -0.0179, -0.0238,  0.0337, -0.0252, -0.0202, -0.0284,\n",
      "         0.0133,  0.0301,  0.0297, -0.0208,  0.0156,  0.0293, -0.0167, -0.0245,\n",
      "        -0.0084,  0.0323,  0.0188, -0.0220, -0.0339])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0378,  0.0452, -0.0343,  0.0309,  0.0323,  0.0552,  0.0513,  0.0416,\n",
      "         0.0595,  0.0385,  0.0452,  0.0305,  0.0402])\n",
      "Gradient of AB Loss for row 9: tensor([0.0296, 0.0562, 0.0268, 0.0362, 0.0356, 0.0440, 0.0223, 0.0492, 0.0452,\n",
      "        0.0552, 0.0380, 0.0232, 0.0207, 0.0298])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0326,  0.0472, -0.0342,  0.0245,  0.0411,  0.0372,  0.0493,  0.0445,\n",
      "         0.0408,  0.0310,  0.0391,  0.0322,  0.0237])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0305, -0.0364, -0.0447,  0.0332,  0.0216, -0.0188,  0.0318,  0.0517,\n",
      "         0.0345,  0.0434,  0.0349,  0.0283,  0.0262, -0.0229,  0.0143, -0.0163])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0298, -0.0174, -0.0156, -0.0309, -0.0235,  0.0237, -0.0140, -0.0290,\n",
      "        -0.0202, -0.0292,  0.0145,  0.0250,  0.0237, -0.0200,  0.0088, -0.0108,\n",
      "        -0.0093, -0.0126, -0.0202,  0.0254, -0.0266, -0.0278])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0304, -0.0564,  0.0572, -0.0374,  0.0404,  0.0450,  0.0368,  0.0466,\n",
      "         0.0406,  0.0472, -0.0272, -0.0265,  0.0254])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0139, -0.0236, -0.0210, -0.0208, -0.0240, -0.0306, -0.0272, -0.0194,\n",
      "        -0.0175,  0.0256, -0.0193, -0.0185, -0.0227, -0.0239, -0.0201, -0.0219,\n",
      "        -0.0250, -0.0167, -0.0212, -0.0189,  0.0224, -0.0290, -0.0336])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0104, -0.0189, -0.0236, -0.0100, -0.0242, -0.0254, -0.0214, -0.0181,\n",
      "         0.0196, -0.0218, -0.0272, -0.0248, -0.0233,  0.0072, -0.0277, -0.0168,\n",
      "        -0.0174, -0.0156, -0.0137, -0.0159,  0.0155, -0.0296, -0.0137])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0213, -0.0344, -0.0256, -0.0364,  0.0375,  0.0329,  0.0346,  0.0216,\n",
      "        -0.0255,  0.0245,  0.0384,  0.0358,  0.0326,  0.0336,  0.0331, -0.0336,\n",
      "        -0.0361,  0.0144])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0325, -0.0430,  0.0427,  0.0203,  0.0288, -0.0459,  0.0335,  0.0504,\n",
      "         0.0414,  0.0450,  0.0259, -0.0335, -0.0452,  0.0318])\n",
      "Gradient of AB Loss for row 18: tensor([-0.4600])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0493, -0.0432,  0.0127,  0.0463, -0.0620,  0.0336,  0.0400,  0.0531,\n",
      "         0.0410,  0.0253, -0.0366, -0.0326,  0.0423])\n",
      "Gradient of AB Loss for row 20: tensor([0.0270, 0.0559, 0.0308, 0.0357, 0.0525, 0.0376, 0.0340, 0.0382, 0.0576,\n",
      "        0.0460, 0.0319, 0.0302, 0.0334, 0.0280])\n",
      "Gradient of AB Loss for row 21: tensor([0.0293, 0.0522, 0.0163, 0.0324, 0.0536, 0.0243, 0.0363, 0.0216, 0.0448,\n",
      "        0.0545, 0.0417, 0.0256, 0.0339, 0.0301])\n",
      "Gradient of AB Loss for row 22: tensor([-0.5532])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0234,  0.0287,  0.0197,  0.0528, -0.0314,  0.0310,  0.0307,  0.0417,\n",
      "         0.0519,  0.0381, -0.0471,  0.0288, -0.0472,  0.0361])\n",
      "Gradient of AB Loss for row 24: tensor([0.0300, 0.0354, 0.0169, 0.0301, 0.0400, 0.0345, 0.0420, 0.0431, 0.0408,\n",
      "        0.0399, 0.0360, 0.0231, 0.0254, 0.0250])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0278, -0.0253, -0.0350, -0.0315, -0.0265,  0.0284, -0.0279, -0.0202,\n",
      "        -0.0325,  0.0318,  0.0143,  0.0395,  0.0268, -0.0130,  0.0306, -0.0193,\n",
      "         0.0259,  0.0229, -0.0187,  0.0211])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0429, -0.0339, -0.0443, -0.0556, -0.0336,  0.0433, -0.0292,  0.0332,\n",
      "         0.0400,  0.0347,  0.0383, -0.0288, -0.0437, -0.0316])\n",
      "Gradient of AB Loss for row 27: tensor([0.0310, 0.0465, 0.0277, 0.0329, 0.0298, 0.0285, 0.0548, 0.0342, 0.0505,\n",
      "        0.0543, 0.0452, 0.0370, 0.0244, 0.0431])\n",
      "Gradient of AB Loss for row 28: tensor([-0.7045])\n",
      "Gradient of AB Loss for row 29: tensor([0.0228, 0.0499, 0.0207, 0.0326, 0.0360, 0.0305, 0.0495, 0.0490, 0.0400,\n",
      "        0.0351, 0.0447, 0.0388, 0.0242, 0.0328])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0424,  0.0364,  0.0125,  0.0357, -0.0417,  0.0373,  0.0414,  0.0334,\n",
      "         0.0246,  0.0461,  0.0299,  0.0444, -0.0288,  0.0401])\n",
      "Gradient of AB Loss for row 31: tensor([0.0339, 0.0397, 0.0391, 0.0203, 0.0391, 0.0303, 0.0383, 0.0356, 0.0279,\n",
      "        0.0508, 0.0316, 0.0274, 0.0251, 0.0254])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09672962874174118\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09367132186889648\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1744615137577057\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9481585621833801\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10706336051225662\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.1777135133743286\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.26066091656684875\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.17746345698833466\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.015166614204645157\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.20178091526031494\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 9.890738228435225e-10\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.306169331073761\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.0801997035741806\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.14655236899852753\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.012173415161669254\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.1698678880929947\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.3550413013874163e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3006729781627655\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08189982920885086\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.16675756871700287\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.020874246954917908\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.17485260963439941\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 9.599588901565426e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3178754150867462\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.08081014454364777\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.15319553017616272\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.016535697504878044\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.15809163451194763\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.3880404603483498e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.34098854660987854\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.09104087203741074\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.1300692856311798\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.013082500547170639\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.12502357363700867\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.020369899684681e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3606928288936615\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08922574669122696\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.14361515641212463\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.018855109810829163\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.14552143216133118\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.1991764248264758e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.361320823431015\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09115669131278992\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.12153495103120804\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.012916242703795433\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.13164283335208893\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 9.934419953339102e-10\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.31888630986213684\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07799996435642242\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.14606279134750366\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.011193852871656418\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.16005732119083405\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.0722414067743102e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3220864534378052\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07592037320137024\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.6666338443756104\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4066438674926758\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04184138402342796\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06153324246406555\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3701045513153076\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04300197586417198\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3546348214149475\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.12485683709383011\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9428666234016418\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09545575082302094\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.18309630453586578\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.22617153823375702\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08318951725959778\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.008452067151665688\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.09015082567930222\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 8.834510900612713e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2927235960960388\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05862731859087944\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.07670223712921143\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.00738536799326539\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.07991856336593628\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.883897978899768e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.27999070286750793\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.05770770460367203\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.0720139890909195\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.007260138634592295\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06719360500574112\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.1602362404161681e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3226751685142517\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.0659637302160263\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.07819928228855133\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007226424757391214\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.07678938657045364\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.681945582809874e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.2817714512348175\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.0580153614282608\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.06934040039777756\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.00606932258233428\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.06318726390600204\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.2377014968478761e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.27484333515167236\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.05566065385937691\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08819806575775146\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.008111070841550827\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09754034131765366\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.0192365840211437e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.28037911653518677\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06021730601787567\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.07820594310760498\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.006237192545086145\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08015070110559464\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.758740316887724e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3074396848678589\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06969686597585678\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.0667695701122284\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.005333905573934317\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.06889224052429199\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.1316741987954515e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2854355573654175\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06034153699874878\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.4975321292877197\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3047063648700714\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04682064428925514\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05463153496384621\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.35701805353164673\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03761419281363487\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.33522263169288635\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11141777038574219\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.962094783782959\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.21934586763381958\n",
      "ab_loss: 24.067977905273438\n",
      "Gradient of AB Loss for row 0: tensor([0.0312, 0.0437, 0.0362, 0.0257, 0.0403, 0.0387, 0.0462, 0.0362, 0.0486,\n",
      "        0.0412, 0.0434, 0.0229, 0.0368, 0.0281])\n",
      "Gradient of AB Loss for row 1: tensor([-0.4590])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0305,  0.0450, -0.0353,  0.0496,  0.0256,  0.0294,  0.0369,  0.0458,\n",
      "         0.0445,  0.0317,  0.0402,  0.0416,  0.0274,  0.0309])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0264, -0.0229, -0.0204, -0.0164, -0.0301, -0.0225, -0.0244,  0.0275,\n",
      "        -0.0295, -0.0213, -0.0301,  0.0235,  0.0229,  0.0249, -0.0163,  0.0312,\n",
      "        -0.0163, -0.0154, -0.0112, -0.0132,  0.0324, -0.0262, -0.0291])\n",
      "Gradient of AB Loss for row 4: tensor([0.0379, 0.0442, 0.0195, 0.0264, 0.0364, 0.0290, 0.0282, 0.0425, 0.0548,\n",
      "        0.0417, 0.0492, 0.0322, 0.0217, 0.0410])\n",
      "Gradient of AB Loss for row 5: tensor([0.0242, 0.0557, 0.0315, 0.0330, 0.0407, 0.0355, 0.0329, 0.0319, 0.0546,\n",
      "        0.0426, 0.0396, 0.0255, 0.0318, 0.0152])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0423,  0.0506,  0.0401, -0.0456,  0.0438,  0.0447,  0.0354,  0.0304,\n",
      "         0.0494,  0.0529,  0.0350,  0.0368, -0.0334,  0.0192])\n",
      "Gradient of AB Loss for row 7: tensor([0.0237, 0.0362, 0.0218, 0.0232, 0.0324, 0.0366, 0.0457, 0.0247, 0.0465,\n",
      "        0.0352, 0.0242, 0.0335, 0.0163, 0.0298])\n",
      "Gradient of AB Loss for row 8: tensor([0.0386, 0.0279, 0.0137, 0.0387, 0.0453, 0.0380, 0.0467, 0.0433, 0.0478,\n",
      "        0.0339, 0.0376, 0.0288, 0.0215, 0.0412])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0218, -0.0226,  0.0182, -0.0178, -0.0380, -0.0305, -0.0179, -0.0236,\n",
      "         0.0213, -0.0222, -0.0258, -0.0197, -0.0270, -0.0251, -0.0159, -0.0158,\n",
      "        -0.0110,  0.0318,  0.0239, -0.0251,  0.0165])\n",
      "Gradient of AB Loss for row 10: tensor([-0.5406])\n",
      "Gradient of AB Loss for row 11: tensor([ 0.0381, -0.0375,  0.0453,  0.0155,  0.0312,  0.0420,  0.0358,  0.0318,\n",
      "         0.0391,  0.0590,  0.0383,  0.0413,  0.0330,  0.0351])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0191, -0.0341, -0.0388, -0.0464, -0.0365,  0.0276,  0.0301, -0.0251,\n",
      "         0.0348,  0.0350,  0.0366,  0.0240,  0.0375,  0.0350, -0.0273, -0.0271,\n",
      "        -0.0333])\n",
      "Gradient of AB Loss for row 13: tensor([0.0398, 0.0504, 0.0224, 0.0316, 0.0363, 0.0325, 0.0408, 0.0416, 0.0582,\n",
      "        0.0271, 0.0459, 0.0337, 0.0332, 0.0336])\n",
      "Gradient of AB Loss for row 14: tensor([0.0392, 0.0306, 0.0237, 0.0251, 0.0391, 0.0300, 0.0374, 0.0389, 0.0444,\n",
      "        0.0270, 0.0309, 0.0359, 0.0312, 0.0378])\n",
      "Gradient of AB Loss for row 15: tensor([ 0.0386, -0.0446,  0.0560,  0.0244,  0.0264,  0.0525,  0.0249,  0.0259,\n",
      "         0.0435,  0.0527,  0.0409,  0.0376,  0.0386,  0.0310])\n",
      "Gradient of AB Loss for row 16: tensor([0.0430, 0.0525, 0.0322, 0.0171, 0.0421, 0.0466, 0.0378, 0.0462, 0.0602,\n",
      "        0.0250, 0.0369, 0.0369, 0.0348, 0.0282])\n",
      "Gradient of AB Loss for row 17: tensor([0.0308, 0.0417, 0.0332, 0.0349, 0.0417, 0.0347, 0.0391, 0.0304, 0.0437,\n",
      "        0.0211, 0.0345, 0.0336, 0.0164, 0.0326])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0335, -0.0351,  0.0462,  0.0274,  0.0262,  0.0473,  0.0427,  0.0361,\n",
      "         0.0535,  0.0255,  0.0313,  0.0311, -0.0268,  0.0164])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0326, -0.0327, -0.0279, -0.0392,  0.0428,  0.0285,  0.0172, -0.0279,\n",
      "         0.0289,  0.0349,  0.0299,  0.0419,  0.0381, -0.0296,  0.0407, -0.0268,\n",
      "         0.0403])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0259, -0.0262,  0.0171, -0.0193, -0.0164, -0.0223, -0.0187,  0.0183,\n",
      "        -0.0220, -0.0179, -0.0194,  0.0240,  0.0073,  0.0307,  0.0228, -0.0158,\n",
      "         0.0223,  0.0255, -0.0139, -0.0123, -0.0114,  0.0228, -0.0227, -0.0217])\n",
      "Gradient of AB Loss for row 21: tensor([ 0.0306, -0.0449,  0.0401,  0.0200,  0.0267,  0.0301,  0.0412,  0.0432,\n",
      "         0.0362,  0.0457,  0.0392,  0.0393,  0.0249,  0.0338])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0353, -0.0385, -0.0327,  0.0419,  0.0405,  0.0415,  0.0298, -0.0351,\n",
      "         0.0315,  0.0331,  0.0364,  0.0401,  0.0338,  0.0208,  0.0288,  0.0190,\n",
      "         0.0204])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0402,  0.0359, -0.0401,  0.0431,  0.0410,  0.0273,  0.0415,  0.0459,\n",
      "         0.0369,  0.0498,  0.0392,  0.0302,  0.0318,  0.0321])\n",
      "Gradient of AB Loss for row 24: tensor([ 0.0408, -0.0399,  0.0484,  0.0241,  0.0319,  0.0477,  0.0259,  0.0302,\n",
      "         0.0350,  0.0486,  0.0391,  0.0326,  0.0338,  0.0390])\n",
      "Gradient of AB Loss for row 25: tensor([0.0318, 0.0461, 0.0195, 0.0218, 0.0376, 0.0316, 0.0465, 0.0394, 0.0448,\n",
      "        0.0428, 0.0475, 0.0246, 0.0407, 0.0390])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0348,  0.0393,  0.0108,  0.0395, -0.0305,  0.0458,  0.0353,  0.0217,\n",
      "         0.0442,  0.0308,  0.0332,  0.0416, -0.0267,  0.0254])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0331, -0.0555, -0.0305,  0.0417,  0.0369,  0.0415,  0.0418,  0.0287,\n",
      "         0.0378,  0.0464,  0.0482,  0.0425,  0.0314, -0.0334,  0.0255])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0254, -0.0377, -0.0345, -0.0309, -0.0384, -0.0356,  0.0358, -0.0333,\n",
      "         0.0271,  0.0383,  0.0382,  0.0282,  0.0342, -0.0310, -0.0403, -0.0232,\n",
      "        -0.0153])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0278, -0.0205, -0.0280, -0.0302,  0.0261,  0.0410, -0.0273, -0.0208,\n",
      "        -0.0333,  0.0216,  0.0192,  0.0362,  0.0323, -0.0150,  0.0287, -0.0097,\n",
      "         0.0315,  0.0272, -0.0216, -0.0329])\n",
      "Gradient of AB Loss for row 30: tensor([0.0422, 0.0305, 0.0146, 0.0222, 0.0317, 0.0335, 0.0410, 0.0436, 0.0494,\n",
      "        0.0343, 0.0357, 0.0365, 0.0354, 0.0362])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0362, -0.0184, -0.0253, -0.0329,  0.0286,  0.0223, -0.0349,  0.0376,\n",
      "         0.0371,  0.0339,  0.0312, -0.0403, -0.0242,  0.0304, -0.0225, -0.0258,\n",
      "        -0.0164])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08423251658678055\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.08204536885023117\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.137867733836174\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.8523015379905701\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.09474082291126251\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.16502700746059418\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.24020501971244812\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1214810386300087\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.01117843110114336\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.13300275802612305\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 2.166440804529657e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.25558754801750183\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.05823822319507599\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.10077034682035446\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.011118345893919468\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.10951939970254898\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.0994831711741426e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2556062340736389\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.05914803966879845\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1656375378370285\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.015070362016558647\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.1887790709733963\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 2.1050750032003407e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.2902314364910126\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.06807504594326019\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.167714461684227\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.011096467263996601\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.1625935137271881\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.4912499013419733e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.2810195982456207\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.06121628358960152\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.09320299327373505\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.01127884816378355\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.09807037562131882\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 6.837396759884484e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.27398526668548584\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.05959579721093178\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.09478013217449188\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.009473510086536407\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.09509796649217606\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 9.545332302351994e-10\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.2771274447441101\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.06784655153751373\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.11123734712600708\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.011770673096179962\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.12536656856536865\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.6785405287933486e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.2614487409591675\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.058944687247276306\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.10519755631685257\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.01197989471256733\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.10814522206783295\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 9.539901091315528e-10\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.2896536588668823\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.06625049561262131\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.223273754119873\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.30375126004219055\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.038798242807388306\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.04832301288843155\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3455168604850769\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.0382261723279953\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.33484137058258057\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1163599044084549\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9272204041481018\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09963636100292206\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.17514698207378387\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.22809872031211853\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08108019083738327\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.008388727903366089\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.07937364280223846\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 8.333235768098746e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2819221019744873\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05724387615919113\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.06506303697824478\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006598477251827717\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.07026012986898422\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.0183286436316052e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.29252633452415466\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06093224138021469\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.059073373675346375\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005231672897934914\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.05807728320360184\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.2733790688557178e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3357090651988983\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06636527925729752\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.08595885336399078\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.009211594238877296\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.08433523774147034\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.0711050935086064e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.28403225541114807\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.05882958322763443\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.06172681972384453\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.0056250873021781445\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.057848405092954636\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.0965680585783844e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.28491443395614624\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.05673491209745407\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.07166528701782227\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.0064519899897277355\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.07701362669467926\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 7.893207754072762e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.28660500049591064\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.0628068670630455\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08351326733827591\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007351002190262079\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07652223110198975\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 8.575699594892683e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.27789783477783203\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.062090158462524414\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07579641789197922\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.008204263634979725\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.06989675760269165\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.1361734886250474e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2736062705516815\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.058536916971206665\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.4484291076660156\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3047797381877899\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05375907942652702\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05423973128199577\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3562120795249939\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03941817954182625\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3410707712173462\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11442062258720398\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.8300877809524536\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.20914901793003082\n",
      "ab_loss: 24.46988868713379\n",
      "Gradient of AB Loss for row 0: tensor([0.0707, 0.1446, 0.1486, 0.1412])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0486, -0.0449,  0.0525,  0.0286,  0.0339,  0.0534,  0.0414,  0.0461,\n",
      "         0.0557,  0.0404,  0.0341, -0.0489,  0.0495])\n",
      "Gradient of AB Loss for row 2: tensor([0.0388, 0.0488, 0.0195, 0.0344, 0.0292, 0.0331, 0.0329, 0.0328, 0.0524,\n",
      "        0.0493, 0.0348, 0.0304, 0.0210, 0.0322])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0203, -0.0374, -0.0353,  0.0144,  0.0333,  0.0278,  0.0256,  0.0286,\n",
      "         0.0362,  0.0291,  0.0393,  0.0428,  0.0325,  0.0190, -0.0289, -0.0377])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0261, -0.0327, -0.0354, -0.0463, -0.0290,  0.0339, -0.0342,  0.0328,\n",
      "         0.0404,  0.0387,  0.0416, -0.0404, -0.0238, -0.0221, -0.0206])\n",
      "Gradient of AB Loss for row 5: tensor([ 0.0315, -0.0397,  0.0366,  0.0141,  0.0322, -0.0260,  0.0461,  0.0535,\n",
      "         0.0345,  0.0444,  0.0473,  0.0391,  0.0276,  0.0319])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0212,  0.0167, -0.0188, -0.0230, -0.0214, -0.0256,  0.0252,  0.0137,\n",
      "        -0.0130, -0.0216, -0.0271,  0.0240,  0.0104,  0.0253,  0.0279, -0.0168,\n",
      "         0.0243, -0.0224, -0.0136, -0.0175,  0.0234, -0.0308,  0.0202])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0407, -0.0272, -0.0161, -0.0305,  0.0350,  0.0488,  0.0334,  0.0141,\n",
      "        -0.0371,  0.0357,  0.0493,  0.0406,  0.0338,  0.0361, -0.0361,  0.0261])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0405, -0.0311, -0.0446, -0.0399, -0.0369,  0.0402, -0.0406,  0.0329,\n",
      "         0.0354,  0.0352,  0.0318, -0.0441, -0.0420, -0.0334, -0.0266])\n",
      "Gradient of AB Loss for row 9: tensor([0.0358, 0.0404, 0.0296, 0.0264, 0.0309, 0.0268, 0.0345, 0.0331, 0.0241,\n",
      "        0.0253, 0.0337, 0.0422, 0.0276, 0.0193])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0152, -0.0220, -0.0220, -0.0172, -0.0207, -0.0284, -0.0172, -0.0123,\n",
      "         0.0264, -0.0244, -0.0173, -0.0324,  0.0130, -0.0236, -0.0212, -0.0157,\n",
      "        -0.0187, -0.0163,  0.0243, -0.0235, -0.0314])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0312, -0.0632, -0.0431, -0.0175,  0.0448, -0.0551,  0.0372,  0.0366,\n",
      "         0.0434,  0.0491,  0.0412, -0.0296, -0.0217])\n",
      "Gradient of AB Loss for row 12: tensor([0.1547, 0.1574, 0.1249, 0.1341])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0509,  0.0548,  0.0304,  0.0427, -0.0410,  0.0408,  0.0354,  0.0432,\n",
      "         0.0570,  0.0398,  0.0351, -0.0343,  0.0267])\n",
      "Gradient of AB Loss for row 14: tensor([0.0314, 0.0402, 0.0130, 0.0249, 0.0391, 0.0300, 0.0325, 0.0345, 0.0292,\n",
      "        0.0312, 0.0287, 0.0351, 0.0374, 0.0223])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0364, -0.0393,  0.0373,  0.0237,  0.0286,  0.0410,  0.0442,  0.0340,\n",
      "         0.0476,  0.0297,  0.0356,  0.0277, -0.0477,  0.0312])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0426,  0.0419,  0.0350,  0.0468, -0.0418,  0.0355,  0.0481,  0.0276,\n",
      "         0.0516,  0.0361,  0.0334, -0.0245, -0.0303,  0.0382])\n",
      "Gradient of AB Loss for row 17: tensor([0.0330, 0.0370, 0.0266, 0.0298, 0.0469, 0.0388, 0.0364, 0.0380, 0.0386,\n",
      "        0.0411, 0.0298, 0.0371, 0.0280, 0.0372])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0331, -0.0296, -0.0257, -0.0188, -0.0320,  0.0255, -0.0193, -0.0223,\n",
      "         0.0463,  0.0296,  0.0355, -0.0455, -0.0320, -0.0280, -0.0189])\n",
      "Gradient of AB Loss for row 19: tensor([0.0393, 0.0593, 0.0416, 0.0282, 0.0462, 0.0408, 0.0321, 0.0422, 0.0628,\n",
      "        0.0487, 0.0339, 0.0245, 0.0363])\n",
      "Gradient of AB Loss for row 20: tensor([0.0252, 0.0383, 0.0200, 0.0351, 0.0451, 0.0352, 0.0197, 0.0451, 0.0351,\n",
      "        0.0449, 0.0362, 0.0201, 0.0268, 0.0351])\n",
      "Gradient of AB Loss for row 21: tensor([ 0.0325, -0.0398,  0.0426,  0.0358,  0.0282,  0.0493,  0.0381,  0.0423,\n",
      "         0.0304,  0.0554,  0.0367,  0.0300,  0.0313,  0.0286])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0361, -0.0398,  0.0351,  0.0117, -0.0382,  0.0253, -0.0228,  0.0366,\n",
      "         0.0500, -0.0364, -0.0451, -0.0345, -0.0246,  0.0349,  0.0407])\n",
      "Gradient of AB Loss for row 23: tensor([-0.6216])\n",
      "Gradient of AB Loss for row 24: tensor([0.0375, 0.0468, 0.0181, 0.0228, 0.0371, 0.0342, 0.0386, 0.0274, 0.0345,\n",
      "        0.0292, 0.0420, 0.0353, 0.0301, 0.0318])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0399, -0.0406,  0.0415,  0.0253,  0.0313, -0.0447,  0.0328,  0.0330,\n",
      "         0.0307,  0.0507,  0.0339, -0.0360, -0.0400,  0.0357])\n",
      "Gradient of AB Loss for row 26: tensor([0.0720, 0.1545, 0.1654, 0.1638])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0318, -0.0454,  0.0308,  0.0230,  0.0244, -0.0499,  0.0413,  0.0422,\n",
      "         0.0421,  0.0363,  0.0338,  0.0338,  0.0223, -0.0370])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0340, -0.0157, -0.0425, -0.0588, -0.0261,  0.0372, -0.0267,  0.0304,\n",
      "         0.0308,  0.0460,  0.0366, -0.0267,  0.0263, -0.0303])\n",
      "Gradient of AB Loss for row 29: tensor([-0.7899])\n",
      "Gradient of AB Loss for row 30: tensor([ 0.0316, -0.0444,  0.0571,  0.0186,  0.0406, -0.0428,  0.0236,  0.0397,\n",
      "         0.0340,  0.0552,  0.0454,  0.0335, -0.0365,  0.0293])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0266, -0.0382,  0.0304,  0.0199,  0.0435, -0.0298,  0.0312,  0.0346,\n",
      "         0.0290,  0.0517,  0.0217, -0.0257, -0.0277,  0.0212,  0.0129])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09788548946380615\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09481535851955414\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.16989412903785706\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0145227909088135\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11270545423030853\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.1735442727804184\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2819787859916687\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.14169247448444366\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.013442372903227806\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1510915458202362\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.6190263574245023e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3481587767601013\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.08557013422250748\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.1283709555864334\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.010592800565063953\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.13950137794017792\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.8243103694359775e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3193986117839813\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.0769568383693695\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.13847804069519043\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01208785455673933\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.14747993648052216\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.0795448979195044e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.32713404297828674\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.07522838562726974\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.15854597091674805\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.013837341219186783\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.15810349583625793\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 2.127862774869982e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3417118191719055\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07871133089065552\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.12290285527706146\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.012721755541861057\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.11547902971506119\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.4221449573526002e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.35646766424179077\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.0845758393406868\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.14990821480751038\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.014814875088632107\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.15347400307655334\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.8371748566892165e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3462974429130554\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.0777800902724266\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.14261691272258759\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.015199162065982819\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.1574554741382599\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.28902588603097e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.37202954292297363\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.0816916823387146\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.13126026093959808\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.011593415401875973\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.12700851261615753\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.5777426032315134e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3526895344257355\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.0762728676199913\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.8064918518066406\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3939860463142395\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.048315826803445816\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06184438616037369\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3512633144855499\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04249540716409683\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.40435871481895447\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.14847999811172485\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9641864895820618\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.10001963376998901\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.17091012001037598\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.22199706733226776\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.0909346342086792\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.007505503017455339\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.08600151538848877\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 9.267763778630922e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.32484012842178345\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06460590660572052\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09978058189153671\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.009617549367249012\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.0983966588973999\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.0928594695869265e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3256186544895172\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06852015852928162\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08466316759586334\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006346913520246744\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08762704581022263\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.669330784714873e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3271234929561615\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06203874945640564\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09203418344259262\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007661997806280851\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.0892762765288353\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 6.190304380204736e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.31021836400032043\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06508274376392365\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.0875144973397255\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.007976418361067772\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.08597972244024277\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 8.92601825785988e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.29949796199798584\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.059789955615997314\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.09585579484701157\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.007608851417899132\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.10896007716655731\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.4571699402665672e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.30987870693206787\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06596284359693527\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.11149482429027557\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.009430055506527424\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.10890964418649673\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.935074100956058e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.29118379950523376\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06312564760446548\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.09716321527957916\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.011032386682927608\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.09469618648290634\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 9.584522064898238e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2963711619377136\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.060860443860292435\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.6322200298309326\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.31288132071495056\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05146845802664757\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05760440230369568\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.387586385011673\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03915209323167801\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3564964830875397\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11673828214406967\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.9670649766921997\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2127252221107483\n",
      "ab_loss: 23.900999069213867\n",
      "Gradient of AB Loss for row 0: tensor([-0.0435,  0.0210,  0.0147,  0.0257, -0.0388,  0.0439,  0.0425,  0.0360,\n",
      "         0.0385, -0.0374,  0.0360, -0.0428, -0.0315,  0.0187,  0.0313])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0274, -0.0367,  0.0197,  0.0565, -0.0565,  0.0422,  0.0545,  0.0497,\n",
      "         0.0453,  0.0378,  0.0404, -0.0379,  0.0214])\n",
      "Gradient of AB Loss for row 2: tensor([ 0.0257, -0.0446,  0.0452,  0.0176,  0.0402,  0.0336,  0.0395,  0.0439,\n",
      "         0.0385,  0.0462,  0.0544,  0.0464,  0.0315,  0.0370])\n",
      "Gradient of AB Loss for row 3: tensor([0.0348, 0.0502, 0.0126, 0.0354, 0.0300, 0.0386, 0.0393, 0.0397, 0.0438,\n",
      "        0.0539, 0.0322, 0.0219, 0.0296, 0.0398])\n",
      "Gradient of AB Loss for row 4: tensor([ 0.0446, -0.0412,  0.0454,  0.0229,  0.0329,  0.0365,  0.0313,  0.0405,\n",
      "         0.0419,  0.0505,  0.0299,  0.0323,  0.0328,  0.0266])\n",
      "Gradient of AB Loss for row 5: tensor([0.0408, 0.0439, 0.0246, 0.0234, 0.0346, 0.0298, 0.0325, 0.0429, 0.0419,\n",
      "        0.0402, 0.0413, 0.0298, 0.0373, 0.0361])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0246,  0.0389, -0.0433,  0.0283,  0.0414,  0.0460,  0.0341,  0.0378,\n",
      "         0.0518,  0.0374,  0.0245,  0.0226,  0.0234])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0399, -0.0335, -0.0424, -0.0191, -0.0311, -0.0420,  0.0388,  0.0345,\n",
      "        -0.0323,  0.0342,  0.0333,  0.0178,  0.0444,  0.0358,  0.0375, -0.0380,\n",
      "        -0.0304, -0.0294])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0287, -0.0352,  0.0143, -0.0165,  0.0368, -0.0384,  0.0395,  0.0349,\n",
      "        -0.0378,  0.0224, -0.0203, -0.0257, -0.0351,  0.0280,  0.0236])\n",
      "Gradient of AB Loss for row 9: tensor([0.0429, 0.0419, 0.0220, 0.0287, 0.0470, 0.0468, 0.0399, 0.0330, 0.0400,\n",
      "        0.0330, 0.0344, 0.0397, 0.0305, 0.0333])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0530,  0.0457,  0.0245,  0.0374, -0.0357,  0.0327,  0.0421,  0.0372,\n",
      "         0.0423,  0.0520,  0.0324,  0.0291, -0.0296,  0.0172])\n",
      "Gradient of AB Loss for row 11: tensor([0.0369, 0.0360, 0.0383, 0.0435, 0.0444, 0.0479, 0.0518, 0.0421, 0.0466,\n",
      "        0.0382, 0.0312, 0.0320, 0.0412, 0.0281])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0296,  0.0218, -0.0152, -0.0228, -0.0400, -0.0341, -0.0202, -0.0352,\n",
      "        -0.0274,  0.0428,  0.0262,  0.0289, -0.0287, -0.0111, -0.0123,  0.0311,\n",
      "        -0.0308, -0.0319])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0303, -0.0316, -0.0263, -0.0359,  0.0339,  0.0302,  0.0156, -0.0220,\n",
      "         0.0313,  0.0445,  0.0392,  0.0290,  0.0230, -0.0312,  0.0316, -0.0276,\n",
      "         0.0119])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0398, -0.0280,  0.0514,  0.0095,  0.0433, -0.0322,  0.0277,  0.0277,\n",
      "         0.0301,  0.0486,  0.0277, -0.0269, -0.0239,  0.0337,  0.0319])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0269,  0.0273, -0.0406, -0.0247,  0.0385,  0.0112,  0.0337, -0.0355,\n",
      "         0.0378,  0.0451,  0.0369,  0.0300, -0.0475,  0.0206,  0.0369])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0247,  0.0374, -0.0556,  0.0449,  0.0376,  0.0333,  0.0247,  0.0454,\n",
      "         0.0518,  0.0381,  0.0215,  0.0441,  0.0251])\n",
      "Gradient of AB Loss for row 17: tensor([0.1109, 0.1624, 0.0720, 0.1245])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0379,  0.0326,  0.0113,  0.0290, -0.0504,  0.0244,  0.0400,  0.0330,\n",
      "         0.0204,  0.0480, -0.0376, -0.0354, -0.0249,  0.0361,  0.0328])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0266, -0.0219, -0.0228, -0.0300,  0.0185,  0.0342, -0.0279, -0.0262,\n",
      "        -0.0296,  0.0215,  0.0208,  0.0269,  0.0216, -0.0226,  0.0143,  0.0270,\n",
      "        -0.0144,  0.0301,  0.0341, -0.0278, -0.0237])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0335,  0.0438,  0.0247,  0.0379,  0.0303,  0.0379,  0.0424,  0.0349,\n",
      "         0.0551,  0.0367,  0.0433, -0.0300, -0.0303,  0.0227])\n",
      "Gradient of AB Loss for row 21: tensor([-0.4949])\n",
      "Gradient of AB Loss for row 22: tensor([0.0397, 0.0333, 0.0279, 0.0303, 0.0306, 0.0452, 0.0298, 0.0395, 0.0422,\n",
      "        0.0401, 0.0289, 0.0423, 0.0323, 0.0261])\n",
      "Gradient of AB Loss for row 23: tensor([0.0417, 0.0534, 0.0152, 0.0358, 0.0471, 0.0401, 0.0358, 0.0482, 0.0474,\n",
      "        0.0588, 0.0354, 0.0293, 0.0239, 0.0332])\n",
      "Gradient of AB Loss for row 24: tensor([0.0258, 0.0471, 0.0228, 0.0388, 0.0380, 0.0193, 0.0327, 0.0319, 0.0531,\n",
      "        0.0264, 0.0433, 0.0293, 0.0339, 0.0316])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0300, -0.0340,  0.0339, -0.0412,  0.0174, -0.0240,  0.0447, -0.0470,\n",
      "         0.0426, -0.0349,  0.0354,  0.0432,  0.0188, -0.0272, -0.0299])\n",
      "Gradient of AB Loss for row 26: tensor([0.0366, 0.0524, 0.0196, 0.0256, 0.0293, 0.0393, 0.0309, 0.0481, 0.0493,\n",
      "        0.0176, 0.0506, 0.0449, 0.0322, 0.0268])\n",
      "Gradient of AB Loss for row 27: tensor([0.0124, 0.0149, 0.0229, 0.0149, 0.0255, 0.0268, 0.0239, 0.0244, 0.0191,\n",
      "        0.0326, 0.0251, 0.0096, 0.0292, 0.0255, 0.0191, 0.0214, 0.0340, 0.0239,\n",
      "        0.0111, 0.0325, 0.0225, 0.0196, 0.0236])\n",
      "Gradient of AB Loss for row 28: tensor([0.0350, 0.0498, 0.0212, 0.0456, 0.0449, 0.0424, 0.0441, 0.0432, 0.0469,\n",
      "        0.0394, 0.0408, 0.0411, 0.0263, 0.0219])\n",
      "Gradient of AB Loss for row 29: tensor([-0.6169])\n",
      "Gradient of AB Loss for row 30: tensor([-0.4698])\n",
      "Gradient of AB Loss for row 31: tensor([0.0338, 0.0502, 0.0157, 0.0405, 0.0473, 0.0393, 0.0265, 0.0407, 0.0561,\n",
      "        0.0292, 0.0295, 0.0317, 0.0278, 0.0383])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10660137236118317\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.11324738711118698\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.19312074780464172\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0783977508544922\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.12420476227998734\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.19538164138793945\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.31370219588279724\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.13703136146068573\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.010748105123639107\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.15036483108997345\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 2.031321777451467e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3383125066757202\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.08944184333086014\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.14122678339481354\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.012990240938961506\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.16304177045822144\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.5812817721894135e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.32994234561920166\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08260475099086761\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1300341933965683\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01148599199950695\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.127197727560997\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.339766297903111e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3514806926250458\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09567396342754364\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.14495185017585754\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.014087212271988392\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.14069591462612152\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.246098335627721e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3729093074798584\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.0855896845459938\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.13304245471954346\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.01578880473971367\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.14083780348300934\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.3766660034164602e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3989104926586151\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.09342431277036667\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.11566933244466782\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.01167721301317215\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.11154073476791382\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.0630313296289273e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.38773563504219055\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09444531053304672\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.13471855223178864\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.010471848770976067\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.1348036825656891\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.4437757656082795e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3706444799900055\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08788245916366577\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.12074790149927139\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.011212515644729137\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.12733998894691467\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.5721233204146756e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3729833662509918\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.09120974689722061\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.9781601428985596\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4456804096698761\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.0532301589846611\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.07170999050140381\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.4026283919811249\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04875247925519943\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.44424906373023987\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.15827542543411255\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0263437032699585\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.11003924906253815\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1808939427137375\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2572331726551056\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.0948723554611206\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.008136277087032795\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.09612978994846344\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.1514544873136856e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.33834293484687805\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.0714857280254364\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.08290298283100128\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.008005976676940918\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.08457811176776886\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 8.877169554999398e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3340895473957062\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.0717884972691536\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07692743837833405\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.00629036920145154\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07624215632677078\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.6828837212656822e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.39174312353134155\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.0765838772058487\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.08388309925794601\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007187467534095049\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.07897011190652847\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.4529333292045976e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.33892884850502014\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07553800940513611\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08614938706159592\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.007065372075885534\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.08150511234998703\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 7.364918119812103e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.34175050258636475\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.07010253518819809\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.09906011819839478\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.010491659864783287\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.10324384272098541\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 7.816465252830085e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.33695682883262634\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07371490448713303\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.11487451940774918\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.009827827103435993\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.11294587701559067\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 6.084990289423331e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3386901617050171\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07591540366411209\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07548357546329498\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.005898233037441969\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07120932638645172\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.000401650408378e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.32220181822776794\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06847871094942093\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.981717824935913\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3775780200958252\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.050382163375616074\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06849699467420578\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3966066837310791\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.04077815264463425\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.38391727209091187\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.1294136643409729\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.2275094985961914\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2505474388599396\n",
      "ab_loss: 23.42687225341797\n",
      "Gradient of AB Loss for row 0: tensor([0.0294, 0.0447, 0.0389, 0.0359, 0.0386, 0.0319, 0.0361, 0.0406, 0.0544,\n",
      "        0.0222, 0.0318, 0.0381, 0.0260, 0.0208])\n",
      "Gradient of AB Loss for row 1: tensor([0.1364, 0.0908, 0.1222, 0.1142])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0442,  0.0426,  0.0202,  0.0444, -0.0426,  0.0220,  0.0290,  0.0404,\n",
      "         0.0549, -0.0465,  0.0341, -0.0289, -0.0247,  0.0252,  0.0345])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0287, -0.0265, -0.0295,  0.0334,  0.0283,  0.0160, -0.0229,  0.0214,\n",
      "         0.0413,  0.0404,  0.0376,  0.0266,  0.0344, -0.0273,  0.0293, -0.0417,\n",
      "         0.0124])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0195, -0.0238,  0.0277,  0.0164, -0.0340, -0.0212, -0.0232,  0.0188,\n",
      "         0.0212,  0.0236,  0.0349,  0.0263,  0.0080,  0.0279, -0.0219, -0.0190,\n",
      "        -0.0187,  0.0225,  0.0229, -0.0300, -0.0186])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0738, -0.0269, -0.0764,  0.0704, -0.0764, -0.0602,  0.1079])\n",
      "Gradient of AB Loss for row 6: tensor([-0.5087])\n",
      "Gradient of AB Loss for row 7: tensor([-0.4548])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0312,  0.0168, -0.0195, -0.0191,  0.0305, -0.0271, -0.0204,  0.0262,\n",
      "         0.0106,  0.0328,  0.0226, -0.0124,  0.0112,  0.0280, -0.0112, -0.0174,\n",
      "        -0.0135,  0.0306,  0.0285,  0.0215, -0.0180,  0.0159])\n",
      "Gradient of AB Loss for row 9: tensor([0.0401, 0.0477, 0.0305, 0.0306, 0.0451, 0.0323, 0.0323, 0.0435, 0.0434,\n",
      "        0.0499, 0.0333, 0.0284, 0.0172, 0.0182])\n",
      "Gradient of AB Loss for row 10: tensor([ 0.0417, -0.0324,  0.0434,  0.0145,  0.0298,  0.0472,  0.0588,  0.0462,\n",
      "         0.0427,  0.0461,  0.0331,  0.0321,  0.0297,  0.0239])\n",
      "Gradient of AB Loss for row 11: tensor([0.0290, 0.0473, 0.0202, 0.0286, 0.0444, 0.0292, 0.0310, 0.0443, 0.0466,\n",
      "        0.0541, 0.0475, 0.0295, 0.0304, 0.0289])\n",
      "Gradient of AB Loss for row 12: tensor([0.1340, 0.1468, 0.1322, 0.1183])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0216, -0.0457, -0.0527, -0.0352,  0.0384, -0.0418,  0.0399,  0.0488,\n",
      "         0.0589,  0.0370,  0.0285, -0.0304, -0.0243,  0.0239])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0358, -0.0246, -0.0392,  0.0126, -0.0176,  0.0166, -0.0521,  0.0320,\n",
      "        -0.0342,  0.0327,  0.0326,  0.0297, -0.0481, -0.0212, -0.0312])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0417, -0.0455,  0.0514,  0.0375, -0.0457,  0.0307,  0.0346,  0.0416,\n",
      "         0.0346,  0.0361,  0.0436,  0.0227, -0.0355,  0.0252])\n",
      "Gradient of AB Loss for row 16: tensor([0.0300, 0.0437, 0.0173, 0.0173, 0.0349, 0.0393, 0.0363, 0.0402, 0.0399,\n",
      "        0.0349, 0.0426, 0.0316, 0.0378, 0.0256])\n",
      "Gradient of AB Loss for row 17: tensor([-0.4542])\n",
      "Gradient of AB Loss for row 18: tensor([0.0295, 0.0448, 0.0236, 0.0321, 0.0417, 0.0265, 0.0270, 0.0389, 0.0495,\n",
      "        0.0339, 0.0347, 0.0331, 0.0272, 0.0206])\n",
      "Gradient of AB Loss for row 19: tensor([ 0.0303, -0.0431,  0.0435,  0.0316,  0.0361, -0.0491,  0.0353,  0.0308,\n",
      "         0.0457,  0.0374,  0.0429, -0.0302, -0.0242,  0.0210,  0.0190])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0242, -0.0472,  0.0199,  0.0381, -0.0443,  0.0495,  0.0372,  0.0374,\n",
      "         0.0562,  0.0467,  0.0298, -0.0393,  0.0377])\n",
      "Gradient of AB Loss for row 21: tensor([0.0275, 0.0498, 0.0072, 0.0329, 0.0420, 0.0264, 0.0326, 0.0332, 0.0506,\n",
      "        0.0493, 0.0455, 0.0272, 0.0310, 0.0414])\n",
      "Gradient of AB Loss for row 22: tensor([ 0.0188,  0.0266, -0.0229,  0.0308,  0.0194,  0.0298,  0.0286, -0.0215,\n",
      "         0.0252,  0.0206,  0.0149,  0.0360,  0.0259,  0.0112,  0.0258, -0.0159,\n",
      "         0.0286,  0.0225,  0.0203, -0.0260,  0.0248,  0.0206])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0608, -0.0429,  0.0516,  0.0347, -0.0386,  0.0275,  0.0405,  0.0341,\n",
      "         0.0383,  0.0538,  0.0412,  0.0306, -0.0305,  0.0404])\n",
      "Gradient of AB Loss for row 24: tensor([0.0315, 0.0457, 0.0208, 0.0330, 0.0413, 0.0436, 0.0294, 0.0375, 0.0361,\n",
      "        0.0499, 0.0430, 0.0296, 0.0255, 0.0215])\n",
      "Gradient of AB Loss for row 25: tensor([-0.5094])\n",
      "Gradient of AB Loss for row 26: tensor([0.1439, 0.1506, 0.0727, 0.1167])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0281, -0.0219,  0.0214, -0.0361, -0.0333, -0.0126, -0.0310,  0.0283,\n",
      "        -0.0252, -0.0271,  0.0283,  0.0301, -0.0248,  0.0365, -0.0141, -0.0208,\n",
      "        -0.0227,  0.0254,  0.0268, -0.0263, -0.0309])\n",
      "Gradient of AB Loss for row 28: tensor([0.0382, 0.0354, 0.0119, 0.0306, 0.0300, 0.0441, 0.0382, 0.0407, 0.0450,\n",
      "        0.0402, 0.0360, 0.0287, 0.0279, 0.0332])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0401,  0.0373, -0.0483,  0.0534,  0.0163,  0.0356,  0.0372,  0.0299,\n",
      "         0.0487,  0.0511,  0.0458,  0.0362,  0.0365,  0.0298])\n",
      "Gradient of AB Loss for row 30: tensor([ 0.1598,  0.1065, -0.1024, -0.1302])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0289, -0.0489, -0.0507, -0.0492, -0.0246, -0.0346,  0.0392, -0.0479,\n",
      "         0.0317, -0.0356,  0.0486,  0.0191, -0.0395, -0.0257, -0.0269])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09389620274305344\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09689923375844955\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.18436607718467712\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.91972416639328\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10726451128721237\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.17054623365402222\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2572088837623596\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.13663138449192047\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.011358577758073807\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.15354691445827484\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.166654883810736e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.28398677706718445\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07787773758172989\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.13890843093395233\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.015528538264334202\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.15867508947849274\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.211222899755171e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2695041000843048\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07517428696155548\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.14542368054389954\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.017061028629541397\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.16214387118816376\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.0965318653077816e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3200305700302124\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.08981812000274658\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.11748233437538147\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.010527707636356354\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.11868061125278473\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.8271402169034445e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3187806308269501\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.08397430926561356\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.10686896741390228\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.01018595602363348\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10387473553419113\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 8.95296281555602e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.32865142822265625\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08868983387947083\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.1140846312046051\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.010118842124938965\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.11841334402561188\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.2553506012480398e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3136839270591736\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08429452031850815\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.1136719137430191\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.012176244519650936\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.12132635712623596\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 9.198504735685731e-10\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.31300950050354004\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08094087243080139\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.11405275017023087\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.013920356519520283\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.11533040553331375\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 9.451081028899466e-10\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3349141776561737\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.08735741674900055\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.553035259246826\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4047045409679413\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04562593251466751\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.05903784558176994\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.33770474791526794\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.03891009837388992\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.37104448676109314\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1323786824941635\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.8812026381492615\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09353135526180267\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1779710054397583\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.22046786546707153\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08657906204462051\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.007978031411767006\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.09167508035898209\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.0062505273467082e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2834744453430176\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06558462977409363\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.06696367263793945\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.00551217794418335\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.07064057141542435\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 9.127649192031129e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.2886206805706024\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06722412258386612\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08114581555128098\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006687242537736893\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07045486569404602\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 8.859707412156581e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3213081955909729\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.0694100558757782\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.07047451287508011\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.006213177461177111\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.0699453204870224\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 6.469895175165163e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.2985903024673462\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06901157647371292\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.06799333542585373\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.00667021656408906\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.0628151223063469\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.30298372091886e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2758687436580658\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.061258796602487564\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.0983305275440216\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.011814375407993793\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.10695406049489975\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.9334289724781684e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.30272963643074036\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07328171283006668\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.09057589620351791\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.00913398340344429\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.09107262641191483\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.340501539943034e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.31437841057777405\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07717663049697876\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.11735354363918304\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.011331245303153992\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.10698903352022171\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.593933342737614e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2911985218524933\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06911575794219971\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.5223467350006104\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3459877073764801\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04476258531212807\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05904378741979599\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.38499996066093445\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.041022591292858124\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3516903519630432\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11763567477464676\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.8727307319641113\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.20817433297634125\n",
      "ab_loss: 23.73721694946289\n",
      "Gradient of AB Loss for row 0: tensor([-0.5779])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0323, -0.0340, -0.0425, -0.0451, -0.0353,  0.0441, -0.0405,  0.0346,\n",
      "         0.0486,  0.0438,  0.0432, -0.0302, -0.0346, -0.0379])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0356, -0.0258,  0.0509,  0.0091,  0.0452, -0.0334,  0.0341,  0.0293,\n",
      "         0.0545,  0.0505, -0.0309,  0.0192, -0.0371,  0.0253,  0.0239])\n",
      "Gradient of AB Loss for row 3: tensor([0.0329, 0.0449, 0.0243, 0.0350, 0.0432, 0.0356, 0.0392, 0.0416, 0.0453,\n",
      "        0.0336, 0.0432, 0.0315, 0.0303, 0.0363])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0245, -0.0229,  0.0254,  0.0238, -0.0272, -0.0218,  0.0203,  0.0121,\n",
      "         0.0269,  0.0212,  0.0280, -0.0214,  0.0186,  0.0265, -0.0171, -0.0288,\n",
      "        -0.0211, -0.0193,  0.0307,  0.0212,  0.0207, -0.0238,  0.0122])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0326, -0.0211, -0.0257, -0.0344, -0.0323, -0.0152,  0.0306, -0.0261,\n",
      "        -0.0194, -0.0303,  0.0193,  0.0350,  0.0230, -0.0217,  0.0259, -0.0207,\n",
      "        -0.0127, -0.0137,  0.0245, -0.0318,  0.0223])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0294, -0.0473, -0.0339, -0.0275, -0.0437,  0.0305,  0.0289, -0.0254,\n",
      "         0.0294,  0.0383,  0.0428,  0.0408,  0.0263,  0.0261, -0.0229,  0.0261])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0302, -0.0391,  0.0430,  0.0108,  0.0509, -0.0291,  0.0326,  0.0376,\n",
      "         0.0335,  0.0484,  0.0256, -0.0287, -0.0411,  0.0254,  0.0334])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0270, -0.0323, -0.0281, -0.0382,  0.0464,  0.0418,  0.0440,  0.0121,\n",
      "        -0.0325,  0.0380,  0.0386,  0.0460,  0.0357,  0.0369, -0.0373,  0.0142])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0474,  0.0451,  0.0107,  0.0407, -0.0365,  0.0323,  0.0369,  0.0269,\n",
      "         0.0487, -0.0311,  0.0253, -0.0433, -0.0254,  0.0215,  0.0234])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0464, -0.0190, -0.0486, -0.0359, -0.0309,  0.0274, -0.0348,  0.0348,\n",
      "         0.0271,  0.0377,  0.0291, -0.0398, -0.0351, -0.0254])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0221, -0.0236, -0.0380, -0.0271,  0.0320,  0.0280,  0.0184,  0.0279,\n",
      "         0.0331, -0.0200, -0.0264,  0.0288,  0.0271,  0.0296,  0.0240,  0.0148,\n",
      "         0.0343,  0.0176,  0.0317, -0.0236,  0.0263, -0.0287])\n",
      "Gradient of AB Loss for row 12: tensor([0.0385, 0.0385, 0.0314, 0.0262, 0.0443, 0.0338, 0.0380, 0.0350, 0.0405,\n",
      "        0.0381, 0.0207, 0.0375, 0.0349, 0.0368])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0299, -0.0478, -0.0421,  0.0143, -0.0226, -0.0369,  0.0328, -0.0312,\n",
      "         0.0390, -0.0389, -0.0337,  0.0253, -0.0309, -0.0262, -0.0205])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0253, -0.0417,  0.0372,  0.0262,  0.0106, -0.0215,  0.0462, -0.0470,\n",
      "         0.0392,  0.0405,  0.0377,  0.0400,  0.0420,  0.0305, -0.0297])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0417,  0.0370,  0.0153,  0.0371, -0.0437,  0.0381,  0.0391,  0.0221,\n",
      "         0.0456, -0.0301,  0.0197, -0.0196, -0.0388,  0.0333,  0.0327])\n",
      "Gradient of AB Loss for row 16: tensor([ 0.0512,  0.0328,  0.0251,  0.0402,  0.0577,  0.0438,  0.0381,  0.0544,\n",
      "         0.0283,  0.0324,  0.0248, -0.0369,  0.0211])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0409,  0.0354,  0.0128,  0.0403, -0.0400,  0.0261,  0.0304,  0.0265,\n",
      "         0.0399,  0.0418,  0.0317, -0.0426, -0.0169,  0.0232,  0.0303])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0234, -0.0310, -0.0418,  0.0311,  0.0128, -0.0193,  0.0304,  0.0358,\n",
      "         0.0402,  0.0334,  0.0275,  0.0428,  0.0290, -0.0324,  0.0239, -0.0319,\n",
      "         0.0083, -0.0245])\n",
      "Gradient of AB Loss for row 19: tensor([0.0396, 0.0534, 0.0186, 0.0308, 0.0449, 0.0451, 0.0377, 0.0413, 0.0456,\n",
      "        0.0417, 0.0357, 0.0417, 0.0303, 0.0267])\n",
      "Gradient of AB Loss for row 20: tensor([-0.4474])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0454,  0.0441,  0.0112,  0.0490, -0.0397,  0.0370,  0.0293,  0.0404,\n",
      "         0.0414,  0.0452,  0.0328,  0.0220, -0.0326,  0.0365])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0397,  0.0415, -0.0370,  0.0522,  0.0226,  0.0169,  0.0462,  0.0262,\n",
      "         0.0313,  0.0321,  0.0270,  0.0415,  0.0287,  0.0265])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0360, -0.0322, -0.0361, -0.0452, -0.0367, -0.0394,  0.0451,  0.0325,\n",
      "        -0.0227,  0.0327,  0.0397,  0.0368,  0.0365, -0.0337, -0.0398,  0.0322])\n",
      "Gradient of AB Loss for row 24: tensor([0.0374, 0.0441, 0.0139, 0.0276, 0.0304, 0.0483, 0.0465, 0.0533, 0.0448,\n",
      "        0.0355, 0.0350, 0.0434, 0.0385, 0.0258])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0457,  0.0403, -0.0454,  0.0423,  0.0124,  0.0337, -0.0294,  0.0351,\n",
      "         0.0386,  0.0351, -0.0350,  0.0275, -0.0179,  0.0316,  0.0296])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0192, -0.0338,  0.0288,  0.0218,  0.0125, -0.0210,  0.0262, -0.0179,\n",
      "         0.0381,  0.0294,  0.0349,  0.0254,  0.0164,  0.0256, -0.0315])\n",
      "Gradient of AB Loss for row 27: tensor([0.0428, 0.0585, 0.0190, 0.0312, 0.0481, 0.0414, 0.0410, 0.0446, 0.0299,\n",
      "        0.0515, 0.0470, 0.0323, 0.0246, 0.0439])\n",
      "Gradient of AB Loss for row 28: tensor([ 0.0406, -0.0340,  0.0504,  0.0226,  0.0382, -0.0433,  0.0326,  0.0468,\n",
      "         0.0268,  0.0540,  0.0248,  0.0411,  0.0298,  0.0313])\n",
      "Gradient of AB Loss for row 29: tensor([-0.3823])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0426,  0.0370, -0.0451,  0.0472,  0.0243,  0.0253, -0.0436,  0.0329,\n",
      "         0.0358,  0.0480,  0.0497,  0.0504,  0.0301,  0.0261])\n",
      "Gradient of AB Loss for row 31: tensor([0.0412, 0.0404, 0.0348, 0.0259, 0.0438, 0.0349, 0.0302, 0.0318, 0.0429,\n",
      "        0.0327, 0.0275, 0.0281, 0.0291, 0.0323])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09648972004652023\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09816096723079681\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.16853760182857513\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9718566536903381\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10940772294998169\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.15882553160190582\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2723327577114105\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.13947363197803497\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.013882382772862911\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1527346819639206\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.0463978572516908e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3033280670642853\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07512947916984558\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.13673165440559387\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.00986742414534092\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.15231676399707794\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.2484537847967658e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.30546092987060547\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08279576152563095\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1678709238767624\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01730184443295002\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.17726872861385345\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.0661574956216668e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.31719309091567993\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.07946239411830902\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.13039328157901764\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.010969062335789204\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.13270895183086395\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.445023878332563e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3563596308231354\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.08038447052240372\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11699734628200531\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.01155869197100401\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.11323433369398117\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 9.329593764206834e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3518502414226532\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08015643060207367\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.11599210649728775\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.010680539533495903\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.116678886115551\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.3488762329316728e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3356465697288513\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08058304339647293\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.14793305099010468\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.014446118846535683\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.16094230115413666\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.0290447383098922e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.32698190212249756\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07843969017267227\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.11115343123674393\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.014570459723472595\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.11258676648139954\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 3.3455707093565934e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.335789293050766\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07751249521970749\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.729645013809204\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.39244452118873596\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.053743038326501846\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06097123399376869\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3743511438369751\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.048959970474243164\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.40039050579071045\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.14336763322353363\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.935671329498291\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09659179300069809\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1600138545036316\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.22063006460666656\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06800615787506104\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.005959006492048502\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.06957929581403732\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 8.109227733754665e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2897125780582428\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.060044240206480026\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.08018234372138977\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.007287194486707449\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.08812157064676285\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.190469331064264e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.30976298451423645\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06599261611700058\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07798986881971359\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.007213931530714035\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07707136869430542\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 7.500268184301717e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3627915382385254\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.07302775233983994\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09936751425266266\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.00884093064814806\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.0965641587972641\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 6.379046180171599e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.31520941853523254\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.0662958100438118\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08731310069561005\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.008926870301365852\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.08775452524423599\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.4011481974662843e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3095545470714569\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.0608455054461956\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.07782656699419022\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.009113200008869171\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.08450378477573395\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.0646920012291616e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3179219663143158\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07068214565515518\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08849242329597473\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.008171770721673965\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08075368404388428\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.451453898354998e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3392634987831116\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07467953860759735\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.09299372881650925\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.008417635224759579\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.08881264925003052\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 7.750334263256775e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.28835660219192505\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.0602448508143425\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.723182201385498\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.340229868888855\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05042979121208191\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06097711995244026\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3336593806743622\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.032813768833875656\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.34035786986351013\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11020594090223312\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.913607120513916\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.20941215753555298\n",
      "ab_loss: 24.358657836914062\n",
      "Gradient of AB Loss for row 0: tensor([ 0.0317, -0.0279,  0.0350,  0.0240,  0.0360, -0.0274,  0.0302,  0.0352,\n",
      "         0.0307,  0.0393,  0.0387,  0.0289,  0.0266,  0.0229])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0317, -0.0239, -0.0442, -0.0322, -0.0425,  0.0229, -0.0286, -0.0243,\n",
      "        -0.0323,  0.0441, -0.0225, -0.0364, -0.0283, -0.0209, -0.0158, -0.0157])\n",
      "Gradient of AB Loss for row 2: tensor([ 0.1687,  0.1390, -0.1138,  0.1388])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0268, -0.0285,  0.0224, -0.0190, -0.0245,  0.0249,  0.0258, -0.0180,\n",
      "        -0.0292,  0.0098,  0.0323,  0.0235, -0.0313,  0.0129, -0.0168, -0.0132,\n",
      "        -0.0188,  0.0294,  0.0222, -0.0291,  0.0165])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0325, -0.0344, -0.0517, -0.0297, -0.0446, -0.0403,  0.0495, -0.0245,\n",
      "        -0.0220,  0.0364, -0.0303,  0.0350, -0.0428, -0.0312, -0.0140])\n",
      "Gradient of AB Loss for row 5: tensor([0.1451, 0.1345, 0.0906, 0.1203])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0268, -0.0358,  0.0116,  0.0419, -0.0393,  0.0462,  0.0400,  0.0396,\n",
      "         0.0513,  0.0274,  0.0324, -0.0361, -0.0372,  0.0345])\n",
      "Gradient of AB Loss for row 7: tensor([ 0.0387, -0.0345,  0.0403,  0.0203,  0.0460, -0.0410,  0.0398,  0.0429,\n",
      "         0.0339,  0.0445,  0.0359,  0.0251,  0.0200,  0.0291])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0394, -0.0410,  0.0323,  0.0277,  0.0365,  0.0315, -0.0339,  0.0403,\n",
      "         0.0410,  0.0352,  0.0332,  0.0382,  0.0317,  0.0316,  0.0275,  0.0279,\n",
      "        -0.0357,  0.0296])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0279,  0.0244, -0.0174,  0.0290,  0.0227, -0.0244,  0.0149,  0.0091,\n",
      "         0.0255,  0.0237,  0.0218, -0.0214,  0.0119,  0.0283, -0.0217, -0.0187,\n",
      "        -0.0273,  0.0356,  0.0215,  0.0173, -0.0320,  0.0190])\n",
      "Gradient of AB Loss for row 10: tensor([0.0425, 0.0447, 0.0174, 0.0284, 0.0347, 0.0442, 0.0338, 0.0393, 0.0449,\n",
      "        0.0525, 0.0353, 0.0205, 0.0350, 0.0255])\n",
      "Gradient of AB Loss for row 11: tensor([-0.4237])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0265, -0.0341, -0.0327, -0.0345,  0.0398,  0.0397,  0.0326,  0.0150,\n",
      "        -0.0278,  0.0312,  0.0348,  0.0385,  0.0404,  0.0276,  0.0289, -0.0356,\n",
      "        -0.0350,  0.0151])\n",
      "Gradient of AB Loss for row 13: tensor([0.0322, 0.0472, 0.0153, 0.0261, 0.0462, 0.0373, 0.0393, 0.0387, 0.0556,\n",
      "        0.0439, 0.0441, 0.0362, 0.0311, 0.0496])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0255, -0.0339, -0.0294,  0.0455,  0.0258,  0.0254,  0.0153, -0.0305,\n",
      "         0.0327,  0.0427,  0.0315,  0.0440,  0.0316,  0.0344,  0.0219,  0.0284,\n",
      "         0.0253])\n",
      "Gradient of AB Loss for row 15: tensor([0.0364, 0.0456, 0.0215, 0.0329, 0.0383, 0.0393, 0.0279, 0.0406, 0.0353,\n",
      "        0.0332, 0.0434, 0.0441, 0.0308, 0.0441])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0339, -0.0421, -0.0337,  0.0198,  0.0140, -0.0310,  0.0319,  0.0459,\n",
      "         0.0377,  0.0286,  0.0342, -0.0401,  0.0318,  0.0257, -0.0236,  0.0236,\n",
      "        -0.0120])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0557, -0.0324,  0.0485,  0.0512, -0.0407,  0.0335,  0.0483,  0.0253,\n",
      "         0.0510,  0.0450,  0.0399,  0.0190, -0.0339,  0.0315])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0383, -0.0401,  0.0456,  0.0237,  0.0348, -0.0384,  0.0353,  0.0443,\n",
      "         0.0380,  0.0493,  0.0205, -0.0224, -0.0384,  0.0197,  0.0250])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0451, -0.0463, -0.0547,  0.0269, -0.0507,  0.0375,  0.0241, -0.0452,\n",
      "         0.0395,  0.0446,  0.0455,  0.0455,  0.0400,  0.0321])\n",
      "Gradient of AB Loss for row 20: tensor([ 0.1631,  0.0909, -0.0858,  0.1653])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0156, -0.0271, -0.0226, -0.0178, -0.0198, -0.0163, -0.0168,  0.0215,\n",
      "        -0.0156, -0.0197, -0.0216, -0.0281,  0.0101, -0.0121, -0.0208,  0.0074,\n",
      "        -0.0154, -0.0122, -0.0143, -0.0128,  0.0231, -0.0229, -0.0257])\n",
      "Gradient of AB Loss for row 22: tensor([-0.6140])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0286, -0.0216,  0.0192, -0.0236, -0.0283, -0.0352, -0.0267, -0.0237,\n",
      "         0.0260, -0.0156, -0.0205,  0.0139, -0.0174, -0.0131,  0.0276, -0.0150,\n",
      "        -0.0218, -0.0225,  0.0299, -0.0198, -0.0285])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0213, -0.0434,  0.0377,  0.0175,  0.0384, -0.0370,  0.0283,  0.0282,\n",
      "         0.0310,  0.0383,  0.0255, -0.0370, -0.0328,  0.0184,  0.0267])\n",
      "Gradient of AB Loss for row 25: tensor([0.0968, 0.2004, 0.1290, 0.1509])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0545, -0.0366,  0.0447,  0.0294, -0.0489,  0.0385,  0.0369,  0.0401,\n",
      "         0.0412,  0.0477,  0.0391,  0.0328, -0.0392,  0.0310])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0348, -0.0434,  0.0525,  0.0115,  0.0247,  0.0418,  0.0303,  0.0377,\n",
      "         0.0503,  0.0523,  0.0391,  0.0305, -0.0341,  0.0399])\n",
      "Gradient of AB Loss for row 28: tensor([-0.1248, -0.1079, -0.0861, -0.1245])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0368, -0.0199, -0.0428, -0.0446, -0.0346, -0.0403,  0.0374, -0.0281,\n",
      "         0.0437,  0.0387,  0.0420,  0.0250, -0.0398,  0.0209, -0.0308])\n",
      "Gradient of AB Loss for row 30: tensor([0.0304, 0.0549, 0.0202, 0.0226, 0.0433, 0.0434, 0.0315, 0.0449, 0.0612,\n",
      "        0.0440, 0.0385, 0.0274, 0.0414, 0.0246])\n",
      "Gradient of AB Loss for row 31: tensor([ 0.0388, -0.0536,  0.0594,  0.0194,  0.0325,  0.0338,  0.0312,  0.0256,\n",
      "         0.0293,  0.0490,  0.0490,  0.0391,  0.0368,  0.0285])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08711903542280197\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.08022896200418472\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1568809598684311\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.8635756969451904\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.09784575551748276\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.1596536934375763\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.24879670143127441\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.10686005651950836\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.010255974717438221\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.11849271506071091\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.1558114465515246e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.2740158140659332\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07617976516485214\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.11316452920436859\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.011285943910479546\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.1330597996711731\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.345328959345693e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2892441749572754\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08695510774850845\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.10991208255290985\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.012837642803788185\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.11984530836343765\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 9.225759045605741e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.31622835993766785\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.08446768671274185\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.20043060183525085\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.014176879078149796\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.19093890488147736\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.289966133910525e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.330119252204895\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.08244145661592484\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.09630588442087173\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.01004049088805914\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.09735113382339478\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.1911185371360489e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.32705676555633545\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07976825535297394\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.08797983080148697\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.011405814439058304\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.08725476264953613\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 9.169469628034221e-10\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3187781572341919\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.0806681215763092\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.1258259266614914\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01417538058012724\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.1352895200252533\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.862796139562306e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.31590744853019714\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08211474120616913\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.09236329048871994\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.008485628291964531\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.0959131196141243\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.3439551693750218e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.31720811128616333\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.08323760330677032\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.483149528503418\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.39096587896347046\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.045197706669569016\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.05813716724514961\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.351051390171051\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.045071080327034\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3524893522262573\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.12439180165529251\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.8963534235954285\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09396086633205414\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.16421684622764587\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.22855913639068604\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.07109913229942322\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.005589890759438276\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.06903482973575592\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 8.202310497473775e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.28032925724983215\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05728438124060631\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.06832150369882584\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006521505769342184\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.07197936624288559\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 9.528319244722638e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.2918335199356079\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06119633838534355\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07380428910255432\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006516468711197376\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07241027057170868\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 5.577942552292825e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.33386871218681335\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06564439088106155\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.07778549939393997\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007638280745595694\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.07008523494005203\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 6.742080227439828e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.2884455919265747\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06069700047373772\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.06999559700489044\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.008041825145483017\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.06301601976156235\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 5.991217522094416e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.27256065607070923\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.056523293256759644\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08778654038906097\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.008783061988651752\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09212972968816757\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.0237506398169671e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2872941195964813\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06387558579444885\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.06851785629987717\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.00731437373906374\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07254635542631149\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.046494499007849e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.2743624150753021\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.0640595331788063\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.08255886286497116\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.008467267267405987\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07835077494382858\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.744153734419058e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.28496384620666504\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.062207527458667755\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.434054374694824\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3040187656879425\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.043173614889383316\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05489106476306915\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3352035880088806\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.036768753081560135\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3273662328720093\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11658693104982376\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.8637242317199707\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.21062752604484558\n",
      "ab_loss: 23.563905715942383\n",
      "Gradient of AB Loss for row 0: tensor([-0.0258,  0.0209, -0.0162, -0.0292, -0.0244, -0.0377, -0.0190, -0.0194,\n",
      "        -0.0208, -0.0276, -0.0247, -0.0215, -0.0114, -0.0175, -0.0103, -0.0201,\n",
      "        -0.0150,  0.0253, -0.0231, -0.0231, -0.0237])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0316, -0.0270,  0.0190, -0.0296,  0.0377, -0.0488,  0.0400,  0.0464,\n",
      "         0.0270,  0.0331,  0.0392,  0.0310, -0.0348, -0.0272])\n",
      "Gradient of AB Loss for row 2: tensor([0.0342, 0.0469, 0.0204, 0.0285, 0.0516, 0.0408, 0.0374, 0.0350, 0.0510,\n",
      "        0.0451, 0.0433, 0.0378, 0.0279, 0.0358])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0270, -0.0309,  0.0376,  0.0240,  0.0301, -0.0315,  0.0341,  0.0355,\n",
      "         0.0355,  0.0474,  0.0237, -0.0326, -0.0326,  0.0284,  0.0189])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0357, -0.0503, -0.0401, -0.0160, -0.0552,  0.0217, -0.0357, -0.0276,\n",
      "        -0.0352,  0.0281,  0.0340, -0.0310,  0.0366])\n",
      "Gradient of AB Loss for row 5: tensor([ 0.0370, -0.0356,  0.0505,  0.0208,  0.0400, -0.0393,  0.0269,  0.0281,\n",
      "         0.0241,  0.0346, -0.0456,  0.0245, -0.0342,  0.0214,  0.0192])\n",
      "Gradient of AB Loss for row 6: tensor([-0.5011])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0317, -0.0381, -0.0419, -0.0292, -0.0493,  0.0378, -0.0243,  0.0233,\n",
      "         0.0428,  0.0333,  0.0519,  0.0358,  0.0246,  0.0228, -0.0221])\n",
      "Gradient of AB Loss for row 8: tensor([0.1434, 0.1464, 0.1473, 0.1346])\n",
      "Gradient of AB Loss for row 9: tensor([-0.4937])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0412,  0.0288, -0.0362,  0.0472,  0.0290,  0.0250,  0.0378,  0.0354,\n",
      "         0.0452,  0.0366,  0.0397,  0.0320,  0.0381,  0.0342])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0248,  0.0325, -0.0284,  0.0431,  0.0292,  0.0291, -0.0357,  0.0414,\n",
      "         0.0376,  0.0386,  0.0347, -0.0412, -0.0389,  0.0353,  0.0381])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0317, -0.0365, -0.0311,  0.0235,  0.0318, -0.0283,  0.0223,  0.0310,\n",
      "         0.0393,  0.0404,  0.0386,  0.0395, -0.0378, -0.0284, -0.0280])\n",
      "Gradient of AB Loss for row 13: tensor([0.0263, 0.0485, 0.0262, 0.0287, 0.0336, 0.0249, 0.0244, 0.0219, 0.0444,\n",
      "        0.0338, 0.0393, 0.0240, 0.0362, 0.0477])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0498, -0.0382,  0.0456,  0.0147,  0.0341,  0.0488,  0.0378,  0.0312,\n",
      "         0.0516,  0.0358,  0.0347,  0.0393, -0.0375,  0.0308])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0267, -0.0357, -0.0299, -0.0370,  0.0456,  0.0323,  0.0345,  0.0129,\n",
      "        -0.0243,  0.0316,  0.0427,  0.0430,  0.0381,  0.0275,  0.0220, -0.0305,\n",
      "         0.0236])\n",
      "Gradient of AB Loss for row 16: tensor([0.0377, 0.0489, 0.0206, 0.0298, 0.0406, 0.0326, 0.0314, 0.0431, 0.0451,\n",
      "        0.0391, 0.0339, 0.0471, 0.0287, 0.0351])\n",
      "Gradient of AB Loss for row 17: tensor([0.0368, 0.0463, 0.0262, 0.0237, 0.0487, 0.0403, 0.0485, 0.0458, 0.0400,\n",
      "        0.0351, 0.0366, 0.0342, 0.0357, 0.0395])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0478,  0.0513,  0.0146,  0.0230,  0.0374,  0.0425,  0.0381,  0.0456,\n",
      "         0.0433,  0.0409,  0.0263,  0.0371, -0.0278,  0.0341])\n",
      "Gradient of AB Loss for row 19: tensor([0.0406, 0.0493, 0.0155, 0.0261, 0.0346, 0.0354, 0.0204, 0.0460, 0.0558,\n",
      "        0.0480, 0.0441, 0.0264, 0.0313, 0.0345])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0632, -0.1147, -0.1612,  0.1622])\n",
      "Gradient of AB Loss for row 21: tensor([0.0335, 0.0454, 0.0167, 0.0257, 0.0270, 0.0382, 0.0475, 0.0317, 0.0490,\n",
      "        0.0577, 0.0346, 0.0196, 0.0348, 0.0410])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0369,  0.0522,  0.0195,  0.0325, -0.0517,  0.0354,  0.0366,  0.0471,\n",
      "         0.0486, -0.0243,  0.0334, -0.0202, -0.0288,  0.0258,  0.0356])\n",
      "Gradient of AB Loss for row 23: tensor([0.0320, 0.0459, 0.0158, 0.0255, 0.0454, 0.0335, 0.0430, 0.0334, 0.0481,\n",
      "        0.0509, 0.0418, 0.0221, 0.0262, 0.0360])\n",
      "Gradient of AB Loss for row 24: tensor([ 0.0403, -0.0420,  0.0557,  0.0336,  0.0238,  0.0379,  0.0272,  0.0366,\n",
      "         0.0388,  0.0456,  0.0379,  0.0281,  0.0279])\n",
      "Gradient of AB Loss for row 25: tensor([0.0393, 0.0518, 0.0188, 0.0327, 0.0442, 0.0365, 0.0336, 0.0467, 0.0343,\n",
      "        0.0407, 0.0406, 0.0288, 0.0219, 0.0333])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0210,  0.0184, -0.0148, -0.0279, -0.0255, -0.0356, -0.0270, -0.0206,\n",
      "        -0.0278, -0.0328, -0.0216, -0.0233, -0.0258, -0.0216, -0.0155, -0.0174,\n",
      "        -0.0222, -0.0278, -0.0317, -0.0226])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0410,  0.0375,  0.0180,  0.0380,  0.0363,  0.0336,  0.0286,  0.0421,\n",
      "         0.0399,  0.0389,  0.0249,  0.0323, -0.0342,  0.0252])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0314, -0.0402,  0.0426,  0.0148,  0.0326, -0.0341,  0.0322,  0.0344,\n",
      "         0.0250,  0.0449,  0.0400, -0.0297, -0.0144,  0.0302,  0.0287])\n",
      "Gradient of AB Loss for row 29: tensor([0.1669, 0.1882, 0.1164, 0.1229])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0343, -0.0153, -0.0288, -0.0546, -0.0219,  0.0418, -0.0290,  0.0356,\n",
      "         0.0492,  0.0302,  0.0459,  0.0311, -0.0319, -0.0372, -0.0224])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0316, -0.0320, -0.0355, -0.0348,  0.0433,  0.0349,  0.0358,  0.0188,\n",
      "        -0.0190,  0.0307,  0.0377,  0.0371,  0.0425,  0.0308,  0.0316,  0.0330,\n",
      "         0.0145])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08914275467395782\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.08852751553058624\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.17731943726539612\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9759565591812134\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.1110566183924675\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.17372754216194153\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.26903989911079407\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.14196014404296875\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.015462168492376804\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.15884210169315338\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.1770484587003693e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.29487401247024536\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.0928615927696228\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.11007252335548401\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.009993120096623898\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.11940926313400269\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 7.590429396131526e-10\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.31417736411094666\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.09293580055236816\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1112041026353836\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.011205215007066727\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.11318091303110123\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.2363109425095331e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.32813912630081177\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09162755310535431\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.10242149233818054\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.011789203621447086\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.09759809076786041\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 9.516546439769513e-10\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.32900381088256836\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.087466299533844\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.09519641101360321\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.01203056052327156\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.09447325766086578\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 9.538362322203398e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3562195301055908\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08865530043840408\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.10066963732242584\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.011060132645070553\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.098424531519413\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.7265077145722785e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3182542324066162\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08546117693185806\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.09041754901409149\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.009135316126048565\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.09589875489473343\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 9.124367927881849e-10\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3029300570487976\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.0779283195734024\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.1385832279920578\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.01166402455419302\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.1390802264213562\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.3575627288986425e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.357584685087204\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.09039153158664703\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.7040185928344727\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.43903595209121704\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.0472869835793972\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06268025189638138\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3650839924812317\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04627852886915207\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3712122142314911\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.14104396104812622\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9590136408805847\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09792818874120712\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1730598509311676\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.22012311220169067\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06638279557228088\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.005919851828366518\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.069711834192276\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.4842241879975404e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.26964765787124634\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05635331943631172\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.08188817650079727\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.00972430408000946\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.08470842987298965\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 8.590366751271006e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.29174184799194336\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06247025355696678\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.06696026027202606\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006182336714118719\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06706941872835159\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 8.131364470642666e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.32732221484184265\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06424075365066528\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.06018988415598869\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.005841398146003485\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.05821368843317032\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.163399709902535e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.2870826721191406\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06170107424259186\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.06193603202700615\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.0057733976282179356\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.0606662817299366\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 8.636630854930161e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3102819323539734\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06650326400995255\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.06679349392652512\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.005902809556573629\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.07074946165084839\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 8.468508116976636e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2754586637020111\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06136464700102806\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.07958441227674484\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.008464928716421127\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07926841080188751\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.2774782343072388e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.2756965160369873\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.058926451951265335\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07006977498531342\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.00741229671984911\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.06724133342504501\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 9.454030891475895e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2707814574241638\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.05674711987376213\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.4633371829986572\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.30542808771133423\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04967948794364929\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.0535874180495739\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3563547432422638\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.0393991656601429\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3303162753582001\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11048228293657303\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.788142204284668\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2047056257724762\n",
      "ab_loss: 23.828487396240234\n",
      "Gradient of AB Loss for row 0: tensor([-0.0181, -0.0191, -0.0263,  0.0181, -0.0131, -0.0207, -0.0203, -0.0215,\n",
      "         0.0278, -0.0201, -0.0265, -0.0163,  0.0184, -0.0247, -0.0121,  0.0251,\n",
      "        -0.0146, -0.0146, -0.0125,  0.0227, -0.0202,  0.0195])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0452, -0.0613, -0.0356,  0.0415, -0.0509,  0.0466,  0.0563,  0.0460,\n",
      "         0.0557,  0.0341, -0.0481, -0.0387,  0.0320])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0256, -0.0328, -0.0226, -0.0359,  0.0361,  0.0365,  0.0165,  0.0208,\n",
      "         0.0437,  0.0462,  0.0356,  0.0431,  0.0237, -0.0229, -0.0317,  0.0253])\n",
      "Gradient of AB Loss for row 3: tensor([ 0.0368, -0.0441,  0.0518,  0.0135,  0.0470,  0.0384,  0.0396,  0.0430,\n",
      "         0.0294,  0.0517,  0.0431,  0.0410,  0.0287,  0.0376])\n",
      "Gradient of AB Loss for row 4: tensor([-0.5797])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0330, -0.0160, -0.0185,  0.0318, -0.0220,  0.0218,  0.0303, -0.0286,\n",
      "        -0.0270, -0.0296,  0.0237,  0.0085,  0.0164,  0.0271, -0.0218,  0.0125,\n",
      "         0.0243, -0.0159,  0.0173,  0.0211, -0.0262, -0.0216])\n",
      "Gradient of AB Loss for row 6: tensor([0.1054, 0.1629, 0.1303, 0.1272])\n",
      "Gradient of AB Loss for row 7: tensor([-0.6488])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0140, -0.0206, -0.0257,  0.0178, -0.0134, -0.0182, -0.0371, -0.0294,\n",
      "        -0.0202, -0.0208, -0.0201, -0.0187, -0.0193, -0.0118, -0.0203, -0.0195,\n",
      "        -0.0102, -0.0132, -0.0093,  0.0206, -0.0267, -0.0231, -0.0241])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0252, -0.0427,  0.0289,  0.0139, -0.0193,  0.0383,  0.0436,  0.0322,\n",
      "         0.0289,  0.0394,  0.0292,  0.0375,  0.0343, -0.0229,  0.0210, -0.0291])\n",
      "Gradient of AB Loss for row 10: tensor([-0.5061])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0305, -0.0398, -0.0453, -0.0262, -0.0361,  0.0266, -0.0302,  0.0396,\n",
      "         0.0397,  0.0350,  0.0418,  0.0214,  0.0356,  0.0243, -0.0363,  0.0237,\n",
      "        -0.0231])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0417, -0.0207, -0.0450, -0.0288, -0.0361,  0.0253, -0.0340,  0.0301,\n",
      "         0.0411,  0.0324,  0.0342, -0.0344, -0.0184, -0.0289, -0.0268])\n",
      "Gradient of AB Loss for row 13: tensor([-0.6338])\n",
      "Gradient of AB Loss for row 14: tensor([ 0.0124, -0.0273, -0.0246,  0.0277,  0.0353,  0.0243, -0.0189,  0.0356,\n",
      "         0.0264,  0.0161,  0.0342,  0.0245,  0.0213,  0.0277, -0.0154,  0.0278,\n",
      "         0.0174,  0.0320, -0.0360,  0.0211,  0.0281])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0325, -0.0585, -0.0388,  0.0381,  0.0254, -0.0355,  0.0259,  0.0305,\n",
      "         0.0383,  0.0310,  0.0347, -0.0369, -0.0386,  0.0314,  0.0459])\n",
      "Gradient of AB Loss for row 16: tensor([0.0420, 0.0417, 0.0314, 0.0291, 0.0387, 0.0474, 0.0440, 0.0431, 0.0448,\n",
      "        0.0369, 0.0307, 0.0377, 0.0372, 0.0269])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0257, -0.0202, -0.0358, -0.0419, -0.0239, -0.0341,  0.0358, -0.0244,\n",
      "         0.0234,  0.0380,  0.0320,  0.0314, -0.0394, -0.0351, -0.0216, -0.0267,\n",
      "        -0.0141])\n",
      "Gradient of AB Loss for row 18: tensor([0.0308, 0.0411, 0.0116, 0.0351, 0.0483, 0.0434, 0.0331, 0.0393, 0.0388,\n",
      "        0.0369, 0.0312, 0.0328, 0.0291, 0.0431])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0200,  0.0160, -0.0281,  0.0231,  0.0176, -0.0172, -0.0109,  0.0190,\n",
      "        -0.0194, -0.0117, -0.0247,  0.0221,  0.0162, -0.0227, -0.0244,  0.0216,\n",
      "         0.0120,  0.0246, -0.0196,  0.0328,  0.0243,  0.0243, -0.0223,  0.0203])\n",
      "Gradient of AB Loss for row 20: tensor([0.1311, 0.1135, 0.1012, 0.1624])\n",
      "Gradient of AB Loss for row 21: tensor([ 0.0303, -0.0407,  0.0497,  0.0237,  0.0347, -0.0479,  0.0290,  0.0256,\n",
      "         0.0352,  0.0445, -0.0276,  0.0265, -0.0251,  0.0269,  0.0216])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0322, -0.0194,  0.0366,  0.0273,  0.0174,  0.0213,  0.0398,  0.0343,\n",
      "         0.0232,  0.0428,  0.0351,  0.0307,  0.0336,  0.0319,  0.0249, -0.0281,\n",
      "         0.0245])\n",
      "Gradient of AB Loss for row 23: tensor([-0.2502])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0303, -0.0388, -0.0492, -0.0477,  0.0345,  0.0236,  0.0526,  0.0355,\n",
      "         0.0308, -0.0302,  0.0306, -0.0308,  0.0331, -0.0343, -0.0152])\n",
      "Gradient of AB Loss for row 25: tensor([0.1365, 0.1319, 0.1193, 0.1261])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0228, -0.0219,  0.0262, -0.0318, -0.0382, -0.0397, -0.0257,  0.0218,\n",
      "         0.0382, -0.0203, -0.0298,  0.0218,  0.0287, -0.0245,  0.0274, -0.0184,\n",
      "         0.0246, -0.0290, -0.0232])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0196, -0.0366, -0.0189, -0.0195, -0.0145, -0.0280, -0.0213,  0.0329,\n",
      "        -0.0291, -0.0246,  0.0183, -0.0238, -0.0197,  0.0160, -0.0178, -0.0243,\n",
      "         0.0274, -0.0335, -0.0250])\n",
      "Gradient of AB Loss for row 28: tensor([0.1489, 0.1353, 0.1208, 0.1337])\n",
      "Gradient of AB Loss for row 29: tensor([0.0314, 0.0448, 0.0265, 0.0287, 0.0336, 0.0250, 0.0267, 0.0424, 0.0297,\n",
      "        0.0354, 0.0277, 0.0336, 0.0402])\n",
      "Gradient of AB Loss for row 30: tensor([0.0458, 0.0516, 0.0320, 0.0361, 0.0382, 0.0287, 0.0459, 0.0434, 0.0434,\n",
      "        0.0347, 0.0382, 0.0291, 0.0239, 0.0212])\n",
      "Gradient of AB Loss for row 31: tensor([0.0509, 0.0507, 0.0303, 0.0363, 0.0418, 0.0385, 0.0423, 0.0352, 0.0277,\n",
      "        0.0216, 0.0387, 0.0351, 0.0284, 0.0324])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10552851855754852\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.10571899265050888\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.18874064087867737\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0970420837402344\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.12144234031438828\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.20839758217334747\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.29282811284065247\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1686372309923172\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.017620239406824112\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.19345588982105255\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 2.2446959846433856e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.360616534948349\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.10380343347787857\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.14089295268058777\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.017333313822746277\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.15301856398582458\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.0795530025475841e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.36228328943252563\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.1045793890953064\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1515965610742569\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.019421596080064774\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.1589151918888092\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.3515378816109092e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.37150025367736816\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09978194534778595\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.17246907949447632\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.013806399889290333\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.1743149608373642\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.6866570362594757e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3944731056690216\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.09943966567516327\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11664001643657684\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.009616382420063019\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.11415890604257584\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.6000575309149667e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.37487778067588806\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.09292345494031906\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.15996292233467102\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.01465800404548645\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.16152323782444\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 2.174125768306112e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3685546815395355\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09479890018701553\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.17647850513458252\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01565193384885788\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.17878736555576324\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.4979351092847537e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.37838438153266907\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.10216745734214783\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.14595775306224823\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.011895086616277695\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.14392481744289398\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 2.21787876952817e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.39294904470443726\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.09508519619703293\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 3.0680131912231445\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.46892186999320984\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.0554061196744442\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06781375408172607\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.41515475511550903\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04810633510351181\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.43095532059669495\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.14335528016090393\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0883361101150513\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.11067003756761551\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.20235009491443634\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.24984709918498993\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.11135223507881165\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.01006871834397316\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.11008352786302567\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.0109302284178057e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.3625895380973816\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.0778781846165657\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09937676042318344\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.009706859476864338\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.10766595602035522\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 9.81573156089155e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.34243297576904297\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07220528274774551\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.0944933220744133\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.009680039249360561\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08911100029945374\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.6716871220623375e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3690028786659241\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.07389037311077118\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.11463242769241333\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.011819787323474884\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.11811443418264389\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 7.935725410135319e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.35199323296546936\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.0758313238620758\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.0722004845738411\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.005773776676505804\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.06996272504329681\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.6049120921124427e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3029235601425171\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.058726802468299866\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.1248566135764122\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.0127555588260293\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.12253539264202118\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.0232744651617054e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.34010350704193115\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07915166765451431\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.09690369665622711\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007447526790201664\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.0946698784828186\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.0092761071334166e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3297767639160156\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07441429048776627\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.09585516154766083\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.010718327015638351\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.09205750375986099\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.4519687674408033e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.327523797750473\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.07013639062643051\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.917957067489624\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3710004389286041\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04968446493148804\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06428278237581253\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.40240418910980225\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.04137754440307617\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.43330565094947815\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.13526928424835205\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.2300140857696533\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2403082698583603\n",
      "ab_loss: 24.304351806640625\n",
      "Gradient of AB Loss for row 0: tensor([-0.0627,  0.0625,  0.0411,  0.0382,  0.0417,  0.0531,  0.0505,  0.0445,\n",
      "         0.0329,  0.0416, -0.0317,  0.0355])\n",
      "Gradient of AB Loss for row 1: tensor([0.1377, 0.1698, 0.1327, 0.1335])\n",
      "Gradient of AB Loss for row 2: tensor([-0.6687])\n",
      "Gradient of AB Loss for row 3: tensor([-0.6295])\n",
      "Gradient of AB Loss for row 4: tensor([0.0378, 0.0521, 0.0233, 0.0229, 0.0275, 0.0501, 0.0426, 0.0502, 0.0445,\n",
      "        0.0175, 0.0395, 0.0395, 0.0226, 0.0196])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0248, -0.0292, -0.0304, -0.0319,  0.0308,  0.0196,  0.0273,  0.0234,\n",
      "        -0.0311,  0.0222,  0.0399,  0.0314,  0.0329,  0.0260,  0.0248, -0.0382,\n",
      "        -0.0314,  0.0208])\n",
      "Gradient of AB Loss for row 6: tensor([0.0388, 0.0391, 0.0208, 0.0333, 0.0366, 0.0312, 0.0363, 0.0505, 0.0570,\n",
      "        0.0424, 0.0329, 0.0322, 0.0285, 0.0337])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0484,  0.0373,  0.0216,  0.0348, -0.0385,  0.0461,  0.0353,  0.0397,\n",
      "         0.0545,  0.0437,  0.0227, -0.0352, -0.0257,  0.0318])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0226,  0.0338, -0.0379,  0.0418,  0.0233,  0.0326,  0.0350,  0.0341,\n",
      "         0.0373,  0.0551,  0.0237,  0.0248,  0.0291,  0.0313])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0203, -0.0259, -0.0187, -0.0304, -0.0362, -0.0205,  0.0222, -0.0316,\n",
      "        -0.0233, -0.0298,  0.0342,  0.0257, -0.0223,  0.0116,  0.0319, -0.0200,\n",
      "        -0.0184, -0.0151, -0.0133,  0.0248, -0.0203, -0.0292])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0262, -0.0289, -0.0355, -0.0445, -0.0244, -0.0378,  0.0443,  0.0250,\n",
      "        -0.0233,  0.0349,  0.0444,  0.0284,  0.0313, -0.0337, -0.0248,  0.0273])\n",
      "Gradient of AB Loss for row 11: tensor([0.0431, 0.0455, 0.0371, 0.0288, 0.0435, 0.0346, 0.0451, 0.0310, 0.0559,\n",
      "        0.0453, 0.0422, 0.0432, 0.0221, 0.0323])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0391,  0.0510,  0.0229,  0.0345, -0.0323,  0.0434,  0.0381,  0.0484,\n",
      "         0.0381,  0.0341,  0.0377,  0.0339, -0.0339,  0.0331])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0485,  0.0273, -0.0279,  0.0325,  0.0418,  0.0357,  0.0386,  0.0386,\n",
      "         0.0455,  0.0313,  0.0226,  0.0311,  0.0270,  0.0220])\n",
      "Gradient of AB Loss for row 14: tensor([0.0345, 0.0306, 0.0330, 0.0319, 0.0401, 0.0432, 0.0444, 0.0350, 0.0517,\n",
      "        0.0207, 0.0301, 0.0289, 0.0301, 0.0318])\n",
      "Gradient of AB Loss for row 15: tensor([0.1562, 0.1537, 0.1385])\n",
      "Gradient of AB Loss for row 16: tensor([-0.1553,  0.1858, -0.1385])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0212, -0.0394,  0.0388,  0.0332,  0.0371,  0.0132, -0.0350,  0.0276,\n",
      "         0.0442,  0.0403,  0.0378,  0.0344, -0.0327,  0.0297,  0.0147,  0.0215])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0225, -0.0418, -0.0401,  0.0279,  0.0116, -0.0175,  0.0421,  0.0336,\n",
      "         0.0380,  0.0371,  0.0371,  0.0310,  0.0183, -0.0269,  0.0248, -0.0232])\n",
      "Gradient of AB Loss for row 19: tensor([ 0.1345,  0.1324, -0.1454,  0.1165])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0451,  0.0490,  0.0307,  0.0412, -0.0413,  0.0465,  0.0422,  0.0390,\n",
      "         0.0524,  0.0449,  0.0253,  0.0269, -0.0213,  0.0296])\n",
      "Gradient of AB Loss for row 21: tensor([0.0347, 0.0523, 0.0320, 0.0341, 0.0522, 0.0357, 0.0361, 0.0380, 0.0457,\n",
      "        0.0445, 0.0390, 0.0428, 0.0309, 0.0428])\n",
      "Gradient of AB Loss for row 22: tensor([-0.1131, -0.0809, -0.1130, -0.1733])\n",
      "Gradient of AB Loss for row 23: tensor([0.0351, 0.0452, 0.0357, 0.0265, 0.0382, 0.0253, 0.0342, 0.0341, 0.0510,\n",
      "        0.0329, 0.0312, 0.0403, 0.0287, 0.0191])\n",
      "Gradient of AB Loss for row 24: tensor([0.0177, 0.0253, 0.0292, 0.0232, 0.0385, 0.0441, 0.0385, 0.0424, 0.0471,\n",
      "        0.0314, 0.0421, 0.0451, 0.0355, 0.0268, 0.0293])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0388, -0.0286,  0.0410,  0.0207,  0.0319, -0.0472,  0.0344,  0.0397,\n",
      "         0.0250,  0.0433,  0.0387, -0.0446, -0.0119,  0.0391,  0.0390])\n",
      "Gradient of AB Loss for row 26: tensor([0.0432, 0.0535, 0.0234, 0.0272, 0.0428, 0.0333, 0.0294, 0.0429, 0.0458,\n",
      "        0.0388, 0.0388, 0.0314, 0.0227, 0.0276])\n",
      "Gradient of AB Loss for row 27: tensor([ 0.1580,  0.0859, -0.1355,  0.1375])\n",
      "Gradient of AB Loss for row 28: tensor([0.0252, 0.0403, 0.0259, 0.0299, 0.0488, 0.0345, 0.0227, 0.0305, 0.0404,\n",
      "        0.0293, 0.0241, 0.0308, 0.0211, 0.0387])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0499, -0.0440, -0.0432,  0.0451, -0.0439,  0.0336,  0.0519,  0.0522,\n",
      "         0.0572,  0.0514, -0.0429, -0.0466,  0.0300])\n",
      "Gradient of AB Loss for row 30: tensor([-0.1364, -0.1225, -0.1142, -0.1523])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0274,  0.0332, -0.0313,  0.0439,  0.0325,  0.0213,  0.0301,  0.0433,\n",
      "         0.0431,  0.0390,  0.0266,  0.0408,  0.0199,  0.0219])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10301899909973145\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09233152866363525\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.19299538433551788\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9961928725242615\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11617108434438705\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.19260211288928986\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2836231291294098\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1675572693347931\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.015267550013959408\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1745736449956894\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.1209967398784215e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3078318238258362\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.09619521349668503\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.13513097167015076\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.016824927181005478\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.1414962112903595\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.2644083557944441e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.31598782539367676\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.09714357554912567\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.14284875988960266\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.016314636915922165\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.18166205286979675\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.0335832190122574e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3343406617641449\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.10169960558414459\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.1805933266878128\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.01607927866280079\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.1842217743396759\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.0615850420947481e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.36357393860816956\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.10809728503227234\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11527883261442184\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.009692329913377762\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.12243686616420746\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.5282095588986522e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3619486391544342\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.09635014086961746\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.20753006637096405\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.01960018090903759\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.21378658711910248\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 9.825925628703658e-10\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.35749998688697815\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.10324455052614212\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.13608090579509735\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.016421305015683174\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.14353494346141815\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 9.11584197016424e-10\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3406180739402771\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.09826483577489853\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.21144603192806244\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.0191972553730011\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.2195461243391037\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.2598571075272957e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.345632404088974\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.09185342490673065\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.785973310470581\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.462944358587265\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04285891726613045\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06711013615131378\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.38048937916755676\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04718711972236633\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.4117893874645233\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1462699919939041\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.012783408164978\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.10869711637496948\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.18796364963054657\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2506210505962372\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06375811994075775\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.005087509751319885\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.06393100321292877\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.0986240805976877e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.30135345458984375\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06700367480516434\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.08984898030757904\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.008880994282662868\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09242954105138779\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.2371612623240935e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.33911800384521484\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07593842595815659\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07863674312829971\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.00674955639988184\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07241401076316833\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 9.39371469499406e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.36540335416793823\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.07304699718952179\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.08127494901418686\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007190408650785685\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.07853797823190689\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 8.088767433633848e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3155921697616577\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07077206671237946\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.0675741657614708\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.005903943907469511\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.06616899371147156\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.5494113769776163e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2927675247192383\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06354139000177383\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08209247887134552\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.007426390890032053\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09122970700263977\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.5042496137596117e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.31007835268974304\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07224247604608536\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.0814870297908783\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.006801482290029526\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08348368853330612\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 5.799823954433236e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.29259011149406433\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06916748732328415\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07649547606706619\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.0068884678184986115\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.06955897808074951\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 7.355456244084735e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.29994726181030273\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06746240705251694\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.6793785095214844\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.35527390241622925\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.051064468920230865\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06227937713265419\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.36481499671936035\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03637932986021042\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.4109345078468323\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.13031959533691406\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.059126138687134\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.227873295545578\n",
      "ab_loss: 24.2321720123291\n",
      "Gradient of AB Loss for row 0: tensor([-0.0358, -0.0244, -0.0353, -0.0439,  0.0398,  0.0146, -0.0256, -0.0260,\n",
      "         0.0296,  0.0422,  0.0365,  0.0407, -0.0305, -0.0268, -0.0217, -0.0224,\n",
      "        -0.0100])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0292, -0.0237, -0.0212, -0.0292, -0.0172,  0.0238,  0.0281, -0.0287,\n",
      "        -0.0231, -0.0262,  0.0283,  0.0115,  0.0259,  0.0230, -0.0162,  0.0343,\n",
      "        -0.0130, -0.0170,  0.0232,  0.0303, -0.0265, -0.0278])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0262, -0.0566,  0.0427,  0.0481,  0.0247,  0.0409,  0.0393,  0.0358,\n",
      "         0.0480,  0.0385,  0.0434,  0.0404,  0.0295,  0.0396])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0210, -0.0297, -0.0298, -0.0420, -0.0321, -0.0389,  0.0475,  0.0245,\n",
      "        -0.0261,  0.0290,  0.0411,  0.0236,  0.0252, -0.0334, -0.0382, -0.0245])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0348,  0.0295,  0.0272,  0.0146,  0.0290,  0.0370,  0.0367,  0.0382,\n",
      "         0.0333,  0.0345,  0.0369,  0.0255, -0.0338,  0.0295, -0.0240,  0.0202,\n",
      "         0.0246])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0450, -0.0389,  0.0528,  0.0152,  0.0359, -0.0327,  0.0382,  0.0384,\n",
      "         0.0266,  0.0380,  0.0333, -0.0397, -0.0288,  0.0282,  0.0258])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0402, -0.0472, -0.0378, -0.0217,  0.0443, -0.0435,  0.0422,  0.0510,\n",
      "         0.0442,  0.0330,  0.0379, -0.0364, -0.0201,  0.0293])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0245, -0.0288, -0.0175, -0.0129, -0.0267, -0.0348, -0.0255, -0.0209,\n",
      "        -0.0107, -0.0233, -0.0173, -0.0237, -0.0200, -0.0253, -0.0312, -0.0190,\n",
      "        -0.0224, -0.0236, -0.0242,  0.0220, -0.0273, -0.0273, -0.0246])\n",
      "Gradient of AB Loss for row 8: tensor([ 0.0278, -0.0477,  0.0594,  0.0445, -0.0512,  0.0343,  0.0385,  0.0409,\n",
      "         0.0544,  0.0405,  0.0456, -0.0512,  0.0336])\n",
      "Gradient of AB Loss for row 9: tensor([0.0380, 0.0579, 0.0125, 0.0332, 0.0449, 0.0309, 0.0423, 0.0325, 0.0397,\n",
      "        0.0566, 0.0376, 0.0275, 0.0336])\n",
      "Gradient of AB Loss for row 10: tensor([0.0310, 0.0550, 0.0234, 0.0274, 0.0445, 0.0300, 0.0271, 0.0331, 0.0553,\n",
      "        0.0559, 0.0470, 0.0286, 0.0315, 0.0250])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0359, -0.0267, -0.0250,  0.0423,  0.0348,  0.0149, -0.0257,  0.0257,\n",
      "         0.0407,  0.0373,  0.0383,  0.0326,  0.0353,  0.0271, -0.0259,  0.0171,\n",
      "        -0.0148,  0.0245])\n",
      "Gradient of AB Loss for row 12: tensor([ 0.0289, -0.0329,  0.0510,  0.0352,  0.0352,  0.0390,  0.0357,  0.0157,\n",
      "         0.0321,  0.0368,  0.0342,  0.0348,  0.0319,  0.0278])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0286,  0.0404,  0.0235,  0.0487,  0.0377,  0.0360,  0.0259,  0.0487,\n",
      "         0.0446,  0.0357,  0.0336, -0.0307, -0.0276,  0.0304])\n",
      "Gradient of AB Loss for row 14: tensor([0.0419, 0.0491, 0.0160, 0.0189, 0.0483, 0.0232, 0.0382, 0.0498, 0.0385,\n",
      "        0.0425, 0.0400, 0.0310, 0.0322, 0.0468])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0781, -0.1306, -0.1212, -0.1675])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0420, -0.0651, -0.0480,  0.0277,  0.0445,  0.0366,  0.0523,  0.0489,\n",
      "         0.0480,  0.0380,  0.0364, -0.0416,  0.0347])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0431,  0.0313, -0.0312,  0.0472,  0.0096, -0.0451,  0.0379, -0.0263,\n",
      "         0.0317,  0.0385, -0.0376, -0.0359, -0.0198,  0.0266,  0.0266])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0555, -0.0403,  0.0439,  0.0307, -0.0389,  0.0433,  0.0447,  0.0491,\n",
      "         0.0573,  0.0343,  0.0467,  0.0210, -0.0452,  0.0135])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0290,  0.0368, -0.0320,  0.0411,  0.0265,  0.0115,  0.0400,  0.0288,\n",
      "         0.0386,  0.0358,  0.0281,  0.0161,  0.0209, -0.0409, -0.0269,  0.0191,\n",
      "         0.0270])\n",
      "Gradient of AB Loss for row 20: tensor([0.0408, 0.0318, 0.0271, 0.0254, 0.0356, 0.0281, 0.0331, 0.0277, 0.0531,\n",
      "        0.0179, 0.0388, 0.0363, 0.0300, 0.0179])\n",
      "Gradient of AB Loss for row 21: tensor([0.0254, 0.0368, 0.0239, 0.0185, 0.0371, 0.0480, 0.0432, 0.0493, 0.0409,\n",
      "        0.0517, 0.0373, 0.0247, 0.0236, 0.0415])\n",
      "Gradient of AB Loss for row 22: tensor([-0.4233])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0273, -0.0297, -0.0365, -0.0260,  0.0362,  0.0311,  0.0137, -0.0219,\n",
      "         0.0171,  0.0401,  0.0329,  0.0315, -0.0331,  0.0263, -0.0365, -0.0161,\n",
      "         0.0099])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0247, -0.0381, -0.0466, -0.0395, -0.0402, -0.0441,  0.0280, -0.0205,\n",
      "         0.0256,  0.0552,  0.0433,  0.0310, -0.0435, -0.0441, -0.0119])\n",
      "Gradient of AB Loss for row 25: tensor([0.0366, 0.0522, 0.0179, 0.0261, 0.0466, 0.0357, 0.0283, 0.0377, 0.0468,\n",
      "        0.0491, 0.0485, 0.0477, 0.0392])\n",
      "Gradient of AB Loss for row 26: tensor([0.0322, 0.0333, 0.0257, 0.0418, 0.0461, 0.0372, 0.0503, 0.0388, 0.0411,\n",
      "        0.0303, 0.0361, 0.0354, 0.0258, 0.0408])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0288, -0.0294, -0.0231, -0.0372,  0.0320,  0.0292,  0.0329,  0.0172,\n",
      "        -0.0231,  0.0255,  0.0442,  0.0416,  0.0303,  0.0309, -0.0231,  0.0256,\n",
      "         0.0165])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0445,  0.0440, -0.0238,  0.0140,  0.0548, -0.0468,  0.0339,  0.0435,\n",
      "         0.0453,  0.0532,  0.0370,  0.0249,  0.0231])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0353, -0.0477,  0.0415,  0.0254,  0.0480,  0.0369,  0.0342,  0.0612,\n",
      "         0.0476,  0.0480,  0.0370, -0.0414,  0.0340])\n",
      "Gradient of AB Loss for row 30: tensor([0.0333, 0.0353, 0.0223, 0.0258, 0.0489, 0.0380, 0.0255, 0.0426, 0.0508,\n",
      "        0.0488, 0.0356, 0.0404, 0.0357, 0.0335])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0300, -0.0312, -0.0246,  0.0381,  0.0393,  0.0304,  0.0106, -0.0296,\n",
      "         0.0399,  0.0384,  0.0399,  0.0392,  0.0292,  0.0297,  0.0241, -0.0284,\n",
      "         0.0235,  0.0221])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10772518068552017\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.1108875498175621\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.19292695820331573\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0645928382873535\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.12536460161209106\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.19915920495986938\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.3105772137641907\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.11924880743026733\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.01581423357129097\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.13546085357666016\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.1835392665915379e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.33037078380584717\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.0888536274433136\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.11857900023460388\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.01027626171708107\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.12948662042617798\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.1420088208424772e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.34018123149871826\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08988482505083084\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.16075369715690613\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.017718954011797905\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.16649331152439117\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.1174732250651687e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3631894588470459\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09609905630350113\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.1639866828918457\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.012272453866899014\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.16591428220272064\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 9.612399765046575e-10\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3662855327129364\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.09108228236436844\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11449981480836868\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.013040873222053051\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.11134634166955948\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.230308965816107e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.37691730260849\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.09014814347028732\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.14523908495903015\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.014420493505895138\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.1513361632823944\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.0608477429840946e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3867374062538147\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09535913914442062\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.14099128544330597\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01848837547004223\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.16283072531223297\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.933003535015132e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.39764851331710815\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.09654883295297623\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.11315502226352692\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.010567273944616318\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.11256983876228333\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 2.023875955714516e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.37223318219184875\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.0865689367055893\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.995875120162964\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.44467800855636597\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.050023153424263\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.07048849761486053\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3879775106906891\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04970322176814079\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.42404866218566895\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.15647292137145996\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0143144130706787\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.10832221806049347\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1836981624364853\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.25459861755371094\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08968645334243774\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.007409772370010614\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.09139180928468704\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.0330107880207606e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.3181295692920685\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06635686755180359\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.12500041723251343\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.012595945037901402\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.12953411042690277\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.7303711796756716e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.34061017632484436\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07254468649625778\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.10231272876262665\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.009846852160990238\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.09121515601873398\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.1649214926023888e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.41832220554351807\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.08077939599752426\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.06234324350953102\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.00450032576918602\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.06296985596418381\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.1416576572997883e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3378680348396301\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07378799468278885\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.07476135343313217\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.0063463239930570126\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.06932037323713303\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.9304309262224706e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3173944652080536\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06299640983343124\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.07609733194112778\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.0064047775231301785\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.07950898259878159\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.1600058691385584e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3200528919696808\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07233926653862\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08140906691551208\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007005686406046152\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.0883188247680664\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.0142061634965671e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3371725380420685\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07584502547979355\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.085594043135643\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.008147267624735832\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.0780891701579094\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.0966700880743474e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.3181946575641632\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.0710623562335968\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.883671998977661\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.35837897658348083\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05729260668158531\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06608634442090988\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3547232747077942\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03591165319085121\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3729040026664734\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.12647107243537903\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.2328126430511475\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.24930545687675476\n",
      "ab_loss: 24.181386947631836\n",
      "Gradient of AB Loss for row 0: tensor([0.1172, 0.1687, 0.1001, 0.1466])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0366, -0.0284,  0.0338,  0.0112, -0.0139,  0.0363,  0.0433,  0.0291,\n",
      "         0.0235,  0.0365,  0.0275,  0.0289,  0.0318,  0.0259, -0.0249,  0.0237,\n",
      "        -0.0188])\n",
      "Gradient of AB Loss for row 2: tensor([0.0309, 0.0417, 0.0218, 0.0439, 0.0404, 0.0344, 0.0350, 0.0484, 0.0424,\n",
      "        0.0371, 0.0463, 0.0453, 0.0376, 0.0357])\n",
      "Gradient of AB Loss for row 3: tensor([0.0476, 0.0511, 0.0202, 0.0343, 0.0355, 0.0471, 0.0316, 0.0249, 0.0467,\n",
      "        0.0584, 0.0272, 0.0268, 0.0340, 0.0233])\n",
      "Gradient of AB Loss for row 4: tensor([ 0.0164, -0.0365,  0.0487,  0.0196,  0.0271,  0.0472,  0.0322,  0.0417,\n",
      "         0.0398,  0.0418,  0.0434,  0.0458,  0.0243,  0.0305])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0159,  0.0156, -0.0229, -0.0129, -0.0265, -0.0280, -0.0241,  0.0255,\n",
      "        -0.0295, -0.0186, -0.0330,  0.0255,  0.0145,  0.0288,  0.0224, -0.0146,\n",
      "         0.0220, -0.0164, -0.0100, -0.0153,  0.0243, -0.0237, -0.0193])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0128, -0.0251, -0.0213, -0.0169, -0.0243, -0.0326, -0.0230, -0.0129,\n",
      "        -0.0163, -0.0161, -0.0230, -0.0163, -0.0265, -0.0180, -0.0175, -0.0294,\n",
      "        -0.0148, -0.0189, -0.0082,  0.0247, -0.0251, -0.0213, -0.0267])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0327, -0.0413, -0.0315,  0.0454,  0.0315,  0.0228,  0.0256,  0.0529,\n",
      "         0.0313,  0.0373,  0.0387, -0.0528,  0.0275, -0.0341])\n",
      "Gradient of AB Loss for row 8: tensor([0.0294, 0.0465, 0.0188, 0.0361, 0.0383, 0.0447, 0.0344, 0.0530, 0.0441,\n",
      "        0.0313, 0.0344, 0.0450, 0.0210, 0.0253])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0214, -0.0299, -0.0264, -0.0331,  0.0356,  0.0086, -0.0157, -0.0281,\n",
      "         0.0223,  0.0420, -0.0344,  0.0245, -0.0206, -0.0225, -0.0339, -0.0143,\n",
      "         0.0159, -0.0135])\n",
      "Gradient of AB Loss for row 10: tensor([-0.1854, -0.2452, -0.1461])\n",
      "Gradient of AB Loss for row 11: tensor([0.0229, 0.0538, 0.0100, 0.0268, 0.0479, 0.0392, 0.0432, 0.0349, 0.0475,\n",
      "        0.0521, 0.0491, 0.0344, 0.0334, 0.0393])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0204, -0.0290,  0.0328, -0.0381,  0.0107, -0.0245,  0.0488, -0.0416,\n",
      "         0.0469, -0.0380,  0.0322,  0.0356,  0.0275, -0.0225, -0.0338])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0416, -0.0356, -0.0470, -0.0299,  0.0411, -0.0358,  0.0361,  0.0478,\n",
      "         0.0348,  0.0361, -0.0387,  0.0410,  0.0445, -0.0334])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0294, -0.0396, -0.0438,  0.0492,  0.0487, -0.0482,  0.0335,  0.0366,\n",
      "         0.0550,  0.0513,  0.0366, -0.0449, -0.0305,  0.0320])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0474, -0.0334,  0.0312,  0.0429, -0.0507,  0.0466,  0.0448,  0.0403,\n",
      "         0.0428,  0.0547,  0.0497,  0.0336, -0.0286,  0.0298])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0373, -0.0610, -0.0414,  0.0426, -0.0415,  0.0520,  0.0384,  0.0461,\n",
      "         0.0441,  0.0291,  0.0364, -0.0278,  0.0355])\n",
      "Gradient of AB Loss for row 17: tensor([0.0357, 0.0460, 0.0278, 0.0315, 0.0511, 0.0259, 0.0337, 0.0350, 0.0539,\n",
      "        0.0374, 0.0288, 0.0360, 0.0303, 0.0296])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0275,  0.0321, -0.0511,  0.0364,  0.0144,  0.0254, -0.0350,  0.0220,\n",
      "         0.0386,  0.0475,  0.0326,  0.0188, -0.0238,  0.0306,  0.0339])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0371, -0.0195, -0.0313, -0.0466, -0.0355,  0.0462, -0.0304,  0.0439,\n",
      "         0.0207,  0.0449,  0.0359, -0.0444, -0.0466, -0.0208, -0.0340])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0218, -0.0180,  0.0215, -0.0220, -0.0184, -0.0275, -0.0211, -0.0200,\n",
      "        -0.0176, -0.0365, -0.0219,  0.0127,  0.0299, -0.0182, -0.0219, -0.0125,\n",
      "        -0.0210,  0.0252, -0.0299, -0.0253,  0.0198])\n",
      "Gradient of AB Loss for row 21: tensor([0.0293, 0.0355, 0.0313, 0.0383, 0.0404, 0.0276, 0.0342, 0.0304, 0.0503,\n",
      "        0.0277, 0.0372, 0.0366, 0.0224, 0.0280])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0266, -0.0331,  0.0409,  0.0401,  0.0244,  0.0146,  0.0366,  0.0376,\n",
      "         0.0373,  0.0189,  0.0348,  0.0318,  0.0312,  0.0369,  0.0238, -0.0236,\n",
      "         0.0232,  0.0188])\n",
      "Gradient of AB Loss for row 23: tensor([ 0.0233, -0.0450,  0.0483,  0.0245,  0.0229,  0.0340,  0.0384,  0.0282,\n",
      "         0.0430,  0.0366,  0.0423,  0.0452,  0.0323,  0.0336])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0338, -0.0446,  0.0267,  0.0189,  0.0498,  0.0322,  0.0364,  0.0566,\n",
      "         0.0508,  0.0384,  0.0334, -0.0293,  0.0424])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0304, -0.0478, -0.0402,  0.0529,  0.0459, -0.0448,  0.0397,  0.0429,\n",
      "         0.0408,  0.0492,  0.0323,  0.0294, -0.0402,  0.0367])\n",
      "Gradient of AB Loss for row 26: tensor([-0.6068])\n",
      "Gradient of AB Loss for row 27: tensor([0.0265, 0.0514, 0.0237, 0.0321, 0.0288, 0.0294, 0.0290, 0.0408, 0.0496,\n",
      "        0.0501, 0.0415, 0.0297, 0.0195, 0.0285])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0401,  0.0372,  0.0426,  0.0126,  0.0356,  0.0371,  0.0454,  0.0422,\n",
      "         0.0314,  0.0401,  0.0408,  0.0327,  0.0295,  0.0359])\n",
      "Gradient of AB Loss for row 29: tensor([ 0.0236, -0.0161, -0.0227,  0.0281,  0.0266, -0.0161,  0.0308, -0.0248,\n",
      "        -0.0129, -0.0170, -0.0313,  0.0240,  0.0129, -0.0179, -0.0221,  0.0268,\n",
      "         0.0295, -0.0141,  0.0220, -0.0153, -0.0225, -0.0260])\n",
      "Gradient of AB Loss for row 30: tensor([-0.5179])\n",
      "Gradient of AB Loss for row 31: tensor([ 0.0339, -0.0409,  0.0354,  0.0313,  0.0377,  0.0345,  0.0365,  0.0390,\n",
      "         0.0335,  0.0439,  0.0290,  0.0208,  0.0338,  0.0307])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08567049354314804\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.0861683338880539\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1498306542634964\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9479031562805176\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10634932667016983\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.14162932336330414\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.26069962978363037\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1140018180012703\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.011661567725241184\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1299627721309662\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.4520424862496384e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.2800965905189514\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07532797008752823\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.099057637155056\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.009154628030955791\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.11050134152173996\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 8.363664760757672e-10\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.28764644265174866\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07243972271680832\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1666921079158783\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.019521621987223625\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.18121296167373657\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.0291684171548354e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.32316574454307556\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.08209097385406494\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.11881480365991592\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.011154524981975555\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.12174342572689056\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.4480473486955248e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.33714058995246887\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07824461162090302\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.10895752161741257\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.010965040884912014\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10168729722499847\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 9.019514579655663e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3335045278072357\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07805775105953217\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.12255993485450745\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.009738891385495663\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.1238449439406395\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.0604829236982027e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3158194124698639\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.07759296149015427\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.15254056453704834\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01717068813741207\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.15925449132919312\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.0596339361512719e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.34140217304229736\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08347360789775848\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.1700611561536789\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.013500198721885681\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.17356780171394348\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.9548342944375463e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3322801887989044\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07419704645872116\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.630887031555176\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3791871666908264\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.045330166816711426\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.059963151812553406\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3418305814266205\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04434897005558014\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.35717394948005676\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.13188323378562927\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9087063074111938\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.0911049097776413\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1522112339735031\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.20125047862529755\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.07108767330646515\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.005633598193526268\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.07292663305997849\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 7.634763932173882e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2805292010307312\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05601079761981964\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.08305105566978455\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.007304253056645393\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.08725997805595398\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 8.663381123596992e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.29629725217819214\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06238307058811188\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.0662226751446724\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005478983279317617\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06199723854660988\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 8.757957137284222e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3598534166812897\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06636714935302734\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09500031173229218\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.009844884276390076\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.08606620132923126\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 7.886916120192211e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.30161094665527344\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06407080590724945\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.06842069327831268\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.006076757330447435\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.0695880576968193\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.0085302593054735e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2823103368282318\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.055631306022405624\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08529765158891678\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.008464067243039608\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09505058079957962\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.3359523487466163e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2947149872779846\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06441438943147659\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.07133442908525467\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.005839178338646889\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07021907716989517\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 9.981896420541148e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.2845018804073334\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06252484768629074\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.10943643003702164\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.010743885301053524\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.10352174937725067\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.928859738598021e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2903573215007782\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06200966611504555\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.5819199085235596\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3162601590156555\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04426906630396843\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05553717911243439\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3300379514694214\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.0339258573949337\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3240032494068146\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.10556802898645401\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.8065166473388672\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.1988830417394638\n",
      "ab_loss: 24.247053146362305\n",
      "Gradient of AB Loss for row 0: tensor([-0.0333,  0.0243,  0.0466,  0.0278,  0.0307,  0.0317,  0.0239,  0.0347,\n",
      "         0.0515,  0.0173,  0.0390,  0.0442,  0.0248,  0.0285])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0392, -0.0531, -0.0395, -0.0078,  0.0405, -0.0483,  0.0424,  0.0371,\n",
      "         0.0299,  0.0429,  0.0246, -0.0416, -0.0282,  0.0341])\n",
      "Gradient of AB Loss for row 2: tensor([ 0.1991, -0.0988,  0.2344])\n",
      "Gradient of AB Loss for row 3: tensor([-0.4292])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0238, -0.0308, -0.0393,  0.0209, -0.0298,  0.0360, -0.0423,  0.0328,\n",
      "        -0.0339,  0.0437,  0.0375,  0.0232, -0.0289, -0.0224, -0.0285])\n",
      "Gradient of AB Loss for row 5: tensor([-0.5234])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0341, -0.0387,  0.0317,  0.0204,  0.0299,  0.0240,  0.0361,  0.0360,\n",
      "         0.0478,  0.0439,  0.0286,  0.0312, -0.0287,  0.0420, -0.0315])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0378, -0.0240, -0.0328, -0.0521, -0.0364,  0.0327, -0.0399,  0.0281,\n",
      "         0.0372,  0.0381,  0.0321, -0.0286, -0.0371, -0.0202, -0.0208])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0566, -0.0419,  0.0490,  0.0395, -0.0425,  0.0348,  0.0449,  0.0296,\n",
      "         0.0527,  0.0419,  0.0283,  0.0345, -0.0359,  0.0303])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0419, -0.0380, -0.0489, -0.0186, -0.0357, -0.0447,  0.0367, -0.0277,\n",
      "        -0.0208,  0.0364,  0.0381, -0.0247,  0.0305, -0.0389, -0.0302, -0.0116])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0419,  0.0454,  0.0156,  0.0458, -0.0536,  0.0333,  0.0379,  0.0411,\n",
      "         0.0593,  0.0489,  0.0341, -0.0319, -0.0276,  0.0389])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0214, -0.0395, -0.0476, -0.0328,  0.0284,  0.0289,  0.0290,  0.0352,\n",
      "         0.0313,  0.0414,  0.0400,  0.0293,  0.0242,  0.0274, -0.0363,  0.0290])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0300,  0.0334,  0.0290,  0.0328,  0.0200, -0.0150,  0.0308,  0.0419,\n",
      "         0.0322,  0.0410,  0.0296,  0.0415,  0.0343,  0.0315, -0.0245,  0.0140,\n",
      "         0.0312])\n",
      "Gradient of AB Loss for row 13: tensor([ 0.1532,  0.1577, -0.1045,  0.1509])\n",
      "Gradient of AB Loss for row 14: tensor([ 0.1363,  0.1078, -0.1282,  0.1102])\n",
      "Gradient of AB Loss for row 15: tensor([0.0394, 0.0491, 0.0421, 0.0336, 0.0515, 0.0296, 0.0334, 0.0395, 0.0483,\n",
      "        0.0288, 0.0296, 0.0412, 0.0372, 0.0231])\n",
      "Gradient of AB Loss for row 16: tensor([0.0249, 0.0446, 0.0204, 0.0482, 0.0446, 0.0375, 0.0237, 0.0281, 0.0424,\n",
      "        0.0288, 0.0243, 0.0499, 0.0239, 0.0204])\n",
      "Gradient of AB Loss for row 17: tensor([ 0.0309, -0.0183,  0.0279,  0.0312,  0.0352,  0.0295,  0.0172,  0.0331,\n",
      "         0.0308,  0.0114,  0.0367,  0.0236,  0.0245,  0.0194,  0.0279,  0.0334,\n",
      "         0.0230,  0.0334,  0.0192,  0.0243])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0321, -0.0378, -0.0217,  0.0415,  0.0140, -0.0353,  0.0354,  0.0397,\n",
      "         0.0373,  0.0422,  0.0345, -0.0386,  0.0248,  0.0334,  0.0428])\n",
      "Gradient of AB Loss for row 19: tensor([0.0231, 0.0468, 0.0292, 0.0345, 0.0476, 0.0366, 0.0362, 0.0418, 0.0400,\n",
      "        0.0344, 0.0408, 0.0267, 0.0303, 0.0321])\n",
      "Gradient of AB Loss for row 20: tensor([0.1109, 0.1827, 0.1721, 0.1036])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0417, -0.0212, -0.0473, -0.0199,  0.0374,  0.0237, -0.0434,  0.0343,\n",
      "         0.0458,  0.0388,  0.0435, -0.0299,  0.0380,  0.0235, -0.0220,  0.0262])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0292, -0.0285, -0.0511, -0.0362, -0.0432,  0.0244, -0.0139, -0.0243,\n",
      "        -0.0266, -0.0280, -0.0286, -0.0397, -0.0398, -0.0220,  0.0221, -0.0333])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0244, -0.0233, -0.0230, -0.0514, -0.0290,  0.0501,  0.0349, -0.0335,\n",
      "         0.0369,  0.0457,  0.0272,  0.0354,  0.0375, -0.0406, -0.0337, -0.0258])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0287, -0.0303, -0.0223, -0.0340,  0.0393,  0.0278,  0.0136, -0.0352,\n",
      "         0.0396,  0.0452,  0.0360,  0.0433, -0.0419, -0.0427,  0.0199])\n",
      "Gradient of AB Loss for row 25: tensor([ 0.1502,  0.1236, -0.0966,  0.1306])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0373,  0.0518,  0.0084,  0.0353,  0.0454,  0.0301,  0.0391,  0.0310,\n",
      "         0.0603,  0.0329,  0.0489,  0.0399, -0.0306,  0.0363])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0273, -0.0490, -0.0332,  0.0443,  0.0385, -0.0373,  0.0432,  0.0370,\n",
      "         0.0501,  0.0368,  0.0420,  0.0296, -0.0369,  0.0211])\n",
      "Gradient of AB Loss for row 28: tensor([0.1616, 0.1572, 0.0840, 0.1276])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0349,  0.0351,  0.0402,  0.0397,  0.0173, -0.0264,  0.0390,  0.0324,\n",
      "         0.0509,  0.0200,  0.0458,  0.0285,  0.0316,  0.0365,  0.0392,  0.0273])\n",
      "Gradient of AB Loss for row 30: tensor([ 0.1354, -0.1257,  0.1772])\n",
      "Gradient of AB Loss for row 31: tensor([0.0448, 0.0526, 0.0357, 0.0245, 0.0371, 0.0466, 0.0261, 0.0463, 0.0506,\n",
      "        0.0378, 0.0387, 0.0330, 0.0212, 0.0241])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08574984222650528\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.08458127081394196\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1441112607717514\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9087265729904175\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10033023357391357\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.15202875435352325\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2409571409225464\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1219577044248581\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.011641806922852993\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.14142045378684998\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.747898048520824e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.2814200222492218\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07700833678245544\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.1259796917438507\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.012452109716832638\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.13731221854686737\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.2251440972832484e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2986913025379181\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08038955926895142\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.10060850530862808\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.011045017279684544\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.10888463258743286\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.1124893228853239e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3004346489906311\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.07813693583011627\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.12989352643489838\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.01638495922088623\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.13275690376758575\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 9.740257489454507e-10\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.31533390283584595\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07543142884969711\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.09862280637025833\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.010508256033062935\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10474944859743118\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 9.485208174453419e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.32452520728111267\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08297861367464066\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.10492076724767685\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.013249248266220093\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.10804865509271622\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 9.507540310593754e-10\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.29719388484954834\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.07767818123102188\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.12294729053974152\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.014581551775336266\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.13607437908649445\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.1288515677776445e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.297964870929718\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07914348691701889\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.11926732957363129\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.011717701330780983\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.11807401478290558\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 2.2656072573568053e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3292664587497711\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.08375129103660583\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.5458297729492188\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.396043986082077\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04703983664512634\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.056136857718229294\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.32173144817352295\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.03772040829062462\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3579988181591034\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.12173479050397873\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9245924353599548\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09501335024833679\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1598019152879715\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.21559342741966248\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.07490935176610947\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.006400491576641798\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.0742097795009613\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.386917580781244e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2672145962715149\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05526109039783478\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09442050755023956\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.009505249559879303\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.0934690311551094\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 8.23901391555637e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.2842801511287689\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06143094599246979\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07140444219112396\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006059674546122551\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07886373996734619\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 7.668576329500354e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3082089126110077\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06127532199025154\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.08197905123233795\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.008167984895408154\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.08092213422060013\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.5314218781981026e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.27270016074180603\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.059418193995952606\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.0640268623828888\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.005419627297669649\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.05853023752570152\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 9.686900170891022e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2715401351451874\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.05576569586992264\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.10308697819709778\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.010486394166946411\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.10656645894050598\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.2148697603464598e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.27809324860572815\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06268233060836792\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08634475618600845\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007273733615875244\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08627451211214066\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.0337256606263168e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.27810579538345337\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06254549324512482\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07646795362234116\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.006337436847388744\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07302571088075638\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 7.825617376333582e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.28111588954925537\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.059143926948308945\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.387404203414917\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.2958812713623047\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.045433104038238525\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.051648061722517014\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.31504353880882263\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.031506165862083435\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.32589468359947205\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.10722590982913971\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.8634674549102783\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.20660261809825897\n",
      "ab_loss: 24.182470321655273\n",
      "Gradient of AB Loss for row 0: tensor([0.0358, 0.0557, 0.0209, 0.0299, 0.0474, 0.0549, 0.0437, 0.0489, 0.0589,\n",
      "        0.0413, 0.0392, 0.0316, 0.0327])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0411,  0.0509,  0.0162,  0.0261,  0.0448,  0.0318,  0.0332,  0.0425,\n",
      "         0.0478,  0.0423,  0.0303,  0.0192, -0.0454,  0.0341])\n",
      "Gradient of AB Loss for row 2: tensor([0.0267, 0.0377, 0.0379, 0.0279, 0.0419, 0.0385, 0.0379, 0.0361, 0.0509,\n",
      "        0.0300, 0.0388, 0.0334, 0.0246, 0.0248])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0272, -0.0322, -0.0520, -0.0380,  0.0393, -0.0257,  0.0432,  0.0338,\n",
      "         0.0451,  0.0448,  0.0226,  0.0260, -0.0342, -0.0268,  0.0139, -0.0174])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0360, -0.0369, -0.0324,  0.0392,  0.0231,  0.0323, -0.0414,  0.0319,\n",
      "         0.0392,  0.0293,  0.0443,  0.0313, -0.0434,  0.0152,  0.0395])\n",
      "Gradient of AB Loss for row 5: tensor([-0.4488])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0582, -0.0368,  0.0507,  0.0368,  0.0387,  0.0346,  0.0331,  0.0423,\n",
      "         0.0394,  0.0477,  0.0434,  0.0452, -0.0289,  0.0374])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0332, -0.0358, -0.0202, -0.0449,  0.0342,  0.0168, -0.0207, -0.0181,\n",
      "        -0.0296, -0.0360, -0.0247, -0.0347, -0.0387, -0.0232, -0.0159, -0.0372,\n",
      "        -0.0166])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0404,  0.0569,  0.0305,  0.0498, -0.0327,  0.0341,  0.0459,  0.0460,\n",
      "         0.0465,  0.0457,  0.0310,  0.0300, -0.0364,  0.0280])\n",
      "Gradient of AB Loss for row 9: tensor([ 0.0410, -0.0342,  0.0502,  0.0410,  0.0221,  0.0478,  0.0406,  0.0261,\n",
      "         0.0432,  0.0576,  0.0397,  0.0360,  0.0310,  0.0213])\n",
      "Gradient of AB Loss for row 10: tensor([0.0426, 0.0334, 0.0138, 0.0349, 0.0265, 0.0295, 0.0448, 0.0517, 0.0301,\n",
      "        0.0540, 0.0519, 0.0308, 0.0396])\n",
      "Gradient of AB Loss for row 11: tensor([0.0305, 0.0439, 0.0201, 0.0317, 0.0424, 0.0293, 0.0396, 0.0431, 0.0462,\n",
      "        0.0382, 0.0347, 0.0263, 0.0302, 0.0388])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0291, -0.0258, -0.0227,  0.0435,  0.0285,  0.0145, -0.0305,  0.0340,\n",
      "         0.0436,  0.0221,  0.0445, -0.0254,  0.0402, -0.0336,  0.0266, -0.0168,\n",
      "         0.0176])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0382,  0.0463,  0.0279,  0.0341,  0.0387,  0.0369,  0.0447,  0.0370,\n",
      "         0.0486,  0.0309,  0.0284,  0.0288, -0.0293,  0.0240])\n",
      "Gradient of AB Loss for row 14: tensor([0.0307, 0.0532, 0.0211, 0.0255, 0.0324, 0.0258, 0.0383, 0.0214, 0.0552,\n",
      "        0.0423, 0.0380, 0.0315, 0.0278, 0.0328])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0230, -0.0364, -0.0359,  0.0159, -0.0289,  0.0419, -0.0454,  0.0421,\n",
      "        -0.0383,  0.0371,  0.0361,  0.0199, -0.0352, -0.0243, -0.0276])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0275, -0.0286, -0.0355,  0.0344,  0.0218,  0.0130, -0.0282,  0.0319,\n",
      "         0.0504,  0.0374,  0.0358,  0.0382, -0.0376,  0.0295,  0.0258])\n",
      "Gradient of AB Loss for row 17: tensor([0.0237, 0.0508, 0.0138, 0.0159, 0.0378, 0.0289, 0.0310, 0.0437, 0.0296,\n",
      "        0.0376, 0.0466, 0.0447, 0.0374, 0.0343])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0537, -0.0332,  0.0418,  0.0457, -0.0449,  0.0366,  0.0414,  0.0425,\n",
      "         0.0413,  0.0500,  0.0446,  0.0244, -0.0374,  0.0358])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0442, -0.0206, -0.0443, -0.0496, -0.0329,  0.0397, -0.0344,  0.0348,\n",
      "         0.0383,  0.0474,  0.0321, -0.0419, -0.0478, -0.0380])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0189, -0.0387, -0.0248, -0.0323,  0.0341,  0.0206,  0.0247,  0.0161,\n",
      "        -0.0178,  0.0176,  0.0469,  0.0360,  0.0367,  0.0267,  0.0335, -0.0336,\n",
      "        -0.0282,  0.0250])\n",
      "Gradient of AB Loss for row 21: tensor([0.0335, 0.0476, 0.0261, 0.0281, 0.0454, 0.0313, 0.0332, 0.0274, 0.0452,\n",
      "        0.0301, 0.0436, 0.0331, 0.0256, 0.0380])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0321, -0.0275, -0.0276,  0.0304,  0.0402,  0.0228,  0.0236, -0.0291,\n",
      "         0.0300,  0.0418,  0.0417,  0.0301,  0.0273,  0.0373, -0.0248,  0.0260])\n",
      "Gradient of AB Loss for row 23: tensor([ 0.2327,  0.1955, -0.1672])\n",
      "Gradient of AB Loss for row 24: tensor([0.0262, 0.0413, 0.0171, 0.0303, 0.0435, 0.0363, 0.0315, 0.0335, 0.0325,\n",
      "        0.0487, 0.0416, 0.0396, 0.0266, 0.0192])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0417,  0.0561,  0.0188,  0.0368, -0.0342,  0.0395,  0.0427,  0.0281,\n",
      "         0.0461,  0.0330, -0.0372, -0.0251, -0.0302,  0.0275])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0338, -0.0140, -0.0324, -0.0408, -0.0406, -0.0185,  0.0225, -0.0349,\n",
      "        -0.0253, -0.0301,  0.0288,  0.0274,  0.0297, -0.0261,  0.0256, -0.0158,\n",
      "         0.0229, -0.0256, -0.0291])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0343, -0.0293, -0.0472, -0.0292,  0.0346,  0.0288, -0.0402,  0.0327,\n",
      "         0.0346,  0.0369,  0.0372,  0.0364, -0.0405, -0.0500, -0.0295])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0374, -0.0607, -0.0510,  0.0391, -0.0343,  0.0342,  0.0533,  0.0552,\n",
      "         0.0563,  0.0434,  0.0317, -0.0403,  0.0219])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0363, -0.0304, -0.0221, -0.0349,  0.0236,  0.0228, -0.0213, -0.0322,\n",
      "         0.0300,  0.0453,  0.0191,  0.0318, -0.0343, -0.0311, -0.0177,  0.0156,\n",
      "        -0.0271])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0322, -0.0234, -0.0202, -0.0269, -0.0278, -0.0228,  0.0279,  0.0287,\n",
      "        -0.0241, -0.0130, -0.0329,  0.0209,  0.0306,  0.0210, -0.0252,  0.0306,\n",
      "        -0.0165, -0.0153,  0.0273,  0.0303, -0.0217,  0.0221])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0402,  0.0279,  0.0204,  0.0173,  0.0394,  0.0453,  0.0384,  0.0376,\n",
      "         0.0418,  0.0509,  0.0533,  0.0301, -0.0350,  0.0356])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09576324373483658\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09584034234285355\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.17193670570850372\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.00332510471344\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11945275217294693\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.18539676070213318\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2928312122821808\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.17405734956264496\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.019186558201909065\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1821790486574173\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.6204327879520974e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.33439305424690247\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.0781431645154953\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.127785325050354\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.011649812571704388\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.14063958823680878\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.1456846582547087e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3060257136821747\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07528171688318253\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1518840491771698\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.017217369750142097\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.17300894856452942\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.2703967877669697e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3457019031047821\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.0944637730717659\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.13409583270549774\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.01220001932233572\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.13897861540317535\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.5578693890461182e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3441520035266876\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.08436638116836548\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.12627138197422028\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.013453415594995022\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.12323229759931564\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.8049660654995137e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.34675222635269165\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08940370380878448\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.13648714125156403\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.011731654405593872\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.13621769845485687\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.1525451704130774e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.34071803092956543\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08130770921707153\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.16057665646076202\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.014070461504161358\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.16573910415172577\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.365969781730314e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.33716872334480286\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08603597432374954\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.13530752062797546\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.018108965829014778\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.14204925298690796\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.1629168739091256e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.35675930976867676\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.0917857438325882\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.7749197483062744\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4199097156524658\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04877590388059616\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06495041400194168\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3566608428955078\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04924904555082321\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.37878865003585815\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1475812792778015\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9482112526893616\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.10204479843378067\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.19290220737457275\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2264685332775116\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08161050826311111\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.00724628334864974\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.08386360853910446\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.981018238339516e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2785319983959198\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06174563616514206\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.07215315103530884\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006356556434184313\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.07587451487779617\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.1956199363893916e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.2816493809223175\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06330983340740204\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07985442876815796\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005708468146622181\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07787749916315079\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.1938311450521155e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3073645830154419\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06743568181991577\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.07017006725072861\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007003961596637964\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.0678652748465538\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.1237845098932553e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.2855335772037506\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06582170724868774\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.09193960577249527\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.008821520954370499\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.08592475205659866\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.7768109206173222e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.30383938550949097\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06698410958051682\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08465374261140823\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.007826200686395168\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09685510396957397\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.5770896810707313e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3085918426513672\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07136613875627518\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08677008002996445\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.008319472894072533\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.09128547459840775\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.771147059187911e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.2965371310710907\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06684985756874084\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.08283112198114395\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.008413507603108883\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07709389179944992\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 7.091973119877082e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.26190614700317383\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.057949893176555634\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.4974350929260254\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3310107886791229\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04708384722471237\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.058733753859996796\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3532560169696808\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03727488964796066\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.33693891763687134\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11424041539430618\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.8553553819656372\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2146095633506775\n",
      "ab_loss: 25.072185516357422\n",
      "Gradient of AB Loss for row 0: tensor([-0.6141])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0417, -0.0396, -0.0385, -0.0413,  0.0319,  0.0168, -0.0316,  0.0512,\n",
      "         0.0477,  0.0318, -0.0269,  0.0394, -0.0469, -0.0459, -0.0326])\n",
      "Gradient of AB Loss for row 2: tensor([-0.4310])\n",
      "Gradient of AB Loss for row 3: tensor([0.0331, 0.0389, 0.0219, 0.0267, 0.0265, 0.0371, 0.0301, 0.0290, 0.0408,\n",
      "        0.0394, 0.0388, 0.0211, 0.0281, 0.0361])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0334,  0.0417,  0.0387,  0.0322,  0.0245,  0.0431,  0.0496,  0.0501,\n",
      "         0.0492,  0.0274,  0.0396,  0.0284,  0.0335,  0.0218])\n",
      "Gradient of AB Loss for row 5: tensor([-0.5109])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0324, -0.0409,  0.0398,  0.0265,  0.0284,  0.0378,  0.0327,  0.0440,\n",
      "         0.0512,  0.0388,  0.0336,  0.0244, -0.0364,  0.0353])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0281, -0.0353, -0.0407,  0.0224,  0.0333,  0.0263,  0.0318,  0.0384,\n",
      "         0.0282,  0.0200,  0.0228,  0.0306, -0.0265,  0.0304, -0.0275, -0.0329,\n",
      "        -0.0139])\n",
      "Gradient of AB Loss for row 8: tensor([0.1452, 0.1595, 0.0905, 0.1595])\n",
      "Gradient of AB Loss for row 9: tensor([-0.5710])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0584, -0.0496, -0.0458,  0.0389,  0.0240, -0.0178,  0.0403,  0.0459,\n",
      "         0.0360,  0.0382,  0.0376,  0.0297,  0.0325, -0.0223])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0272, -0.0209, -0.0275, -0.0256, -0.0233,  0.0294,  0.0451, -0.0331,\n",
      "        -0.0337,  0.0162,  0.0342, -0.0252, -0.0120, -0.0215,  0.0386,  0.0244,\n",
      "        -0.0295, -0.0404])\n",
      "Gradient of AB Loss for row 12: tensor([0.0299, 0.0418, 0.0325, 0.0250, 0.0439, 0.0322, 0.0336, 0.0281, 0.0438,\n",
      "        0.0403, 0.0419, 0.0364, 0.0309, 0.0334])\n",
      "Gradient of AB Loss for row 13: tensor([ 0.1599,  0.1260, -0.1123,  0.1467])\n",
      "Gradient of AB Loss for row 14: tensor([0.0291, 0.0398, 0.0207, 0.0300, 0.0400, 0.0246, 0.0305, 0.0442, 0.0377,\n",
      "        0.0460, 0.0413, 0.0314, 0.0322, 0.0192])\n",
      "Gradient of AB Loss for row 15: tensor([ 0.1288,  0.1164, -0.1153,  0.1197])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0329,  0.0437,  0.0261,  0.0451, -0.0382,  0.0319,  0.0297,  0.0389,\n",
      "         0.0529,  0.0375,  0.0296,  0.0323, -0.0395,  0.0311])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0298, -0.0330, -0.0297, -0.0302,  0.0195,  0.0338, -0.0362,  0.0448,\n",
      "         0.0330,  0.0362,  0.0399,  0.0458, -0.0423,  0.0320, -0.0222])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0200, -0.0565, -0.0462,  0.0447,  0.0241, -0.0400,  0.0446,  0.0401,\n",
      "         0.0561,  0.0457,  0.0412,  0.0346, -0.0404,  0.0240])\n",
      "Gradient of AB Loss for row 19: tensor([-0.7044])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0529,  0.0459,  0.0289,  0.0386,  0.0504,  0.0322,  0.0255,  0.0488,\n",
      "         0.0388,  0.0456,  0.0281,  0.0316, -0.0426,  0.0410])\n",
      "Gradient of AB Loss for row 21: tensor([0.0288, 0.0369, 0.0102, 0.0341, 0.0372, 0.0343, 0.0499, 0.0264, 0.0480,\n",
      "        0.0516, 0.0344, 0.0375, 0.0380])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0246, -0.0465, -0.0429,  0.0465,  0.0251,  0.0344,  0.0318,  0.0272,\n",
      "         0.0595,  0.0348,  0.0366,  0.0380, -0.0375,  0.0223])\n",
      "Gradient of AB Loss for row 23: tensor([-0.5334])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0324, -0.0381, -0.0308,  0.0200, -0.0300,  0.0382, -0.0457,  0.0389,\n",
      "        -0.0414,  0.0399, -0.0171,  0.0300, -0.0197, -0.0313, -0.0229])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0240,  0.0177,  0.0235, -0.0127, -0.0138, -0.0322, -0.0272,  0.0270,\n",
      "         0.0180,  0.0353,  0.0136, -0.0231,  0.0218,  0.0219,  0.0248,  0.0134,\n",
      "        -0.0200,  0.0198, -0.0092, -0.0125,  0.0240,  0.0131, -0.0224, -0.0200])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0453,  0.0433,  0.0425,  0.0260,  0.0235,  0.0370,  0.0401,  0.0344,\n",
      "         0.0551,  0.0283,  0.0336,  0.0367,  0.0314,  0.0390])\n",
      "Gradient of AB Loss for row 27: tensor([ 0.0600,  0.0213,  0.0351,  0.0528,  0.0404,  0.0325,  0.0412,  0.0625,\n",
      "         0.0360,  0.0363,  0.0284, -0.0374,  0.0299])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0140, -0.0383,  0.0327,  0.0235,  0.0292, -0.0404,  0.0421,  0.0444,\n",
      "         0.0490,  0.0412,  0.0275, -0.0371, -0.0369,  0.0367])\n",
      "Gradient of AB Loss for row 29: tensor([ 0.1602,  0.1454, -0.1471,  0.1335])\n",
      "Gradient of AB Loss for row 30: tensor([-0.6993])\n",
      "Gradient of AB Loss for row 31: tensor([-0.5587])\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:06\n",
      "Epoch 2/2\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09115490317344666\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09469068795442581\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1594104915857315\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.8953770399093628\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10257391631603241\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.16896867752075195\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.25285249948501587\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.12101230770349503\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.012436063028872013\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.137896329164505\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.2175062069630371e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.2598150670528412\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07049562782049179\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.10552535206079483\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.010119769722223282\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.12077490240335464\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.1249653431022466e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.28318554162979126\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07873780280351639\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.10460197925567627\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.010934444144368172\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.11786799132823944\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 8.599135847831008e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.2929205596446991\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.08200786262750626\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.12403329461812973\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.010354272089898586\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.11615355312824249\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.2296117457566424e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3209943473339081\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.08109838515520096\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.103447325527668\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.010167134925723076\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10114697366952896\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.2012570937969258e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.30708929896354675\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.0797480121254921\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.0958726778626442\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.009806646034121513\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.10433916002511978\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.227873025477777e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.29336777329444885\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.0774058997631073\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.1185825765132904\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.011699240654706955\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.12112925946712494\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.0711226350323955e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.29116955399513245\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07620715349912643\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.1960681974887848\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.009967805817723274\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.19220688939094543\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.986592224056949e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3227526545524597\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07857518643140793\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.4057347774505615\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3781513571739197\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04280120134353638\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.05635980889201164\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3229638338088989\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04077087715268135\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.37594592571258545\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1360623687505722\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.8994821906089783\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.0942995697259903\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1674094945192337\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.22858428955078125\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.07469113916158676\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.007220293395221233\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.07136857509613037\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 7.821882586078743e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.266816109418869\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05638774484395981\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.06434579938650131\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.005322430282831192\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.0672965943813324\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 8.071448509561208e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.28726616501808167\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06463190913200378\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.06484416872262955\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005003264173865318\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06331086158752441\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.3862962999766637e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3265971839427948\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06849277764558792\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.07875849306583405\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.0073318807408213615\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.07909325510263443\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.1335259397782238e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.27498266100883484\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.05872360244393349\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08125492930412292\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.01007628720253706\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.07560261338949203\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.2934789905827415e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2929729223251343\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.061763446778059006\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.10514101386070251\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.009826981462538242\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.10648451745510101\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 6.874800728695618e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.27440187335014343\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.062043193727731705\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.0792052149772644\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.00716145196929574\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07694268226623535\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 8.430836029305055e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.26757681369781494\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.05868631228804588\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.06712032854557037\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.00708754314109683\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.0648694559931755\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 9.835281478132174e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2602444589138031\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.05817190930247307\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.3417623043060303\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.29756325483322144\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.052371494472026825\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.053038693964481354\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3273470103740692\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.035016559064388275\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3306417763233185\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11724954098463058\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.8188672065734863\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.20955783128738403\n",
      "ab_loss: 23.50170135498047\n",
      "Gradient of AB Loss for row 0: tensor([-0.0299, -0.0303, -0.0352, -0.0530, -0.0385,  0.0379, -0.0257,  0.0448,\n",
      "         0.0434,  0.0428,  0.0408, -0.0460, -0.0433, -0.0318])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0440, -0.0407, -0.0311, -0.0253, -0.0409,  0.0260, -0.0368,  0.0361,\n",
      "         0.0440,  0.0409,  0.0405,  0.0376,  0.0247,  0.0187, -0.0206])\n",
      "Gradient of AB Loss for row 2: tensor([0.1335, 0.1298, 0.1964, 0.1625])\n",
      "Gradient of AB Loss for row 3: tensor([0.0374, 0.0550, 0.0422, 0.0316, 0.0404, 0.0204, 0.0349, 0.0330, 0.0483,\n",
      "        0.0442, 0.0404, 0.0327, 0.0246, 0.0256])\n",
      "Gradient of AB Loss for row 4: tensor([-0.5301])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0260, -0.0305, -0.0221, -0.0441,  0.0224, -0.0201, -0.0292,  0.0352,\n",
      "         0.0252,  0.0396,  0.0397,  0.0433,  0.0214, -0.0251,  0.0327])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0395,  0.0448,  0.0372,  0.0420,  0.0125, -0.0235,  0.0318,  0.0410,\n",
      "         0.0241,  0.0413,  0.0291,  0.0318,  0.0248,  0.0227, -0.0239,  0.0240,\n",
      "         0.0158])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0164, -0.0381,  0.0307,  0.0156,  0.0348, -0.0358,  0.0319,  0.0368,\n",
      "         0.0436,  0.0487,  0.0170, -0.0269, -0.0359,  0.0257,  0.0338])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0234, -0.0121, -0.0199,  0.0235, -0.0167,  0.0188, -0.0218, -0.0231,\n",
      "        -0.0264,  0.0252,  0.0073,  0.0222, -0.0165,  0.0078,  0.0236, -0.0215,\n",
      "        -0.0182, -0.0123,  0.0284,  0.0174,  0.0216, -0.0198,  0.0201, -0.0319])\n",
      "Gradient of AB Loss for row 9: tensor([0.0370, 0.0453, 0.0095, 0.0340, 0.0379, 0.0274, 0.0265, 0.0266, 0.0365,\n",
      "        0.0502, 0.0344, 0.0249, 0.0284, 0.0458])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0365,  0.0487, -0.0500,  0.0231,  0.0170,  0.0397,  0.0378,  0.0481,\n",
      "         0.0581,  0.0390,  0.0320,  0.0393,  0.0429])\n",
      "Gradient of AB Loss for row 11: tensor([0.0411, 0.0589, 0.0152, 0.0316, 0.0446, 0.0332, 0.0366, 0.0483, 0.0569,\n",
      "        0.0467, 0.0352, 0.0318, 0.0240, 0.0205])\n",
      "Gradient of AB Loss for row 12: tensor([0.0255, 0.0546, 0.0242, 0.0296, 0.0396, 0.0333, 0.0450, 0.0281, 0.0432,\n",
      "        0.0469, 0.0438, 0.0357, 0.0345, 0.0378])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0299, -0.0294, -0.0510, -0.0304, -0.0415,  0.0336, -0.0345,  0.0238,\n",
      "         0.0532,  0.0349,  0.0360, -0.0388,  0.0381,  0.0213, -0.0292])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0271, -0.0275, -0.0360, -0.0451, -0.0258,  0.0350, -0.0323,  0.0413,\n",
      "         0.0249,  0.0325,  0.0412, -0.0411, -0.0394, -0.0356, -0.0213])\n",
      "Gradient of AB Loss for row 15: tensor([-0.4089])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0456, -0.0341,  0.0451,  0.0395, -0.0388,  0.0462,  0.0481,  0.0411,\n",
      "         0.0482,  0.0418,  0.0374,  0.0303, -0.0227,  0.0235])\n",
      "Gradient of AB Loss for row 17: tensor([0.0237, 0.0407, 0.0142, 0.0317, 0.0505, 0.0392, 0.0459, 0.0355, 0.0423,\n",
      "        0.0518, 0.0384, 0.0387, 0.0262, 0.0359])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0340, -0.0207,  0.0214, -0.0201, -0.0183, -0.0267, -0.0251,  0.0190,\n",
      "        -0.0234, -0.0146, -0.0262,  0.0195,  0.0206,  0.0263,  0.0214, -0.0158,\n",
      "         0.0141,  0.0235, -0.0259, -0.0175, -0.0140,  0.0177, -0.0222, -0.0252])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0273,  0.0335, -0.0331,  0.0400,  0.0286,  0.0230,  0.0325,  0.0460,\n",
      "         0.0332,  0.0582,  0.0176,  0.0371,  0.0211,  0.0218])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0235, -0.0421,  0.0230,  0.0209, -0.0195,  0.0387,  0.0488,  0.0332,\n",
      "         0.0263,  0.0416,  0.0436,  0.0390,  0.0314, -0.0274,  0.0148, -0.0228])\n",
      "Gradient of AB Loss for row 21: tensor([ 0.1507, -0.1160,  0.1732])\n",
      "Gradient of AB Loss for row 22: tensor([0.1708, 0.1632, 0.1147, 0.1229])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0540,  0.0468,  0.0168,  0.0313,  0.0287,  0.0219,  0.0321,  0.0406,\n",
      "         0.0428,  0.0510,  0.0489,  0.0308, -0.0371,  0.0396])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0334, -0.0379, -0.0577, -0.0360, -0.0453,  0.0406, -0.0228,  0.0410,\n",
      "         0.0282,  0.0487, -0.0263,  0.0408,  0.0420, -0.0333])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0150, -0.0308, -0.0161, -0.0222, -0.0221, -0.0246, -0.0161,  0.0295,\n",
      "        -0.0353, -0.0219,  0.0196, -0.0323, -0.0177,  0.0087, -0.0153, -0.0196,\n",
      "         0.0289, -0.0294, -0.0354])\n",
      "Gradient of AB Loss for row 26: tensor([0.1340, 0.1135, 0.1168, 0.1664])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0439, -0.0616, -0.0507,  0.0339,  0.0477,  0.0298,  0.0385,  0.0591,\n",
      "         0.0431,  0.0337,  0.0245, -0.0431,  0.0313])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0298, -0.0341,  0.0108, -0.0226,  0.0289, -0.0300,  0.0309,  0.0433,\n",
      "        -0.0269,  0.0237, -0.0232, -0.0328, -0.0339,  0.0300,  0.0227])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0207, -0.0369, -0.0214, -0.0304,  0.0280,  0.0185, -0.0166, -0.0240,\n",
      "         0.0243,  0.0387, -0.0243,  0.0274, -0.0325, -0.0179, -0.0315, -0.0185,\n",
      "         0.0133, -0.0140])\n",
      "Gradient of AB Loss for row 30: tensor([-0.1541,  0.1743,  0.1773])\n",
      "Gradient of AB Loss for row 31: tensor([0.0365, 0.0435, 0.0240, 0.0346, 0.0389, 0.0380, 0.0333, 0.0432, 0.0408,\n",
      "        0.0365, 0.0347, 0.0374, 0.0273, 0.0250])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08691232651472092\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.07795747369527817\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.15651381015777588\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9563676118850708\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10067767649888992\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.1614244431257248\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2561882734298706\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1376272588968277\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.013718911446630955\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1484372317790985\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.324298337657126e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3249146044254303\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07727240025997162\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.1266324669122696\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.012467644177377224\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.14726653695106506\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.7019494702452675e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.30272871255874634\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07515452057123184\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.13850082457065582\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01559055969119072\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.14468194544315338\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.543405514503604e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.31678640842437744\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.0798877701163292\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.1269373893737793\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.014728560112416744\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.12912599742412567\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.1754804907226912e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3091008961200714\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07085974514484406\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11652113497257233\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.010849577374756336\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.1182454526424408\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 2.5006408055361362e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.34674376249313354\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08113013952970505\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.1285226047039032\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.015803318470716476\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.1257510781288147\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.1508886066380342e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3180205225944519\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08359228074550629\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.1287061870098114\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.011991795152425766\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.1279028356075287\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.861432452621159e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3104825019836426\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.0734434574842453\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.16013064980506897\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.010879099369049072\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.16298289597034454\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.174713770701885e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.31998592615127563\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.0771547257900238\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.610503911972046\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.38236260414123535\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04463884234428406\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.055988386273384094\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3663409650325775\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.045045893639326096\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3825901746749878\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.12621396780014038\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9064098596572876\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.08864916861057281\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.15903180837631226\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.21930891275405884\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08565139770507812\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.007797920610755682\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.08320572972297668\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.061435717097936e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.27768030762672424\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05095116049051285\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.07626175880432129\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006698181387037039\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.07777158170938492\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 8.410982466067196e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.27047213912010193\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.05232038348913193\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07110652327537537\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005272909998893738\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07537423074245453\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.5419336918398585e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.30423206090927124\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.058985527604818344\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.07776473462581635\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.006418945267796516\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.07895851135253906\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.2581429231772745e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3082982003688812\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.061499204486608505\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.09518474340438843\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.008384759537875652\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.08606081455945969\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 7.606431595696961e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2810328006744385\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.0533163920044899\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08754929155111313\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.006714233197271824\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09500862658023834\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.198769861154858e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2760765254497528\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.05413520336151123\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08249106258153915\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007172514218837023\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07850988954305649\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.786083999761217e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.28273382782936096\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.05608325079083443\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.08637556433677673\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.007991105318069458\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.0792955532670021\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.4024154060265914e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2804515063762665\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.05640812963247299\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.4254817962646484\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.28742414712905884\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04675288870930672\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.04924565553665161\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.37466496229171753\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03786972537636757\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3623640239238739\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11624141782522202\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.960481882095337\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.21168988943099976\n",
      "ab_loss: 24.232328414916992\n",
      "Gradient of AB Loss for row 0: tensor([0.1509, 0.1278, 0.1446, 0.1601])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0160, -0.0456, -0.0440,  0.0466, -0.0382,  0.0445,  0.0456,  0.0412,\n",
      "         0.0538,  0.0240,  0.0312, -0.0298,  0.0422])\n",
      "Gradient of AB Loss for row 2: tensor([ 0.0402, -0.0372,  0.0419,  0.0085, -0.0324,  0.0334,  0.0356, -0.0408,\n",
      "         0.0345,  0.0468, -0.0309,  0.0316, -0.0190,  0.0394])\n",
      "Gradient of AB Loss for row 3: tensor([0.0383, 0.0495, 0.0110, 0.0255, 0.0320, 0.0355, 0.0290, 0.0339, 0.0521,\n",
      "        0.0403, 0.0375, 0.0277, 0.0317, 0.0231])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0548, -0.0361,  0.0364,  0.0354, -0.0374,  0.0491,  0.0358,  0.0373,\n",
      "         0.0473,  0.0449,  0.0512,  0.0377, -0.0376,  0.0308])\n",
      "Gradient of AB Loss for row 5: tensor([0.0390, 0.0562, 0.0225, 0.0285, 0.0487, 0.0233, 0.0386, 0.0369, 0.0416,\n",
      "        0.0418, 0.0207, 0.0241, 0.0388, 0.0331])\n",
      "Gradient of AB Loss for row 6: tensor([0.1199, 0.1559, 0.2065, 0.1483])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0278,  0.0350, -0.0389, -0.0459, -0.0396,  0.0262, -0.0183,  0.0357,\n",
      "         0.0241,  0.0302, -0.0283,  0.0220, -0.0436, -0.0163,  0.0308, -0.0227])\n",
      "Gradient of AB Loss for row 8: tensor([-0.4560])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0393, -0.0572, -0.0398,  0.0573,  0.0487, -0.0532,  0.0442,  0.0434,\n",
      "         0.0407,  0.0484,  0.0309,  0.0326, -0.0228,  0.0251])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0284, -0.0219, -0.0321,  0.0419,  0.0369,  0.0129, -0.0250,  0.0259,\n",
      "         0.0409,  0.0375,  0.0364, -0.0179,  0.0303, -0.0332,  0.0197, -0.0192,\n",
      "         0.0167])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0422, -0.0384, -0.0268, -0.0326, -0.0407, -0.0449,  0.0350, -0.0397,\n",
      "         0.0324,  0.0405,  0.0537,  0.0411, -0.0371, -0.0341])\n",
      "Gradient of AB Loss for row 12: tensor([0.0355, 0.0393, 0.0505, 0.0349, 0.0397, 0.0349, 0.0453, 0.0415, 0.0527,\n",
      "        0.0461, 0.0439, 0.0324, 0.0307, 0.0150])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0289,  0.0331,  0.0227,  0.0348, -0.0353,  0.0282,  0.0314,  0.0251,\n",
      "         0.0420, -0.0222,  0.0254, -0.0311, -0.0340,  0.0231,  0.0339])\n",
      "Gradient of AB Loss for row 14: tensor([0.0316, 0.0362, 0.0243, 0.0309, 0.0366, 0.0346, 0.0379, 0.0257, 0.0508,\n",
      "        0.0388, 0.0347, 0.0368, 0.0265, 0.0243])\n",
      "Gradient of AB Loss for row 15: tensor([-0.1252, -0.1364, -0.1119, -0.1110])\n",
      "Gradient of AB Loss for row 16: tensor([ 0.0332, -0.0477,  0.0377,  0.0258,  0.0159,  0.0351,  0.0342,  0.0334,\n",
      "         0.0302,  0.0537,  0.0305,  0.0397,  0.0275,  0.0269])\n",
      "Gradient of AB Loss for row 17: tensor([0.0403, 0.0390, 0.0316, 0.0273, 0.0461, 0.0384, 0.0341, 0.0367, 0.0432,\n",
      "        0.0365, 0.0422, 0.0386, 0.0375, 0.0370])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0345,  0.0530,  0.0074,  0.0400, -0.0316,  0.0348,  0.0367,  0.0354,\n",
      "         0.0489,  0.0435, -0.0468, -0.0484, -0.0233,  0.0329])\n",
      "Gradient of AB Loss for row 19: tensor([0.0392, 0.0404, 0.0221, 0.0377, 0.0452, 0.0317, 0.0325, 0.0381, 0.0425,\n",
      "        0.0425, 0.0363, 0.0276, 0.0416])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0198, -0.0220, -0.0131, -0.0155, -0.0198,  0.0233, -0.0190, -0.0245,\n",
      "        -0.0215, -0.0209,  0.0080,  0.0343,  0.0262, -0.0265,  0.0145, -0.0218,\n",
      "        -0.0208, -0.0171, -0.0103,  0.0130, -0.0342, -0.0221])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0411, -0.0314, -0.0383,  0.0375,  0.0372,  0.0403,  0.0252,  0.0342,\n",
      "         0.0412,  0.0342,  0.0365,  0.0476,  0.0334,  0.0360,  0.0253, -0.0326,\n",
      "         0.0254,  0.0174])\n",
      "Gradient of AB Loss for row 22: tensor([0.1575, 0.1252, 0.1327, 0.1667])\n",
      "Gradient of AB Loss for row 23: tensor([0.0337, 0.0367, 0.0338, 0.0262, 0.0362, 0.0392, 0.0389, 0.0428, 0.0446,\n",
      "        0.0234, 0.0310, 0.0371, 0.0210, 0.0226])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0337, -0.0366, -0.0484, -0.0380,  0.0377, -0.0202,  0.0320,  0.0368,\n",
      "         0.0394,  0.0373,  0.0337,  0.0215, -0.0349, -0.0182,  0.0243, -0.0147])\n",
      "Gradient of AB Loss for row 25: tensor([0.1046, 0.1620, 0.1326, 0.1147])\n",
      "Gradient of AB Loss for row 26: tensor([0.0341, 0.0576, 0.0305, 0.0290, 0.0456, 0.0314, 0.0368, 0.0419, 0.0487,\n",
      "        0.0394, 0.0402, 0.0374, 0.0285, 0.0242])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0432,  0.0393,  0.0435,  0.0240, -0.0442,  0.0311, -0.0258,  0.0431,\n",
      "         0.0427, -0.0300,  0.0252,  0.0272, -0.0191,  0.0180,  0.0311])\n",
      "Gradient of AB Loss for row 28: tensor([0.0370, 0.0462, 0.0319, 0.0377, 0.0428, 0.0417, 0.0255, 0.0426, 0.0417,\n",
      "        0.0373, 0.0423, 0.0355, 0.0437, 0.0162])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0495,  0.0499,  0.0406,  0.0297, -0.0514,  0.0467,  0.0503,  0.0375,\n",
      "         0.0571,  0.0452,  0.0257, -0.0283,  0.0175])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0354, -0.0286,  0.0313,  0.0228,  0.0207, -0.0191,  0.0507, -0.0422,\n",
      "         0.0299,  0.0393,  0.0447,  0.0423,  0.0378,  0.0198, -0.0243])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0449, -0.0204, -0.0435, -0.0314, -0.0351,  0.0390, -0.0272,  0.0353,\n",
      "         0.0406,  0.0452,  0.0369, -0.0447, -0.0334, -0.0370, -0.0219])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08318057656288147\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.08353514224290848\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1474388986825943\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.8702049851417542\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.09521898627281189\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.15066854655742645\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.23634177446365356\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.12330061197280884\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.01187640056014061\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.13687951862812042\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 2.1996402477242327e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.268501341342926\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.0711153969168663\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.0985724925994873\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.009172753430902958\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.10889685153961182\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 8.52320214406177e-10\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.28870466351509094\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07394206523895264\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.14837822318077087\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.014505879022181034\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.15941114723682404\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.1398514354610256e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3107205331325531\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.07171767204999924\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.11138015240430832\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.008050515316426754\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.10842419415712357\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 9.779602683224198e-10\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.26369431614875793\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.05908150225877762\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.08904264867305756\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.008111435920000076\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.0866972804069519\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 9.343016360574552e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.2882131040096283\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.0642288476228714\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.1250990331172943\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.012296904809772968\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.13296371698379517\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.30818156307555e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3059313893318176\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.07305600494146347\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.12638993561267853\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.012703008949756622\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.1351626068353653\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 2.1531454397205607e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.29816052317619324\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07109805941581726\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.10090766847133636\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.010682130232453346\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.10343533754348755\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.0972255326535674e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3225165605545044\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.06916069984436035\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.4225497245788574\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.34487655758857727\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04496292024850845\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.05232267081737518\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3346267640590668\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.03884198144078255\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3403394818305969\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.11909831315279007\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.8850663304328918\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09230220317840576\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.17251068353652954\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.20639926195144653\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06951842457056046\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.006725666578859091\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.06994316726922989\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.2142357119770963e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.24535110592842102\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.048862721771001816\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.07115165144205093\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006816107779741287\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.06995487958192825\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.313338268311043e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.28321951627731323\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.059962183237075806\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.06486619263887405\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005815331824123859\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06479419022798538\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.2510660285514064e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.33844542503356934\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06685931980609894\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.08655829727649689\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007839043624699116\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.08434127271175385\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 6.905659932776587e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3029365837574005\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06356007605791092\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.05991862714290619\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.004809551872313023\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.05911979079246521\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.0150412732556902e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.280860036611557\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.0569809153676033\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.07215020060539246\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.005313460249453783\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.07924792170524597\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.1397329746642981e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2636042833328247\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.05927927792072296\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.07360930740833282\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.0063601951114833355\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07708858698606491\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.080447894658448e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.26833510398864746\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06075391173362732\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.06390739232301712\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.006275486666709185\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.06109965965151787\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.3636839435449133e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2544194757938385\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.05321517959237099\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.3817362785339355\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.29016122221946716\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.040172822773456573\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05275829881429672\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3060363233089447\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.0314340814948082\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.31747016310691833\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.10343018919229507\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.7962251901626587\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.20178620517253876\n",
      "ab_loss: 25.012542724609375\n",
      "Gradient of AB Loss for row 0: tensor([0.0275, 0.0499, 0.0294, 0.0288, 0.0356, 0.0365, 0.0401, 0.0443, 0.0543,\n",
      "        0.0324, 0.0307, 0.0408, 0.0225, 0.0256])\n",
      "Gradient of AB Loss for row 1: tensor([0.0322, 0.0484, 0.0172, 0.0258, 0.0305, 0.0289, 0.0297, 0.0470, 0.0361,\n",
      "        0.0511, 0.0373, 0.0240, 0.0347, 0.0379])\n",
      "Gradient of AB Loss for row 2: tensor([0.0203, 0.0410, 0.0217, 0.0357, 0.0439, 0.0419, 0.0398, 0.0361, 0.0395,\n",
      "        0.0437, 0.0328, 0.0343, 0.0240, 0.0366])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0380, -0.0405, -0.0449, -0.0343, -0.0277,  0.0322, -0.0307, -0.0280,\n",
      "        -0.0280,  0.0381, -0.0209, -0.0412, -0.0243, -0.0204, -0.0271, -0.0179])\n",
      "Gradient of AB Loss for row 4: tensor([-0.5202])\n",
      "Gradient of AB Loss for row 5: tensor([0.0338, 0.0520, 0.0080, 0.0319, 0.0339, 0.0318, 0.0424, 0.0471, 0.0437,\n",
      "        0.0434, 0.0367, 0.0181, 0.0259, 0.0341])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0414, -0.0334,  0.0398,  0.0108,  0.0383, -0.0354,  0.0325,  0.0388,\n",
      "         0.0399,  0.0389,  0.0227, -0.0276, -0.0357,  0.0273,  0.0358])\n",
      "Gradient of AB Loss for row 7: tensor([ 0.0221, -0.0263,  0.0320,  0.0308, -0.0258,  0.0170,  0.0323,  0.0258,\n",
      "        -0.0248,  0.0231,  0.0206,  0.0251,  0.0313, -0.0202,  0.0177,  0.0202,\n",
      "         0.0281,  0.0275,  0.0274, -0.0193,  0.0262,  0.0260])\n",
      "Gradient of AB Loss for row 8: tensor([-0.2360, -0.1647,  0.2011])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0380, -0.0337,  0.0439,  0.0161,  0.0306,  0.0302,  0.0452,  0.0184,\n",
      "         0.0388,  0.0356,  0.0348, -0.0410,  0.0402,  0.0334, -0.0308, -0.0184])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0288, -0.0398, -0.0255, -0.0348,  0.0386,  0.0272,  0.0286,  0.0127,\n",
      "        -0.0291,  0.0227,  0.0413,  0.0277,  0.0216,  0.0273, -0.0394,  0.0215,\n",
      "         0.0176])\n",
      "Gradient of AB Loss for row 11: tensor([0.0313, 0.0563, 0.0168, 0.0238, 0.0389, 0.0374, 0.0287, 0.0394, 0.0464,\n",
      "        0.0468, 0.0438, 0.0309, 0.0307, 0.0226])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0443, -0.0622, -0.0303,  0.0303, -0.0436,  0.0328,  0.0372,  0.0512,\n",
      "         0.0342,  0.0568,  0.0357, -0.0518,  0.0291])\n",
      "Gradient of AB Loss for row 13: tensor([0.0312, 0.0487, 0.0301, 0.0404, 0.0394, 0.0345, 0.0405, 0.0352, 0.0310,\n",
      "        0.0359, 0.0164, 0.0290, 0.0201, 0.0238])\n",
      "Gradient of AB Loss for row 14: tensor([-0.5428])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0417, -0.0426, -0.0292, -0.0219, -0.0435,  0.0494,  0.0340, -0.0250,\n",
      "         0.0413,  0.0395,  0.0397,  0.0337,  0.0248,  0.0265, -0.0185,  0.0252])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0302, -0.0299,  0.0436,  0.0124,  0.0271,  0.0542,  0.0398,  0.0368,\n",
      "         0.0541,  0.0421,  0.0423,  0.0294, -0.0424,  0.0337])\n",
      "Gradient of AB Loss for row 17: tensor([ 0.1557,  0.1119, -0.0858,  0.1592])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0402, -0.0280, -0.0371, -0.0542, -0.0305,  0.0360, -0.0290,  0.0320,\n",
      "         0.0357,  0.0402,  0.0269, -0.0456, -0.0441, -0.0234])\n",
      "Gradient of AB Loss for row 19: tensor([ 0.0397, -0.0516,  0.0417,  0.0160,  0.0250, -0.0357,  0.0434,  0.0408,\n",
      "         0.0505,  0.0505,  0.0358,  0.0327,  0.0266,  0.0335])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0427, -0.0292, -0.0302, -0.0360,  0.0436,  0.0299,  0.0197, -0.0229,\n",
      "         0.0346,  0.0466,  0.0311,  0.0394,  0.0426, -0.0279, -0.0305, -0.0202,\n",
      "         0.0173])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0411, -0.0362, -0.0542,  0.0402, -0.0426,  0.0447,  0.0308, -0.0445,\n",
      "         0.0509,  0.0519,  0.0562,  0.0514,  0.0347,  0.0382])\n",
      "Gradient of AB Loss for row 22: tensor([0.0401, 0.0391, 0.0179, 0.0280, 0.0475, 0.0314, 0.0392, 0.0310, 0.0446,\n",
      "        0.0534, 0.0372, 0.0314, 0.0225, 0.0358])\n",
      "Gradient of AB Loss for row 23: tensor([-0.6833])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0322, -0.0528, -0.0423,  0.0523,  0.0448, -0.0513,  0.0277,  0.0336,\n",
      "         0.0491,  0.0377,  0.0421,  0.0238, -0.0410,  0.0305])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0194,  0.0223, -0.0222, -0.0205,  0.0264, -0.0279, -0.0223,  0.0249,\n",
      "         0.0108,  0.0256,  0.0257, -0.0147,  0.0201,  0.0253, -0.0102, -0.0178,\n",
      "        -0.0137,  0.0318,  0.0182,  0.0145, -0.0285,  0.0138])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0547, -0.0569, -0.0436,  0.0457, -0.0539,  0.0288,  0.0419,  0.0550,\n",
      "         0.0458,  0.0369,  0.0273, -0.0356,  0.0353])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0231, -0.0327, -0.0232,  0.0324,  0.0160,  0.0212,  0.0153, -0.0320,\n",
      "         0.0324,  0.0439,  0.0274,  0.0356,  0.0206,  0.0198,  0.0267, -0.0259,\n",
      "         0.0250])\n",
      "Gradient of AB Loss for row 28: tensor([0.1069, 0.1661, 0.1637, 0.1808])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0514, -0.0354,  0.0190,  0.0513, -0.0435,  0.0441,  0.0442,  0.0422,\n",
      "         0.0503,  0.0434,  0.0295, -0.0422,  0.0322])\n",
      "Gradient of AB Loss for row 30: tensor([ 0.0385, -0.0302,  0.0479,  0.0235,  0.0299,  0.0443,  0.0295,  0.0285,\n",
      "         0.0447,  0.0484,  0.0264,  0.0442,  0.0302,  0.0354])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0308, -0.0324, -0.0303, -0.0363,  0.0237,  0.0248, -0.0290, -0.0235,\n",
      "        -0.0239, -0.0321, -0.0383, -0.0264, -0.0339, -0.0294, -0.0223, -0.0307,\n",
      "        -0.0210])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09796003997325897\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09168105572462082\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1879061609506607\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9761255979537964\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11558591574430466\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.1686151623725891\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.28954070806503296\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.13866865634918213\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.014561858028173447\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1610507220029831\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 2.302492418948532e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3325430154800415\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.09380132704973221\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.14993391931056976\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.013741849921643734\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.16618062555789948\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.1115979248188523e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.29736772179603577\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08563587814569473\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1725260615348816\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.015676921233534813\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.18769776821136475\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.2597841658745779e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.33711203932762146\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09148097783327103\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.182968407869339\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.016722306609153748\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.18826258182525635\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.4866985420525225e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3456086814403534\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.0935915932059288\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11422206461429596\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.009920922107994556\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.11861278861761093\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.2094396595330181e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.35173386335372925\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08916783332824707\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.16766047477722168\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.011498210951685905\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.16319218277931213\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.0464838995360992e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3392064571380615\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09567342698574066\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.13109807670116425\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01733240857720375\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.13523106276988983\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.8952230895763478e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.32992231845855713\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.0877329558134079\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.13531339168548584\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.013957295566797256\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.14046955108642578\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.352658096642756e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3613481819629669\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.0983845591545105\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.7683043479919434\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4537157714366913\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04644386097788811\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.0674709901213646\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3430224061012268\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.045513782650232315\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3765449523925781\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.14500553905963898\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9557489156723022\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.10175785422325134\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.18278220295906067\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.22631220519542694\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.07090941816568375\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.006400172598659992\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.07209821045398712\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.0556652219051443e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.29264360666275024\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06415010988712311\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.07154017686843872\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006311613600701094\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.07363881915807724\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.7236486682392638e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.29496774077415466\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06648504734039307\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08010471612215042\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.007398305460810661\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07781122624874115\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 9.478392515305245e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3371390998363495\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06887327134609222\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09020482003688812\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.00884890928864479\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.09392832964658737\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.3517522656769643e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3369237780570984\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07742094993591309\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08659747242927551\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.00854184664785862\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.07691503316164017\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.0646965531435626e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.29085859656333923\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06369753181934357\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08564165234565735\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.0073640248738229275\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.08715290576219559\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.650632852623346e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2941007912158966\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06865108013153076\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08458476513624191\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007278895005583763\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08253081887960434\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.538355998143004e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.31250834465026855\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.0744640976190567\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.088539257645607\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.01024380698800087\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.08551181852817535\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 9.58907619974525e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.32064583897590637\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.07436817139387131\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.6198251247406006\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3413909375667572\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05035828426480293\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06011457368731499\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3612819015979767\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03643858805298805\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3457137644290924\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11052015423774719\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.8888769149780273\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.22018788754940033\n",
      "ab_loss: 24.43180274963379\n",
      "Gradient of AB Loss for row 0: tensor([0.0298, 0.0507, 0.0145, 0.0318, 0.0435, 0.0288, 0.0346, 0.0471, 0.0470,\n",
      "        0.0426, 0.0370, 0.0207, 0.0270, 0.0438])\n",
      "Gradient of AB Loss for row 1: tensor([0.1406, 0.1295, 0.0843, 0.1286])\n",
      "Gradient of AB Loss for row 2: tensor([0.1661, 0.1275, 0.1279, 0.1559])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0306, -0.0332,  0.0416,  0.0245,  0.0195,  0.0338, -0.0345,  0.0377,\n",
      "         0.0433,  0.0318,  0.0406,  0.0497,  0.0282,  0.0279, -0.0219])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0250, -0.0315, -0.0491, -0.0391,  0.0371,  0.0346,  0.0397,  0.0368,\n",
      "         0.0319,  0.0375,  0.0453,  0.0379,  0.0266,  0.0363, -0.0388,  0.0245])\n",
      "Gradient of AB Loss for row 5: tensor([0.0280, 0.0454, 0.0248, 0.0309, 0.0397, 0.0384, 0.0294, 0.0475, 0.0467,\n",
      "        0.0305, 0.0377, 0.0392, 0.0274, 0.0296])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0312,  0.0461,  0.0409,  0.0244,  0.0293,  0.0487,  0.0462,  0.0466,\n",
      "         0.0474,  0.0285,  0.0213,  0.0372,  0.0248,  0.0244])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0395, -0.0447,  0.0402,  0.0170,  0.0378, -0.0316,  0.0242,  0.0285,\n",
      "         0.0493,  0.0322,  0.0402, -0.0423, -0.0161,  0.0253,  0.0391])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0421,  0.0425,  0.0270,  0.0380, -0.0415,  0.0353,  0.0307,  0.0371,\n",
      "         0.0446,  0.0356,  0.0421, -0.0207, -0.0222,  0.0236])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0453, -0.0438,  0.0420,  0.0139,  0.0335,  0.0448,  0.0398,  0.0377,\n",
      "         0.0526,  0.0324,  0.0309,  0.0270, -0.0298,  0.0307])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0398,  0.0454, -0.0319,  0.0265,  0.0475, -0.0304,  0.0420,  0.0419,\n",
      "         0.0437,  0.0396,  0.0345,  0.0278,  0.0295])\n",
      "Gradient of AB Loss for row 11: tensor([ 0.0306, -0.0423,  0.0410,  0.0341,  0.0242,  0.0489,  0.0329,  0.0562,\n",
      "         0.0420,  0.0506,  0.0289,  0.0298,  0.0215,  0.0234])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0286,  0.0298, -0.0284,  0.0356,  0.0257,  0.0332,  0.0380,  0.0259,\n",
      "         0.0303,  0.0395,  0.0263,  0.0386,  0.0285,  0.0361])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0369, -0.0322, -0.0262, -0.0403,  0.0386,  0.0300,  0.0365,  0.0149,\n",
      "        -0.0260,  0.0276,  0.0441,  0.0429,  0.0411,  0.0294,  0.0375,  0.0215,\n",
      "         0.0239])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0231, -0.0322, -0.0146,  0.0189, -0.0146, -0.0347, -0.0257, -0.0126,\n",
      "         0.0302, -0.0242, -0.0229, -0.0270,  0.0109, -0.0212, -0.0249, -0.0122,\n",
      "        -0.0207, -0.0225, -0.0299, -0.0387])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0277, -0.0452, -0.0490,  0.0398,  0.0392, -0.0388,  0.0214,  0.0300,\n",
      "         0.0402,  0.0509,  0.0251, -0.0314, -0.0392,  0.0232,  0.0315])\n",
      "Gradient of AB Loss for row 16: tensor([-0.5450])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0306, -0.0382, -0.0436, -0.0288, -0.0358,  0.0299, -0.0338, -0.0151,\n",
      "        -0.0343, -0.0277, -0.0298, -0.0347, -0.0247, -0.0247,  0.0377, -0.0245])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0445, -0.0426,  0.0607,  0.0510, -0.0395,  0.0379,  0.0482,  0.0437,\n",
      "         0.0646,  0.0355,  0.0307, -0.0430,  0.0512])\n",
      "Gradient of AB Loss for row 19: tensor([0.0428, 0.0573, 0.0240, 0.0199, 0.0409, 0.0261, 0.0280, 0.0445, 0.0538,\n",
      "        0.0315, 0.0523, 0.0371, 0.0336, 0.0284])\n",
      "Gradient of AB Loss for row 20: tensor([0.1440, 0.1690, 0.1137, 0.1219])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0284,  0.0346, -0.0456,  0.0385,  0.0174,  0.0425, -0.0358,  0.0343,\n",
      "         0.0423,  0.0347,  0.0453,  0.0177, -0.0346,  0.0292,  0.0184])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0178, -0.0311,  0.0299,  0.0108, -0.0449,  0.0373, -0.0241,  0.0313,\n",
      "         0.0451, -0.0463, -0.0440, -0.0332, -0.0363,  0.0322,  0.0337])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0417,  0.0371, -0.0278,  0.0436,  0.0181,  0.0280,  0.0468,  0.0486,\n",
      "         0.0462,  0.0446,  0.0378,  0.0344,  0.0201,  0.0317])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0256, -0.0367, -0.0242,  0.0374,  0.0236, -0.0269,  0.0372,  0.0413,\n",
      "         0.0275,  0.0388,  0.0414, -0.0254,  0.0288,  0.0271,  0.0371])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0545, -0.0377,  0.0499,  0.0497, -0.0391,  0.0367,  0.0276,  0.0309,\n",
      "         0.0592,  0.0266,  0.0342,  0.0243, -0.0324,  0.0217])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0291,  0.0396,  0.0301,  0.0508, -0.0462,  0.0349,  0.0267,  0.0419,\n",
      "         0.0498,  0.0455,  0.0453,  0.0302, -0.0305,  0.0383])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0249, -0.0331, -0.0365,  0.0381,  0.0181,  0.0232,  0.0386,  0.0444,\n",
      "         0.0312,  0.0257,  0.0415,  0.0329, -0.0251,  0.0258, -0.0299, -0.0189,\n",
      "         0.0198])\n",
      "Gradient of AB Loss for row 28: tensor([0.0390, 0.0441, 0.0222, 0.0352, 0.0429, 0.0433, 0.0347, 0.0378, 0.0491,\n",
      "        0.0384, 0.0434, 0.0435, 0.0231, 0.0294])\n",
      "Gradient of AB Loss for row 29: tensor([-0.6617])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0217, -0.0174, -0.0158, -0.0334, -0.0331, -0.0150,  0.0200, -0.0201,\n",
      "        -0.0196, -0.0330,  0.0262,  0.0230, -0.0286,  0.0217,  0.0305, -0.0176,\n",
      "        -0.0094, -0.0160, -0.0198,  0.0307, -0.0290, -0.0244])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0357, -0.0218, -0.0493,  0.0218,  0.0448,  0.0186,  0.0347,  0.0529,\n",
      "         0.0276,  0.0342,  0.0379, -0.0389,  0.0267, -0.0331])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.11771238595247269\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.10994258522987366\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.20243310928344727\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.1542432308197021\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.13609352707862854\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.2023133486509323\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.33902549743652344\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.13182055950164795\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.012937206774950027\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.14853435754776\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.6930956636684868e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.34085333347320557\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.08996209502220154\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.18514196574687958\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.0200748760253191\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.20466211438179016\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.0992208254734237e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3528883755207062\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.09146706014871597\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1727159321308136\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.019268805161118507\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.18688847124576569\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.1454961423851273e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3715085983276367\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09660350531339645\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.2709899842739105\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.028851095587015152\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.2627485394477844\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.5467143121838944e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.41777700185775757\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.10929065197706223\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.13402466475963593\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.01330788154155016\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.12885689735412598\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.425536910737435e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.372014582157135\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.09794390946626663\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.14138968288898468\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.013759877532720566\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.14604821801185608\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 2.227863005188624e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3436596095561981\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09147019684314728\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.18394789099693298\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.019419532269239426\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.20127007365226746\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.50267787102365e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3972730338573456\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.10367267578840256\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.16443821787834167\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.01439028512686491\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.16556258499622345\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.1009799738559423e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3696853816509247\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.09784445911645889\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 3.0300798416137695\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4659215211868286\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.05110787972807884\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.07471082359552383\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.4471174478530884\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.05642301216721535\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.45584049820899963\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1685829609632492\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.181012749671936\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.13303498923778534\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.20652978122234344\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.3060501217842102\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.09285695850849152\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.008679460734128952\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.09298635274171829\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.1074100525476638e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.35329580307006836\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.08052206039428711\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.10772913694381714\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.010133949108421803\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.11138205230236053\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.036449201081041e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.38972142338752747\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.09153731167316437\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08749552071094513\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.00800802931189537\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08979968726634979\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 9.61920099129543e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.42680227756500244\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.09031876921653748\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09650027006864548\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.008515667170286179\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.09703804552555084\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.6402394997783176e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.38428592681884766\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.08979468792676926\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08562979102134705\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.007907002232968807\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.08012764155864716\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 8.669001072547644e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3852287828922272\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.08804494142532349\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.1049373522400856\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.00916388537734747\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.11520755290985107\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.3504117823970319e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3628152012825012\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.08470761775970459\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.09001410752534866\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.008289927616715431\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08786847442388535\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.5653535134774188e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3624723553657532\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.08593001216650009\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.09034503996372223\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.009143540635704994\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.08273352682590485\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.261663020370236e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.33814987540245056\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.0761716291308403\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 3.1964683532714844\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.42237699031829834\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.06357891112565994\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.0778162032365799\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.46329689025878906\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.052147917449474335\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.44769901037216187\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.15690813958644867\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.370333194732666\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.27367520332336426\n",
      "ab_loss: 23.185611724853516\n",
      "Gradient of AB Loss for row 0: tensor([-0.0301, -0.0198, -0.0187, -0.0223, -0.0414, -0.0198,  0.0222, -0.0272,\n",
      "        -0.0232, -0.0285,  0.0152,  0.0240,  0.0253, -0.0238,  0.0257, -0.0109,\n",
      "        -0.0194, -0.0115,  0.0261, -0.0212,  0.0230])\n",
      "Gradient of AB Loss for row 1: tensor([0.0307, 0.0414, 0.0118, 0.0295, 0.0323, 0.0350, 0.0239, 0.0285, 0.0505,\n",
      "        0.0444, 0.0390, 0.0197, 0.0218, 0.0298])\n",
      "Gradient of AB Loss for row 2: tensor([0.0376, 0.0421, 0.0212, 0.0357, 0.0432, 0.0257, 0.0363, 0.0444, 0.0320,\n",
      "        0.0366, 0.0453, 0.0305, 0.0293, 0.0250])\n",
      "Gradient of AB Loss for row 3: tensor([-0.4066])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0445,  0.0386,  0.0160,  0.0365, -0.0388,  0.0307,  0.0210,  0.0292,\n",
      "         0.0460,  0.0499,  0.0450,  0.0254, -0.0343,  0.0231])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0481,  0.0410,  0.0123,  0.0414,  0.0449,  0.0327,  0.0349,  0.0393,\n",
      "         0.0435,  0.0578,  0.0409,  0.0297, -0.0363,  0.0380])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0349,  0.0450,  0.0169,  0.0441, -0.0417,  0.0288,  0.0328,  0.0230,\n",
      "         0.0357,  0.0405,  0.0203,  0.0291, -0.0406,  0.0228])\n",
      "Gradient of AB Loss for row 7: tensor([ 0.1442,  0.1035, -0.0996,  0.1010])\n",
      "Gradient of AB Loss for row 8: tensor([-0.6157])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0370,  0.0507,  0.0197,  0.0300,  0.0521,  0.0390,  0.0381,  0.0408,\n",
      "         0.0535,  0.0319,  0.0382,  0.0251, -0.0355,  0.0421])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0430, -0.0313, -0.0424, -0.0305,  0.0325,  0.0268, -0.0390,  0.0392,\n",
      "         0.0413,  0.0322,  0.0386,  0.0431, -0.0339,  0.0280, -0.0213])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0260,  0.0386,  0.0416,  0.0234,  0.0300,  0.0458,  0.0461,  0.0388,\n",
      "         0.0474,  0.0311,  0.0306,  0.0320,  0.0294,  0.0345])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0409,  0.0494,  0.0149,  0.0473, -0.0379,  0.0393,  0.0375,  0.0402,\n",
      "         0.0483,  0.0440,  0.0368,  0.0272, -0.0363,  0.0341])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0179, -0.0235, -0.0277, -0.0348,  0.0367,  0.0339,  0.0321,  0.0134,\n",
      "        -0.0295,  0.0285,  0.0401,  0.0397,  0.0418,  0.0286,  0.0313, -0.0342,\n",
      "        -0.0394,  0.0232])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0192, -0.0247, -0.0247,  0.0189, -0.0159, -0.0312, -0.0214, -0.0142,\n",
      "         0.0211, -0.0226,  0.0282, -0.0157, -0.0168,  0.0161, -0.0134, -0.0193,\n",
      "         0.0194, -0.0181, -0.0137, -0.0066,  0.0189, -0.0269,  0.0194])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0335, -0.0406, -0.0376,  0.0254, -0.0167,  0.0343, -0.0383,  0.0287,\n",
      "        -0.0400,  0.0404,  0.0287,  0.0227, -0.0395, -0.0287, -0.0361])\n",
      "Gradient of AB Loss for row 16: tensor([0.0274, 0.0495, 0.0190, 0.0405, 0.0537, 0.0318, 0.0480, 0.0451, 0.0496,\n",
      "        0.0401, 0.0458, 0.0300, 0.0324, 0.0318])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0473, -0.0472, -0.0464,  0.0355,  0.0235, -0.0196,  0.0295,  0.0321,\n",
      "         0.0445,  0.0261,  0.0347,  0.0404,  0.0323, -0.0271])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0259, -0.0272, -0.0361,  0.0162, -0.0224,  0.0253, -0.0373,  0.0341,\n",
      "        -0.0378,  0.0234, -0.0327,  0.0226, -0.0319, -0.0267, -0.0193])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0409, -0.0546, -0.0551, -0.0515,  0.0321, -0.0492,  0.0428,  0.0334,\n",
      "         0.0277, -0.0370,  0.0437, -0.0361,  0.0485])\n",
      "Gradient of AB Loss for row 20: tensor([-0.3472])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0279,  0.0155, -0.0245, -0.0126, -0.0154, -0.0273, -0.0160,  0.0227,\n",
      "        -0.0325, -0.0214, -0.0228,  0.0250,  0.0163,  0.0228,  0.0206, -0.0128,\n",
      "         0.0218, -0.0156, -0.0132, -0.0130,  0.0258, -0.0259, -0.0340])\n",
      "Gradient of AB Loss for row 22: tensor([ 0.1743,  0.1135, -0.1591, -0.1087])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0388, -0.0595, -0.0384,  0.0472,  0.0352, -0.0335,  0.0328,  0.0414,\n",
      "         0.0388,  0.0464,  0.0334, -0.0484, -0.0367,  0.0218])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0410, -0.0516,  0.0411,  0.0492,  0.0262,  0.0412,  0.0393,  0.0487,\n",
      "         0.0408,  0.0265,  0.0403,  0.0426,  0.0303,  0.0334])\n",
      "Gradient of AB Loss for row 25: tensor([0.0439, 0.0378, 0.0182, 0.0367, 0.0509, 0.0525, 0.0311, 0.0625, 0.0377,\n",
      "        0.0307, 0.0460])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0375, -0.0216,  0.0269, -0.0243, -0.0353, -0.0247, -0.0178,  0.0284,\n",
      "        -0.0309, -0.0251,  0.0222,  0.0217, -0.0266,  0.0265, -0.0145, -0.0155,\n",
      "        -0.0239,  0.0337,  0.0201, -0.0342, -0.0214])\n",
      "Gradient of AB Loss for row 27: tensor([0.0362, 0.0486, 0.0297, 0.0249, 0.0374, 0.0251, 0.0305, 0.0351, 0.0414,\n",
      "        0.0499, 0.0413, 0.0230, 0.0345, 0.0364])\n",
      "Gradient of AB Loss for row 28: tensor([ 0.0337, -0.0466,  0.0491,  0.0170,  0.0363,  0.0292,  0.0385,  0.0327,\n",
      "         0.0531,  0.0437,  0.0410,  0.0271,  0.0384,  0.0341])\n",
      "Gradient of AB Loss for row 29: tensor([ 0.0285, -0.0433,  0.0508,  0.0080,  0.0316, -0.0321,  0.0282,  0.0230,\n",
      "         0.0404,  0.0492, -0.0264,  0.0250, -0.0207,  0.0332,  0.0286])\n",
      "Gradient of AB Loss for row 30: tensor([0.1311, 0.1314, 0.1048, 0.1322])\n",
      "Gradient of AB Loss for row 31: tensor([0.1064, 0.2009, 0.1124, 0.1073])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08534613251686096\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.07746580988168716\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.15661907196044922\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.8932550549507141\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10107887536287308\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.15006083250045776\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2562105357646942\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.13233712315559387\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.014612019062042236\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.13940519094467163\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.5653855989228305e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.2975563108921051\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07696505635976791\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.11872214078903198\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.01620432175695896\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.13417726755142212\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.2536088833670078e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.29796260595321655\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07814282178878784\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1366824507713318\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.015383167192339897\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.15855981409549713\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 9.441318837843937e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3371107578277588\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.08890102803707123\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.11593777686357498\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.012007800862193108\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.11353150010108948\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.359656942589993e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.31812039017677307\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07902741432189941\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.10765916109085083\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.00960052665323019\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10214442759752274\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.4938682513232493e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.342217355966568\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08324534446001053\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.12069421261548996\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.017666110768914223\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.11567013710737228\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.129616955530821e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.32057714462280273\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.07707662135362625\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.09537892788648605\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.010321049951016903\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.10037106275558472\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 2.0522428201275034e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3128125071525574\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07943065464496613\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.13870523869991302\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.011316204443573952\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.13927321135997772\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.110344372001748e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.33193251490592957\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07972585409879684\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.5855491161346436\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3894745409488678\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.045388009399175644\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.05926673859357834\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3169787526130676\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.038469891995191574\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.34838417172431946\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1288662999868393\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.8914439678192139\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09461960941553116\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.15637435019016266\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.21311289072036743\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06460300832986832\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.005076874978840351\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.06498545408248901\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.2326026865849826e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2558665871620178\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05251327157020569\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.07146584987640381\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006089598871767521\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.07486807554960251\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 6.448493405919464e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.2704813480377197\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.058814264833927155\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07566609978675842\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.00642008101567626\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07137095928192139\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.1287040191376718e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3257215619087219\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06400390714406967\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.0616568960249424\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.005386035423725843\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.061210136860609055\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.2431397022893975e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.28011447191238403\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.060399070382118225\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.057308316230773926\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.004598366562277079\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.05633075535297394\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.1619146755847964e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2422201931476593\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.04709519445896149\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08317126333713531\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.007532054092735052\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.08533812314271927\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.274369054726776e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2618476152420044\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.058969639241695404\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.07329466193914413\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.006837691180408001\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07254167646169662\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.393904436319815e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.2642320692539215\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.057623472064733505\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.06918126344680786\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.006410672795027494\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.06523589789867401\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.2694358897391567e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2612513601779938\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.05437430366873741\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.343156576156616\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.2944721579551697\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.048552971333265305\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.053669240325689316\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3142249882221222\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03239171579480171\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3161783516407013\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.10797875374555588\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.8050614595413208\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2088773548603058\n",
      "ab_loss: 24.222545623779297\n",
      "Gradient of AB Loss for row 0: tensor([ 0.0470,  0.0273,  0.0436,  0.0406,  0.0357,  0.0438,  0.0309,  0.0451,\n",
      "         0.0625,  0.0364,  0.0283, -0.0352,  0.0235])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0283, -0.0380,  0.0326,  0.0291,  0.0435, -0.0355,  0.0365,  0.0301,\n",
      "         0.0329,  0.0465,  0.0230, -0.0351, -0.0281,  0.0405,  0.0276])\n",
      "Gradient of AB Loss for row 2: tensor([0.0350, 0.0466, 0.0138, 0.0243, 0.0285, 0.0462, 0.0511, 0.0432, 0.0362,\n",
      "        0.0512, 0.0477, 0.0300, 0.0359, 0.0325])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0403, -0.0453, -0.0432, -0.0342, -0.0368,  0.0395, -0.0237,  0.0300,\n",
      "         0.0378,  0.0409,  0.0308,  0.0423,  0.0342, -0.0324, -0.0294])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0344, -0.0262, -0.0309, -0.0227, -0.0251,  0.0302,  0.0362, -0.0190,\n",
      "        -0.0267,  0.0103,  0.0386, -0.0164, -0.0189, -0.0254,  0.0346,  0.0310,\n",
      "        -0.0334, -0.0301])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0376, -0.0234,  0.0285, -0.0320,  0.0120, -0.0361,  0.0437, -0.0418,\n",
      "         0.0343, -0.0270,  0.0352,  0.0374,  0.0368, -0.0343, -0.0176])\n",
      "Gradient of AB Loss for row 6: tensor([0.0421, 0.0392, 0.0162, 0.0371, 0.0543, 0.0282, 0.0473, 0.0309, 0.0362,\n",
      "        0.0490, 0.0272, 0.0397, 0.0406, 0.0345])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0456, -0.0248, -0.0310,  0.0462,  0.0341,  0.0212, -0.0332,  0.0459,\n",
      "         0.0484,  0.0324,  0.0363,  0.0386, -0.0339,  0.0307,  0.0220])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0341, -0.0351,  0.0457,  0.0232,  0.0433, -0.0409,  0.0283,  0.0319,\n",
      "         0.0429,  0.0397,  0.0269, -0.0297, -0.0390,  0.0212,  0.0416])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0552, -0.0463,  0.0560,  0.0388, -0.0382,  0.0453,  0.0457,  0.0436,\n",
      "         0.0538,  0.0440,  0.0420,  0.0383, -0.0329,  0.0246])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0374, -0.0496,  0.0281,  0.0345,  0.0237,  0.0385,  0.0388,  0.0364,\n",
      "         0.0264,  0.0542,  0.0302,  0.0382, -0.0378,  0.0391])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0378,  0.0453,  0.0273,  0.0426, -0.0426,  0.0406,  0.0290,  0.0386,\n",
      "         0.0373,  0.0476,  0.0342,  0.0259, -0.0334,  0.0301])\n",
      "Gradient of AB Loss for row 12: tensor([0.0345, 0.0397, 0.0171, 0.0237, 0.0334, 0.0393, 0.0314, 0.0379, 0.0527,\n",
      "        0.0356, 0.0433, 0.0405, 0.0351, 0.0372])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0320, -0.0556, -0.0509,  0.0466,  0.0389, -0.0405,  0.0445,  0.0340,\n",
      "         0.0519,  0.0376,  0.0296,  0.0241, -0.0280,  0.0371])\n",
      "Gradient of AB Loss for row 14: tensor([-0.5992])\n",
      "Gradient of AB Loss for row 15: tensor([0.0409, 0.0481, 0.0322, 0.0207, 0.0436, 0.0386, 0.0388, 0.0464, 0.0468,\n",
      "        0.0329, 0.0362, 0.0340, 0.0275, 0.0218])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0318, -0.0370, -0.0351, -0.0357, -0.0299,  0.0267, -0.0391,  0.0259,\n",
      "         0.0400,  0.0247,  0.0331,  0.0358,  0.0347,  0.0276, -0.0318,  0.0232,\n",
      "        -0.0242])\n",
      "Gradient of AB Loss for row 17: tensor([ 0.1507,  0.1174, -0.1141,  0.1633])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0262, -0.0310, -0.0248, -0.0429, -0.0316,  0.0413,  0.0134, -0.0220,\n",
      "         0.0462,  0.0466,  0.0452,  0.0415,  0.0373, -0.0192,  0.0192, -0.0279])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0215, -0.0282, -0.0382,  0.0137, -0.0207,  0.0338, -0.0305,  0.0258,\n",
      "         0.0328,  0.0391,  0.0181, -0.0315, -0.0408, -0.0180,  0.0184])\n",
      "Gradient of AB Loss for row 20: tensor([0.1100, 0.1274, 0.1506, 0.1469])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0362, -0.0291, -0.0322, -0.0309,  0.0460,  0.0222,  0.0261, -0.0271,\n",
      "         0.0191,  0.0365,  0.0390,  0.0230, -0.0293, -0.0280, -0.0390,  0.0214])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0306,  0.0390,  0.0167,  0.0415, -0.0309,  0.0348,  0.0448,  0.0279,\n",
      "         0.0420,  0.0423,  0.0404,  0.0311, -0.0376,  0.0238])\n",
      "Gradient of AB Loss for row 23: tensor([0.0395, 0.0488, 0.0255, 0.0305, 0.0513, 0.0299, 0.0282, 0.0262, 0.0480,\n",
      "        0.0289, 0.0383, 0.0416, 0.0205, 0.0288])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0308, -0.0143, -0.0425, -0.0540, -0.0299,  0.0452, -0.0308,  0.0341,\n",
      "         0.0242,  0.0379,  0.0465, -0.0341, -0.0314, -0.0382])\n",
      "Gradient of AB Loss for row 25: tensor([-0.6244])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0299, -0.0412,  0.0426,  0.0222,  0.0231, -0.0368,  0.0238,  0.0441,\n",
      "         0.0364,  0.0401,  0.0428, -0.0377, -0.0325,  0.0319])\n",
      "Gradient of AB Loss for row 27: tensor([0.0282, 0.0571, 0.0311, 0.0203, 0.0399, 0.0223, 0.0340, 0.0249, 0.0398,\n",
      "        0.0386, 0.0296, 0.0389, 0.0264, 0.0441])\n",
      "Gradient of AB Loss for row 28: tensor([0.0252, 0.0443, 0.0190, 0.0422, 0.0368, 0.0360, 0.0448, 0.0245, 0.0319,\n",
      "        0.0572, 0.0367, 0.0331, 0.0354, 0.0404])\n",
      "Gradient of AB Loss for row 29: tensor([0.0410, 0.0288, 0.0155, 0.0254, 0.0416, 0.0389, 0.0526, 0.0448, 0.0342,\n",
      "        0.0462, 0.0406, 0.0224, 0.0333, 0.0232])\n",
      "Gradient of AB Loss for row 30: tensor([0.0346, 0.0394, 0.0148, 0.0275, 0.0299, 0.0445, 0.0418, 0.0330, 0.0546,\n",
      "        0.0505, 0.0503, 0.0245, 0.0295, 0.0428])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0466, -0.0306, -0.0371, -0.0220, -0.0497,  0.0290, -0.0308, -0.0370,\n",
      "        -0.0339,  0.0398,  0.0456, -0.0437,  0.0309])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10209448635578156\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.10721637308597565\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.16165049374103546\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0148690938949585\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10495784133672714\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.20618608593940735\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.25770652294158936\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.16008049249649048\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.012643871828913689\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.17325890064239502\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.4187420127598216e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3299286365509033\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.0714072659611702\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.1458299607038498\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.011094450950622559\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.15500101447105408\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.0356593360683064e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3282722234725952\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07159886509180069\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.17578664422035217\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.0152498884126544\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.1822921633720398\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.4279099014302687e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.36036771535873413\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.07611465454101562\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.1383845955133438\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.012700362130999565\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.14024704694747925\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.3458002490196463e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.32809939980506897\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.06893523037433624\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.1490987092256546\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.013713509775698185\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.14457572996616364\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.420423112463709e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3397027254104614\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07303903996944427\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.16494113206863403\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.013725299388170242\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.16558921337127686\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.421040951576913e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.33244696259498596\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.06627726554870605\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.17283818125724792\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.016406016424298286\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.18445169925689697\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 2.5978852402630537e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3366491496562958\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.06605877727270126\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.16730225086212158\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.012160676531493664\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.15925975143909454\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.5576410161699528e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.33912935853004456\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07110123336315155\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.6995725631713867\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.33798912167549133\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04880766570568085\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.05493507534265518\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.400361567735672\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.044278569519519806\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.41484469175338745\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.13246238231658936\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0998351573944092\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.11178813129663467\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.2131997048854828\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.25513318181037903\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08744711428880692\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.007883659563958645\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.08525404334068298\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 9.192879235619955e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.3148026168346405\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05880597233772278\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09214004129171371\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.008707609958946705\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09581934660673141\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.921899802809662e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.31149977445602417\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.05519073083996773\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.10810630768537521\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.008649391122162342\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.10541835427284241\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 9.901941488976718e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.34286776185035706\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06274393945932388\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09978406131267548\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.009454199112951756\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.10219909250736237\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.5955600174422102e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3546205163002014\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07102151960134506\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08113264292478561\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.006525521166622639\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.07474950700998306\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.41550349219699e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.29661795496940613\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.05202430859208107\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.10644660145044327\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.008246760815382004\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.11838701367378235\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 9.510984222416141e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.29663100838661194\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.055025599896907806\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.098195381462574\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.008478500880300999\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.0975353866815567\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.604077922884755e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3250727951526642\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06257713586091995\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.11022572219371796\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.009034219197928905\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.10767048597335815\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.4330097108938844e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.31466516852378845\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.058389559388160706\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.8052690029144287\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3239823579788208\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.06053895130753517\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06074651703238487\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.41392073035240173\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.042493078857660294\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.41931796073913574\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.13633690774440765\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.191127061843872\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2425176203250885\n",
      "ab_loss: 24.22907066345215\n",
      "Gradient of AB Loss for row 0: tensor([0.1044, 0.1764, 0.1421, 0.1155])\n",
      "Gradient of AB Loss for row 1: tensor([0.0323, 0.0427, 0.0246, 0.0255, 0.0375, 0.0336, 0.0383, 0.0295, 0.0492,\n",
      "        0.0288, 0.0302, 0.0399, 0.0262, 0.0162])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0316, -0.0377,  0.0266,  0.0427, -0.0499,  0.0452,  0.0462,  0.0449,\n",
      "         0.0385,  0.0347,  0.0275, -0.0375, -0.0265,  0.0309])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0226, -0.0232, -0.0501, -0.0279, -0.0338, -0.0396,  0.0271, -0.0270,\n",
      "         0.0233,  0.0483,  0.0399,  0.0311,  0.0348, -0.0358, -0.0254, -0.0296,\n",
      "        -0.0189])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0457,  0.0464,  0.0212,  0.0311,  0.0450,  0.0264,  0.0439,  0.0370,\n",
      "         0.0466,  0.0298,  0.0448, -0.0284, -0.0221,  0.0391])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0362,  0.0435,  0.0418,  0.0218,  0.0327,  0.0437,  0.0500,  0.0482,\n",
      "         0.0461,  0.0393,  0.0414,  0.0312,  0.0327,  0.0254])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0349, -0.0232, -0.0253, -0.0335, -0.0334, -0.0283,  0.0178, -0.0267,\n",
      "        -0.0283, -0.0310,  0.0205,  0.0301,  0.0295, -0.0259,  0.0304, -0.0179,\n",
      "        -0.0172,  0.0245,  0.0329, -0.0252, -0.0271])\n",
      "Gradient of AB Loss for row 7: tensor([-0.4578])\n",
      "Gradient of AB Loss for row 8: tensor([0.0334, 0.0443, 0.0204, 0.0389, 0.0495, 0.0276, 0.0279, 0.0362, 0.0366,\n",
      "        0.0463, 0.0416, 0.0332, 0.0328, 0.0389])\n",
      "Gradient of AB Loss for row 9: tensor([ 0.0356, -0.0425,  0.0358,  0.0084,  0.0304,  0.0338,  0.0357,  0.0472,\n",
      "         0.0411,  0.0455,  0.0494,  0.0397,  0.0370,  0.0442])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0214, -0.0245, -0.0286, -0.0407, -0.0307, -0.0344,  0.0391,  0.0323,\n",
      "        -0.0108,  0.0323,  0.0414,  0.0410,  0.0380, -0.0274, -0.0331,  0.0273])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0280, -0.0297,  0.0278, -0.0286, -0.0345, -0.0350, -0.0199, -0.0252,\n",
      "         0.0259, -0.0269, -0.0288, -0.0175, -0.0203, -0.0271, -0.0185, -0.0154,\n",
      "        -0.0141,  0.0287,  0.0289, -0.0314,  0.0200])\n",
      "Gradient of AB Loss for row 12: tensor([-0.6113])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0551, -0.0358,  0.0504,  0.0281,  0.0374,  0.0315,  0.0437,  0.0415,\n",
      "         0.0355,  0.0494,  0.0297, -0.0409,  0.0315])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0257, -0.0395, -0.0399,  0.0411,  0.0341, -0.0432,  0.0313,  0.0419,\n",
      "         0.0487,  0.0524,  0.0341,  0.0358, -0.0371,  0.0283])\n",
      "Gradient of AB Loss for row 15: tensor([ 0.0328, -0.0414,  0.0550,  0.0201,  0.0304,  0.0450,  0.0418,  0.0455,\n",
      "         0.0470,  0.0602,  0.0399,  0.0454,  0.0352,  0.0231])\n",
      "Gradient of AB Loss for row 16: tensor([0.0279, 0.0279, 0.0272, 0.0338, 0.0461, 0.0436, 0.0333, 0.0392, 0.0401,\n",
      "        0.0414, 0.0396, 0.0469, 0.0367, 0.0229])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0289, -0.0283,  0.0161, -0.0265, -0.0270,  0.0207,  0.0213, -0.0247,\n",
      "        -0.0328,  0.0104,  0.0207,  0.0235, -0.0263,  0.0169, -0.0128, -0.0144,\n",
      "        -0.0174,  0.0347,  0.0150, -0.0255,  0.0177])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0347,  0.0477, -0.0363,  0.0425,  0.0150,  0.0435, -0.0419,  0.0469,\n",
      "         0.0357,  0.0410, -0.0228,  0.0307, -0.0386,  0.0354,  0.0191])\n",
      "Gradient of AB Loss for row 19: tensor([ 0.0196, -0.0234,  0.0334, -0.0269,  0.0272,  0.0323,  0.0234, -0.0177,\n",
      "         0.0279,  0.0266,  0.0145,  0.0364,  0.0216,  0.0089,  0.0224,  0.0289,\n",
      "         0.0281,  0.0273, -0.0258,  0.0275,  0.0205])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0443,  0.0380, -0.0325,  0.0434,  0.0216,  0.0275,  0.0434,  0.0382,\n",
      "         0.0495,  0.0368,  0.0282,  0.0413,  0.0305,  0.0305])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0209, -0.0326,  0.0309,  0.0251,  0.0386, -0.0431,  0.0316,  0.0410,\n",
      "         0.0322,  0.0408,  0.0213, -0.0404, -0.0308,  0.0218,  0.0267])\n",
      "Gradient of AB Loss for row 22: tensor([0.0359, 0.0379, 0.0216, 0.0368, 0.0399, 0.0307, 0.0456, 0.0267, 0.0577,\n",
      "        0.0379, 0.0311, 0.0353, 0.0352])\n",
      "Gradient of AB Loss for row 23: tensor([0.0375, 0.0482, 0.0170, 0.0204, 0.0434, 0.0298, 0.0302, 0.0351, 0.0512,\n",
      "        0.0419, 0.0362, 0.0234, 0.0251, 0.0452])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0262, -0.0580, -0.0489,  0.0406,  0.0400, -0.0446,  0.0455,  0.0359,\n",
      "         0.0494,  0.0503,  0.0431,  0.0250, -0.0376,  0.0288])\n",
      "Gradient of AB Loss for row 25: tensor([0.0361, 0.0463, 0.0162, 0.0347, 0.0259, 0.0345, 0.0374, 0.0399, 0.0493,\n",
      "        0.0263, 0.0411, 0.0357, 0.0187, 0.0251])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0198, -0.0246, -0.0155, -0.0148, -0.0236, -0.0249, -0.0170, -0.0174,\n",
      "         0.0211, -0.0180, -0.0244, -0.0176, -0.0225,  0.0153, -0.0230, -0.0202,\n",
      "        -0.0149, -0.0168, -0.0121, -0.0201,  0.0210, -0.0243, -0.0209])\n",
      "Gradient of AB Loss for row 27: tensor([0.0315, 0.0384, 0.0211, 0.0324, 0.0272, 0.0392, 0.0382, 0.0466, 0.0429,\n",
      "        0.0468, 0.0421, 0.0251, 0.0372, 0.0551])\n",
      "Gradient of AB Loss for row 28: tensor([0.0403, 0.0378, 0.0221, 0.0304, 0.0467, 0.0309, 0.0288, 0.0349, 0.0483,\n",
      "        0.0433, 0.0456, 0.0266, 0.0393, 0.0529])\n",
      "Gradient of AB Loss for row 29: tensor([0.1342, 0.1329, 0.1735, 0.1427])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0879, -0.1225, -0.1056, -0.1771])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0344,  0.0316,  0.0372,  0.0310,  0.0332,  0.0288,  0.0287,  0.0497,\n",
      "         0.0404,  0.0369,  0.0308, -0.0380, -0.0381,  0.0361])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10198379307985306\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.09730006754398346\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1697862595319748\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9689406752586365\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11037589609622955\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.18857067823410034\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.27370280027389526\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.12998464703559875\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.012812168337404728\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.14705055952072144\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.7386235784400128e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.2618747353553772\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07077928632497787\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.109235979616642\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.009462841786444187\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.11517773568630219\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 8.545831264861192e-10\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2845979332923889\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07699710130691528\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.13768276572227478\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.013326628133654594\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.16568157076835632\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.13432196968688e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.31771016120910645\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.0830356627702713\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.14071276783943176\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.011839873157441616\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.1421220302581787\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.0306103748192186e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3396414518356323\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.08415727317333221\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.08766969293355942\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.010402737185359001\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.08896657824516296\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 9.175745163680915e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.33844077587127686\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.0854324921965599\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.10561481863260269\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.011990202590823174\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.10270024091005325\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 9.109970000586998e-10\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3504548668861389\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.0840635821223259\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.12971946597099304\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.010098839178681374\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.14456039667129517\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.5137301412337933e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3185659945011139\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.0754900574684143\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.13084018230438232\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.011304439045488834\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.12771201133728027\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.0858792753865032e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3348550796508789\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.0749000534415245\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.6287548542022705\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.38922280073165894\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.046438802033662796\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06069573760032654\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.371013879776001\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04592422768473625\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.39209675788879395\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.13572241365909576\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9802708029747009\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.10518921911716461\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1903940737247467\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2546294033527374\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06542816013097763\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.005583851132541895\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.06559909880161285\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 8.363320036508526e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2890687584877014\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.056936148554086685\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09377366304397583\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.008784587495028973\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09041493386030197\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 9.541883949637509e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.2700287401676178\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.05431417003273964\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.06762422621250153\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005987528711557388\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06492070108652115\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 9.38810251760458e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3489351272583008\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06722529977560043\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.07403074204921722\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.006343599408864975\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.076822929084301\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.3265826215302923e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.29879772663116455\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06350598484277725\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08100941777229309\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.008208873681724072\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.07206618785858154\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 9.936664824294894e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.29837682843208313\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06003090366721153\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.07256445288658142\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.00714748864993453\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.0798797458410263\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 6.28749496911496e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2591429650783539\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.055188581347465515\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.0830574780702591\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007897541858255863\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07567933946847916\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.7821045750210374e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.27553367614746094\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.05910542607307434\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.06761995702981949\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.005873754154890776\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07192467153072357\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.282419281878333e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.28335511684417725\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.05810185894370079\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.5318446159362793\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.29266679286956787\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.050605516880750656\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05504484102129936\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.34588703513145447\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03761056065559387\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3643495738506317\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.12831149995326996\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.140528440475464\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.24346548318862915\n",
      "ab_loss: 24.891271591186523\n",
      "Gradient of AB Loss for row 0: tensor([-0.0179, -0.0210, -0.0167, -0.0195, -0.0231, -0.0331, -0.0310, -0.0193,\n",
      "        -0.0135, -0.0148, -0.0207, -0.0195, -0.0260, -0.0187, -0.0218, -0.0152,\n",
      "        -0.0191, -0.0140, -0.0314,  0.0192, -0.0251, -0.0255, -0.0199])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0280, -0.0235, -0.0163, -0.0269, -0.0395, -0.0352, -0.0180, -0.0225,\n",
      "         0.0311, -0.0279, -0.0295, -0.0337,  0.0241,  0.0326,  0.0277, -0.0174,\n",
      "        -0.0212,  0.0244, -0.0285, -0.0315])\n",
      "Gradient of AB Loss for row 2: tensor([0.0370, 0.0398, 0.0371, 0.0299, 0.0438, 0.0338, 0.0452, 0.0410, 0.0385,\n",
      "        0.0390, 0.0335, 0.0467, 0.0206, 0.0255])\n",
      "Gradient of AB Loss for row 3: tensor([-0.6225])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0354, -0.0341,  0.0308,  0.0163, -0.0125,  0.0382,  0.0313,  0.0300,\n",
      "         0.0220,  0.0377,  0.0245,  0.0309,  0.0286,  0.0278, -0.0180,  0.0220,\n",
      "        -0.0149])\n",
      "Gradient of AB Loss for row 5: tensor([0.1603, 0.1467, 0.1315, 0.1424])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0379, -0.0457, -0.0471,  0.0431,  0.0412,  0.0363,  0.0366,  0.0396,\n",
      "         0.0520,  0.0475,  0.0451,  0.0311,  0.0309, -0.0376,  0.0309])\n",
      "Gradient of AB Loss for row 7: tensor([-0.5796])\n",
      "Gradient of AB Loss for row 8: tensor([0.0365, 0.0349, 0.0134, 0.0361, 0.0475, 0.0414, 0.0376, 0.0465, 0.0423,\n",
      "        0.0219, 0.0443, 0.0395, 0.0253, 0.0305])\n",
      "Gradient of AB Loss for row 9: tensor([0.0388, 0.0527, 0.0396, 0.0301, 0.0303, 0.0263, 0.0282, 0.0586, 0.0340,\n",
      "        0.0456, 0.0358, 0.0372, 0.0365])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0253, -0.0225, -0.0358, -0.0427, -0.0332, -0.0333,  0.0410,  0.0310,\n",
      "        -0.0370,  0.0274,  0.0330,  0.0222,  0.0411, -0.0276, -0.0310, -0.0252])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0338, -0.0291,  0.0507,  0.0187,  0.0440, -0.0417,  0.0288,  0.0408,\n",
      "         0.0383,  0.0528,  0.0344, -0.0298, -0.0265,  0.0302,  0.0226])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0309, -0.0191, -0.0275, -0.0374,  0.0336,  0.0262,  0.0219,  0.0155,\n",
      "        -0.0228, -0.0347,  0.0291,  0.0472,  0.0402,  0.0330,  0.0429,  0.0259,\n",
      "         0.0250])\n",
      "Gradient of AB Loss for row 13: tensor([0.0417, 0.0499, 0.0099, 0.0314, 0.0260, 0.0413, 0.0394, 0.0473, 0.0529,\n",
      "        0.0509, 0.0403, 0.0310, 0.0345, 0.0412])\n",
      "Gradient of AB Loss for row 14: tensor([-0.6127])\n",
      "Gradient of AB Loss for row 15: tensor([ 0.0309, -0.0472,  0.0422,  0.0203,  0.0224,  0.0265,  0.0485,  0.0430,\n",
      "         0.0409,  0.0459,  0.0551,  0.0506,  0.0408,  0.0375])\n",
      "Gradient of AB Loss for row 16: tensor([0.0300, 0.0361, 0.0180, 0.0259, 0.0363, 0.0375, 0.0294, 0.0351, 0.0534,\n",
      "        0.0305, 0.0367, 0.0277, 0.0404, 0.0256])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0256,  0.0247, -0.0433,  0.0386,  0.0162,  0.0384,  0.0339,  0.0340,\n",
      "         0.0463,  0.0506,  0.0388,  0.0235,  0.0223,  0.0298])\n",
      "Gradient of AB Loss for row 18: tensor([0.1026, 0.1722, 0.1288, 0.1006])\n",
      "Gradient of AB Loss for row 19: tensor([ 0.1181,  0.1348, -0.1309,  0.1266])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0344, -0.0249,  0.0316,  0.0230,  0.0248,  0.0425,  0.0236,  0.0434,\n",
      "         0.0313,  0.0339,  0.0368,  0.0420, -0.0302,  0.0265, -0.0225])\n",
      "Gradient of AB Loss for row 21: tensor([-0.5956])\n",
      "Gradient of AB Loss for row 22: tensor([ 0.0243, -0.0460,  0.0534,  0.0119,  0.0284,  0.0262,  0.0354,  0.0359,\n",
      "         0.0428,  0.0382,  0.0507,  0.0343,  0.0321,  0.0464])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0193, -0.0232, -0.0305, -0.0224,  0.0244,  0.0333, -0.0282, -0.0279,\n",
      "        -0.0302,  0.0341,  0.0169,  0.0281,  0.0241, -0.0133,  0.0211,  0.0333,\n",
      "        -0.0248,  0.0233,  0.0187, -0.0305, -0.0366])\n",
      "Gradient of AB Loss for row 24: tensor([0.1509, 0.1402, 0.1783, 0.0975])\n",
      "Gradient of AB Loss for row 25: tensor([0.0424, 0.0467, 0.0396, 0.0396, 0.0498, 0.0321, 0.0272, 0.0381, 0.0578,\n",
      "        0.0333, 0.0329, 0.0306, 0.0261, 0.0292])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0321, -0.0406, -0.0395,  0.0182, -0.0203,  0.0336, -0.0420,  0.0338,\n",
      "        -0.0340,  0.0504,  0.0278,  0.0495,  0.0265, -0.0425, -0.0247])\n",
      "Gradient of AB Loss for row 27: tensor([ 0.0470, -0.0392,  0.0491,  0.0322,  0.0208,  0.0339,  0.0495,  0.0327,\n",
      "         0.0389,  0.0488,  0.0479,  0.0365,  0.0344,  0.0263])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0286, -0.0160, -0.0238, -0.0353, -0.0235,  0.0248,  0.0328, -0.0267,\n",
      "        -0.0207, -0.0298,  0.0269,  0.0170,  0.0268,  0.0302, -0.0134,  0.0217,\n",
      "        -0.0122, -0.0098,  0.0234,  0.0252, -0.0263, -0.0293])\n",
      "Gradient of AB Loss for row 29: tensor([0.0286, 0.0361, 0.0255, 0.0375, 0.0348, 0.0245, 0.0398, 0.0326, 0.0196,\n",
      "        0.0235, 0.0310, 0.0297, 0.0200, 0.0382])\n",
      "Gradient of AB Loss for row 30: tensor([-0.5161])\n",
      "Gradient of AB Loss for row 31: tensor([0.0299, 0.0359, 0.0237, 0.0350, 0.0360, 0.0339, 0.0447, 0.0431, 0.0354,\n",
      "        0.0449, 0.0547, 0.0326, 0.0388, 0.0300])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09439235925674438\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.08301275223493576\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.14225108921527863\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.8892154097557068\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.09380677342414856\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.16859383881092072\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2362729161977768\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.13523630797863007\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.012113407254219055\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.14845071732997894\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.3077587901477727e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.270073264837265\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.06176697090268135\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.12709863483905792\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.011890674009919167\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.1482032984495163\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.6786142476021837e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2778428792953491\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.06549765169620514\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.11142440885305405\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.010425584390759468\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.11858969926834106\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 9.880145590557277e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.28414055705070496\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.062732994556427\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.1672152280807495\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.012942133471369743\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.16679233312606812\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.1121712439887688e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3046497404575348\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07212431728839874\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11654258519411087\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.00897329393774271\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.122056744992733\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.5623180527057912e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3199811577796936\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.06782599538564682\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.11227115243673325\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.010622224770486355\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.10792593657970428\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.1629343044106122e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.29193058609962463\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.062101177871227264\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.13436177372932434\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01627952978014946\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.14609616994857788\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 2.0236514686189366e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3043888509273529\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07031157612800598\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.18367138504981995\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.012327497825026512\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.18759164214134216\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.26909249775764e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3092415928840637\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.06330563873052597\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.3945679664611816\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.30745887756347656\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.0380530059337616\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.04818905144929886\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3737395107746124\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.042170800268650055\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.37300947308540344\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.12326528877019882\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9587987065315247\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09661787003278732\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.17163614928722382\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.22659775614738464\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06742695719003677\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.0054606422781944275\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.0697125792503357\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.2244766312008437e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.3061722218990326\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.053022220730781555\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.07840792834758759\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.00826954934746027\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.0809592604637146\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.340281715784158e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.2736908793449402\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.053310565650463104\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07161101698875427\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006782914511859417\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.0698927491903305\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.0038737618955906e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.29092898964881897\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.050360292196273804\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09294087439775467\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.009205805137753487\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.09419822692871094\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 9.680731771766204e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.2862668037414551\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.05621204152703285\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.07221955060958862\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.006567591335624456\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.07115305215120316\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.3255476716267367e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3021906614303589\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.055961281061172485\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.09091658890247345\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.009308124892413616\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09229442477226257\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.4236812839740765e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2843412756919861\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.05739503353834152\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08320207893848419\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.0073441858403384686\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07944051921367645\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.3239432883338509e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.2649892270565033\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.05169122666120529\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07414764910936356\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.005854516290128231\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07371509820222855\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 9.801910394457991e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.273206502199173\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.0525335893034935\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.414203643798828\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.26861265301704407\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05056407302618027\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.049591612070798874\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.35574907064437866\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03584854304790497\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3666427433490753\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11633200198411942\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.9809703826904297\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.216446191072464\n",
      "ab_loss: 23.452335357666016\n",
      "Gradient of AB Loss for row 0: tensor([0.0387, 0.0407, 0.0307, 0.0328, 0.0411, 0.0479, 0.0462, 0.0498, 0.0429,\n",
      "        0.0439, 0.0453, 0.0326, 0.0331, 0.0227])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0217,  0.0252, -0.0309,  0.0314,  0.0120, -0.0188, -0.0130,  0.0221,\n",
      "        -0.0203, -0.0120, -0.0247,  0.0265,  0.0166, -0.0147, -0.0178,  0.0226,\n",
      "         0.0150,  0.0226, -0.0179,  0.0277,  0.0293,  0.0162, -0.0239,  0.0218])\n",
      "Gradient of AB Loss for row 2: tensor([-0.3901])\n",
      "Gradient of AB Loss for row 3: tensor([0.1275, 0.1537, 0.1179, 0.1371])\n",
      "Gradient of AB Loss for row 4: tensor([-0.5451])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0317,  0.0354, -0.0432,  0.0518,  0.0227,  0.0390,  0.0302,  0.0404,\n",
      "         0.0411,  0.0577,  0.0464,  0.0355,  0.0410,  0.0300])\n",
      "Gradient of AB Loss for row 6: tensor([0.0264, 0.0550, 0.0141, 0.0356, 0.0289, 0.0355, 0.0308, 0.0404, 0.0439,\n",
      "        0.0403, 0.0505, 0.0207, 0.0266, 0.0460])\n",
      "Gradient of AB Loss for row 7: tensor([0.1813, 0.2127, 0.1482])\n",
      "Gradient of AB Loss for row 8: tensor([ 0.0263, -0.0346,  0.0523,  0.0455,  0.0250,  0.0452,  0.0416,  0.0189,\n",
      "         0.0424,  0.0447,  0.0420,  0.0390,  0.0235,  0.0138])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0399, -0.0322, -0.0351, -0.0252, -0.0325, -0.0394,  0.0436,  0.0398,\n",
      "        -0.0191,  0.0269,  0.0400,  0.0313,  0.0390,  0.0345,  0.0299, -0.0207,\n",
      "        -0.0319, -0.0150])\n",
      "Gradient of AB Loss for row 10: tensor([0.0304, 0.0503, 0.0285, 0.0224, 0.0320, 0.0356, 0.0376, 0.0414, 0.0519,\n",
      "        0.0260, 0.0451, 0.0420, 0.0217, 0.0320])\n",
      "Gradient of AB Loss for row 11: tensor([0.0340, 0.0445, 0.0359, 0.0307, 0.0489, 0.0262, 0.0281, 0.0277, 0.0449,\n",
      "        0.0157, 0.0368, 0.0386, 0.0228, 0.0263])\n",
      "Gradient of AB Loss for row 12: tensor([0.0400, 0.0361, 0.0226, 0.0348, 0.0489, 0.0366, 0.0366, 0.0343, 0.0567,\n",
      "        0.0356, 0.0319, 0.0343, 0.0342, 0.0410])\n",
      "Gradient of AB Loss for row 13: tensor([0.0303, 0.0491, 0.0222, 0.0378, 0.0349, 0.0372, 0.0401, 0.0392, 0.0428,\n",
      "        0.0532, 0.0295, 0.0311, 0.0424])\n",
      "Gradient of AB Loss for row 14: tensor([0.0365, 0.0432, 0.0217, 0.0381, 0.0382, 0.0280, 0.0328, 0.0388, 0.0451,\n",
      "        0.0315, 0.0392, 0.0362, 0.0313, 0.0315])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0143, -0.0111, -0.0159, -0.0171,  0.0169, -0.0149, -0.0175, -0.0121,\n",
      "        -0.0162, -0.0192, -0.0180, -0.0123, -0.0199, -0.0131, -0.0101, -0.0130,\n",
      "        -0.0143, -0.0176, -0.0186,  0.0102, -0.0139,  0.0192, -0.0138, -0.0126,\n",
      "        -0.0194, -0.0144, -0.0093, -0.0118, -0.0150, -0.0165,  0.0187, -0.0168,\n",
      "        -0.0106])\n",
      "Gradient of AB Loss for row 16: tensor([0.0254, 0.0461, 0.0107, 0.0263, 0.0365, 0.0275, 0.0364, 0.0302, 0.0423,\n",
      "        0.0524, 0.0377, 0.0228, 0.0359, 0.0251])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0253, -0.0325, -0.0511, -0.0459, -0.0190, -0.0335,  0.0375, -0.0335,\n",
      "         0.0191, -0.0360,  0.0344,  0.0299, -0.0373, -0.0166, -0.0201])\n",
      "Gradient of AB Loss for row 18: tensor([0.0353, 0.0445, 0.0140, 0.0284, 0.0413, 0.0357, 0.0433, 0.0372, 0.0560,\n",
      "        0.0281, 0.0420, 0.0426, 0.0322, 0.0316])\n",
      "Gradient of AB Loss for row 19: tensor([0.0416, 0.0427, 0.0274, 0.0202, 0.0430, 0.0440, 0.0318, 0.0526, 0.0453,\n",
      "        0.0426, 0.0284, 0.0388, 0.0395, 0.0257])\n",
      "Gradient of AB Loss for row 20: tensor([0.0404, 0.0527, 0.0313, 0.0553, 0.0286, 0.0302, 0.0422, 0.0620, 0.0413,\n",
      "        0.0406, 0.0329, 0.0318, 0.0313])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0228,  0.0152, -0.0114, -0.0126,  0.0292, -0.0219,  0.0193,  0.0349,\n",
      "        -0.0278, -0.0208,  0.0246,  0.0126,  0.0302,  0.0239, -0.0168,  0.0134,\n",
      "         0.0221, -0.0164,  0.0209,  0.0244, -0.0258, -0.0284])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0264, -0.0339, -0.0335,  0.0165,  0.0140, -0.0363,  0.0275,  0.0498,\n",
      "         0.0225,  0.0471,  0.0373,  0.0157,  0.0230, -0.0378,  0.0184, -0.0225])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0426,  0.0222,  0.0267,  0.0413,  0.0429,  0.0212,  0.0348,  0.0321,\n",
      "         0.0530,  0.0237,  0.0382,  0.0252, -0.0394,  0.0360])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0405, -0.0360,  0.0245, -0.0426,  0.0090, -0.0382,  0.0351, -0.0441,\n",
      "         0.0192, -0.0380,  0.0256,  0.0329,  0.0190, -0.0203, -0.0363])\n",
      "Gradient of AB Loss for row 25: tensor([0.0421, 0.0443, 0.0357, 0.0300, 0.0492, 0.0379, 0.0442, 0.0483, 0.0427,\n",
      "        0.0388, 0.0320, 0.0436, 0.0289, 0.0260])\n",
      "Gradient of AB Loss for row 26: tensor([0.0299, 0.0530, 0.0278, 0.0273, 0.0373, 0.0233, 0.0362, 0.0429, 0.0467,\n",
      "        0.0360, 0.0486, 0.0332, 0.0332, 0.0199])\n",
      "Gradient of AB Loss for row 27: tensor([-0.5361])\n",
      "Gradient of AB Loss for row 28: tensor([ 0.0431, -0.0549,  0.0495,  0.0227,  0.0206,  0.0185,  0.0369,  0.0366,\n",
      "         0.0354,  0.0433,  0.0522,  0.0452,  0.0324,  0.0451])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0130, -0.0135, -0.0227, -0.0184, -0.0233, -0.0309, -0.0166, -0.0156,\n",
      "         0.0222, -0.0161, -0.0248, -0.0187, -0.0249,  0.0187, -0.0161, -0.0163,\n",
      "        -0.0091, -0.0158, -0.0133,  0.0282, -0.0238, -0.0310])\n",
      "Gradient of AB Loss for row 30: tensor([ 0.0314, -0.0437,  0.0575,  0.0337,  0.0286,  0.0458,  0.0365,  0.0352,\n",
      "         0.0345,  0.0477,  0.0378,  0.0416,  0.0363,  0.0261])\n",
      "Gradient of AB Loss for row 31: tensor([0.0340, 0.0490, 0.0319, 0.0219, 0.0464, 0.0283, 0.0324, 0.0350, 0.0338,\n",
      "        0.0386, 0.0309, 0.0371, 0.0376, 0.0372])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10217482596635818\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.10441853106021881\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.188108429312706\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9675909876823425\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10595674812793732\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.17360608279705048\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.26487699151039124\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.13804608583450317\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.014449265785515308\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.15079905092716217\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.6314390949290214e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3401944041252136\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.08735861629247665\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.15135137736797333\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.016318999230861664\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.16273003816604614\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.0015607232460866e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3479589521884918\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.09531934559345245\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.17289981245994568\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.021557368338108063\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.19877319037914276\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 9.575625847801916e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3624013364315033\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09759624302387238\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.22761288285255432\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.0202255230396986\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.23300595581531525\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.757843648420021e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3948400914669037\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.0937647894024849\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.10739634186029434\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.009703036397695541\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10721544176340103\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.0951729523256404e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.39296954870224\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.0908382385969162\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.16189044713974\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.012757742777466774\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.15951302647590637\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.0255304383477437e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.37051922082901\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08795489370822906\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.14524205029010773\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.014521993696689606\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.15395978093147278\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.036716601454657e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3517906069755554\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08531222492456436\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.11560043692588806\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.010635476559400558\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.11191490292549133\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 2.4733197712123456e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3875938653945923\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.09445061534643173\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 3.0031542778015137\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.44696709513664246\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.05369780585169792\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.07035743445158005\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3539397418498993\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.042232029139995575\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3786851763725281\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.13297796249389648\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9331791996955872\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09251735359430313\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1782997101545334\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2076655775308609\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08095294237136841\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.007991763763129711\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.08356525748968124\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 9.367200348719962e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.31039664149284363\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06192928925156593\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09697958081960678\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.00998049508780241\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09522116184234619\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.909384813764575e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3044934570789337\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06550111621618271\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.06848789751529694\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005956455133855343\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06688273698091507\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.5184855595151703e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.35850003361701965\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.07181384414434433\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.05885792896151543\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.004455019719898701\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.05920528247952461\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 5.873443398307643e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.29407820105552673\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.062404926866292953\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08870173990726471\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.007550987880676985\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.080204077064991\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.816726435954763e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3077915906906128\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.059855684638023376\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.11222395300865173\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.012009236961603165\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.12467227131128311\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.3275476273832965e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3204900622367859\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07246585190296173\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08652625977993011\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.008561760187149048\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08325352519750595\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 6.496889692897412e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3045365512371063\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06641843914985657\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07720071822404861\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.007759846281260252\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07375070452690125\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.065955879120395e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.31313371658325195\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06625907868146896\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.6332499980926514\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.32120826840400696\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05279884487390518\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.0575590506196022\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.370947003364563\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03634724020957947\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3399209678173065\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.10587771236896515\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.9500863552093506\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2036840319633484\n",
      "ab_loss: 23.969995498657227\n",
      "Gradient of AB Loss for row 0: tensor([0.1419, 0.1280, 0.1247, 0.1399])\n",
      "Gradient of AB Loss for row 1: tensor([ 0.0671,  0.1523, -0.0828,  0.1553])\n",
      "Gradient of AB Loss for row 2: tensor([-0.4467])\n",
      "Gradient of AB Loss for row 3: tensor([0.0171, 0.0508, 0.0244, 0.0320, 0.0349, 0.0240, 0.0379, 0.0350, 0.0459,\n",
      "        0.0461, 0.0340, 0.0389, 0.0352, 0.0381])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0337, -0.0395,  0.0555,  0.0328,  0.0293,  0.0449,  0.0394,  0.0379,\n",
      "         0.0506,  0.0292,  0.0199,  0.0297, -0.0397,  0.0288])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0304,  0.0421,  0.0212,  0.0339, -0.0304,  0.0283,  0.0425,  0.0443,\n",
      "         0.0355,  0.0530, -0.0415, -0.0363, -0.0143,  0.0165,  0.0325])\n",
      "Gradient of AB Loss for row 6: tensor([0.0355, 0.0506, 0.0202, 0.0354, 0.0398, 0.0424, 0.0261, 0.0295, 0.0463,\n",
      "        0.0398, 0.0313, 0.0407, 0.0195, 0.0425])\n",
      "Gradient of AB Loss for row 7: tensor([-0.4152])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0601,  0.0450,  0.0446, -0.0491,  0.0350,  0.0283,  0.0355,  0.0434,\n",
      "         0.0569,  0.0470,  0.0451,  0.0260, -0.0357,  0.0333])\n",
      "Gradient of AB Loss for row 9: tensor([ 0.1100,  0.1537, -0.0754,  0.1259])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0373, -0.0487, -0.0580, -0.0469, -0.0256,  0.0379, -0.0425,  0.0364,\n",
      "         0.0214,  0.0534,  0.0379,  0.0289, -0.0350, -0.0297])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0352, -0.0443,  0.0185,  0.0119,  0.0298,  0.0457,  0.0346,  0.0268,\n",
      "         0.0498,  0.0410,  0.0345, -0.0409,  0.0248,  0.0223, -0.0175, -0.0249])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0206, -0.0362, -0.0247,  0.0351,  0.0358,  0.0259,  0.0162, -0.0374,\n",
      "         0.0225,  0.0443,  0.0350,  0.0442,  0.0283,  0.0405,  0.0310,  0.0244,\n",
      "         0.0289])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0437, -0.0498, -0.0344,  0.0144, -0.0258, -0.0515,  0.0436,  0.0468,\n",
      "        -0.0341,  0.0244, -0.0321, -0.0202, -0.0202])\n",
      "Gradient of AB Loss for row 14: tensor([0.0407, 0.0485, 0.0176, 0.0168, 0.0365, 0.0292, 0.0437, 0.0307, 0.0430,\n",
      "        0.0463, 0.0493, 0.0327, 0.0314, 0.0389])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0401,  0.0433,  0.0110,  0.0456, -0.0356,  0.0367,  0.0321,  0.0455,\n",
      "         0.0439,  0.0413,  0.0362,  0.0304, -0.0253,  0.0314])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0367, -0.0358,  0.0369,  0.0474,  0.0132, -0.0196,  0.0372,  0.0309,\n",
      "         0.0389,  0.0258,  0.0265,  0.0177,  0.0285, -0.0183])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0281,  0.0386,  0.0413,  0.0244,  0.0245,  0.0322,  0.0366,  0.0403,\n",
      "         0.0497,  0.0266,  0.0302,  0.0283,  0.0324,  0.0282])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0170, -0.0259, -0.0389, -0.0242,  0.0288,  0.0250,  0.0156,  0.0357,\n",
      "         0.0250, -0.0264, -0.0327,  0.0269,  0.0340,  0.0314,  0.0269,  0.0150,\n",
      "         0.0270,  0.0200,  0.0185, -0.0283,  0.0248, -0.0242])\n",
      "Gradient of AB Loss for row 19: tensor([0.0431, 0.0537, 0.0335, 0.0270, 0.0500, 0.0280, 0.0304, 0.0385, 0.0494,\n",
      "        0.0395, 0.0311, 0.0368, 0.0259, 0.0296])\n",
      "Gradient of AB Loss for row 20: tensor([0.1183, 0.1273, 0.1196, 0.1330])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0400, -0.0279, -0.0322, -0.0304,  0.0306,  0.0487,  0.0218, -0.0239,\n",
      "         0.0402,  0.0451,  0.0389,  0.0376,  0.0390,  0.0349, -0.0234])\n",
      "Gradient of AB Loss for row 22: tensor([-0.6284])\n",
      "Gradient of AB Loss for row 23: tensor([0.0486, 0.0562, 0.0284, 0.0301, 0.0424, 0.0482, 0.0444, 0.0537, 0.0506,\n",
      "        0.0407, 0.0298, 0.0288, 0.0294])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0222, -0.0345, -0.0332, -0.0510, -0.0379,  0.0351,  0.0293,  0.0286,\n",
      "        -0.0358,  0.0288,  0.0455,  0.0371,  0.0417,  0.0397, -0.0332, -0.0255])\n",
      "Gradient of AB Loss for row 25: tensor([0.0443, 0.0433, 0.0358, 0.0340, 0.0494, 0.0441, 0.0413, 0.0410, 0.0505,\n",
      "        0.0437, 0.0238, 0.0295, 0.0270, 0.0298])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0195, -0.0197, -0.0244,  0.0140, -0.0186, -0.0267, -0.0226, -0.0327,\n",
      "         0.0174, -0.0213, -0.0285, -0.0241,  0.0190, -0.0203, -0.0149,  0.0183,\n",
      "        -0.0120, -0.0191, -0.0143,  0.0242, -0.0244,  0.0186])\n",
      "Gradient of AB Loss for row 27: tensor([0.0407, 0.0446, 0.0267, 0.0276, 0.0275, 0.0424, 0.0428, 0.0417, 0.0478,\n",
      "        0.0347, 0.0422, 0.0350, 0.0336, 0.0287])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0521,  0.0497,  0.0284, -0.0459,  0.0333,  0.0426,  0.0419,  0.0306,\n",
      "         0.0459,  0.0560,  0.0503,  0.0297, -0.0437,  0.0298])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0268,  0.0237, -0.0351, -0.0345, -0.0466, -0.0311, -0.0181,  0.0428,\n",
      "         0.0252,  0.0474,  0.0266,  0.0337, -0.0326,  0.0276, -0.0256])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0340, -0.0296, -0.0338, -0.0351,  0.0440,  0.0305,  0.0400,  0.0091,\n",
      "        -0.0183,  0.0295,  0.0454,  0.0424,  0.0437,  0.0312,  0.0310, -0.0373,\n",
      "         0.0202])\n",
      "Gradient of AB Loss for row 31: tensor([0.1157, 0.1551, 0.1390, 0.1301])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09979328513145447\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.10269644111394882\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1613456755876541\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9706714153289795\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.10250373929738998\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.18305401504039764\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2578130066394806\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.15634724497795105\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.014911466278135777\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.16031885147094727\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.3907084372988265e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3280709683895111\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.08008205890655518\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.13143406808376312\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.013085422106087208\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.14184923470020294\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.324450771278407e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.32614922523498535\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07234550267457962\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.14172765612602234\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.012851742096245289\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.1421123892068863\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.67570024522945e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3413529396057129\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.08763028681278229\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.15239103138446808\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.011447361670434475\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.14563718438148499\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.217047684853867e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.32253190875053406\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.06948015838861465\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.12519000470638275\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.01144029013812542\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.13042333722114563\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.0091184554639199e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3473043441772461\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07417917251586914\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.1412556916475296\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.014148867689073086\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.14303772151470184\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.151072459570912e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.34351977705955505\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.07494409382343292\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.198848158121109\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01673930510878563\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.19358135759830475\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.2440664054480521e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.33072566986083984\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.06947331130504608\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.16347162425518036\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.010566326789557934\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.15955397486686707\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.2558970530207603e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.35010871291160583\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07884152978658676\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.792633533477783\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3791133761405945\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04849740490317345\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06150215119123459\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3964552879333496\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04343338683247566\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.4047239422798157\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.13734105229377747\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0082350969314575\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.10079916566610336\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.18512094020843506\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.22797134518623352\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.07445116341114044\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.00535541819408536\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.08167102187871933\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 6.998084889353606e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.30715954303741455\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05646344646811485\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09215223789215088\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.008246654644608498\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09478996694087982\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 2.3215411815158404e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3067290782928467\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.05710110440850258\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.09119301289319992\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006694424897432327\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08135885745286942\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 8.54335824307384e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.34217745065689087\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.05601441487669945\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09101425856351852\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.008481518365442753\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.08451507240533829\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 7.928648848576358e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3061648905277252\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06007822975516319\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.09939242154359818\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.010824743658304214\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.09029418975114822\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.0870020439313066e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.318151593208313\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.059643931686878204\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.10850395262241364\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.01105805765837431\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.11244817823171616\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.0226691715686798e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.29176366329193115\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.05862932279706001\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.09548483788967133\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.008530623279511929\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.10088818520307541\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.5652067419935634e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.30266234278678894\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.0592229962348938\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.08820966631174088\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.008840265683829784\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.08886480331420898\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.238639215285559e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.29517480731010437\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.056372568011283875\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.642162322998047\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.29437869787216187\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.0507885180413723\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05788265913724899\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.37090131640434265\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03533487394452095\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3638792634010315\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.1110653430223465\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.1446781158447266\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2384531944990158\n",
      "ab_loss: 24.05099105834961\n",
      "Gradient of AB Loss for row 0: tensor([-0.0549, -0.0352,  0.0431,  0.0307,  0.0362,  0.0348,  0.0337,  0.0410,\n",
      "         0.0501,  0.0544,  0.0404,  0.0257, -0.0324,  0.0404])\n",
      "Gradient of AB Loss for row 1: tensor([0.0276, 0.0344, 0.0293, 0.0327, 0.0414, 0.0366, 0.0367, 0.0355, 0.0485,\n",
      "        0.0394, 0.0488, 0.0260, 0.0230, 0.0457])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0243, -0.0285,  0.0191,  0.0199, -0.0190,  0.0330,  0.0397,  0.0282,\n",
      "         0.0238,  0.0424,  0.0351,  0.0237,  0.0364, -0.0220,  0.0321,  0.0153,\n",
      "        -0.0323])\n",
      "Gradient of AB Loss for row 3: tensor([ 0.0412, -0.0395,  0.0559,  0.0274,  0.0364,  0.0375,  0.0318,  0.0268,\n",
      "         0.0289,  0.0490,  0.0410,  0.0332,  0.0338,  0.0364])\n",
      "Gradient of AB Loss for row 4: tensor([0.0354, 0.0548, 0.0138, 0.0238, 0.0408, 0.0303, 0.0335, 0.0279, 0.0444,\n",
      "        0.0433, 0.0437, 0.0294, 0.0283, 0.0429])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0103, -0.0260, -0.0177, -0.0189, -0.0256, -0.0290,  0.0307,  0.0304,\n",
      "        -0.0227, -0.0342, -0.0208,  0.0111, -0.0270,  0.0268, -0.0223,  0.0094,\n",
      "        -0.0096, -0.0219, -0.0133,  0.0176, -0.0351, -0.0253])\n",
      "Gradient of AB Loss for row 6: tensor([0.0391, 0.0460, 0.0200, 0.0250, 0.0379, 0.0423, 0.0317, 0.0483, 0.0561,\n",
      "        0.0410, 0.0434, 0.0307, 0.0224, 0.0287])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0365, -0.0444, -0.0367,  0.0379,  0.0326, -0.0420,  0.0297,  0.0473,\n",
      "         0.0391,  0.0326,  0.0313, -0.0425, -0.0345,  0.0345,  0.0307])\n",
      "Gradient of AB Loss for row 8: tensor([ 0.0425, -0.0352,  0.0504,  0.0453,  0.0243,  0.0380,  0.0316,  0.0245,\n",
      "         0.0323,  0.0467,  0.0422,  0.0383,  0.0375,  0.0407])\n",
      "Gradient of AB Loss for row 9: tensor([ 0.0387, -0.0445,  0.0440,  0.0282,  0.0276,  0.0430,  0.0302,  0.0303,\n",
      "         0.0364,  0.0519,  0.0280,  0.0335,  0.0262,  0.0297])\n",
      "Gradient of AB Loss for row 10: tensor([0.0284, 0.0513, 0.0156, 0.0420, 0.0193, 0.0277, 0.0271, 0.0374, 0.0423,\n",
      "        0.0280, 0.0330, 0.0215, 0.0333, 0.0269])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0132, -0.0231, -0.0198, -0.0191, -0.0235, -0.0343, -0.0265, -0.0198,\n",
      "        -0.0198, -0.0140, -0.0210, -0.0200, -0.0265, -0.0166, -0.0152, -0.0165,\n",
      "        -0.0189, -0.0122, -0.0085,  0.0215, -0.0211, -0.0173, -0.0248])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0348, -0.0405,  0.0291, -0.0324,  0.0410, -0.0530,  0.0347,  0.0307,\n",
      "         0.0300,  0.0438,  0.0373,  0.0261, -0.0380, -0.0282])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0247, -0.0287, -0.0282,  0.0302,  0.0307,  0.0241,  0.0139,  0.0339,\n",
      "         0.0421,  0.0260,  0.0362,  0.0372,  0.0259,  0.0387,  0.0243, -0.0276,\n",
      "        -0.0364,  0.0222])\n",
      "Gradient of AB Loss for row 14: tensor([0.0372, 0.0461, 0.0177, 0.0258, 0.0468, 0.0351, 0.0371, 0.0435, 0.0379,\n",
      "        0.0306, 0.0352, 0.0407, 0.0381, 0.0362])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0349, -0.0312,  0.0457, -0.0311,  0.0429, -0.0379,  0.0499,  0.0498,\n",
      "         0.0414,  0.0377,  0.0296, -0.0265, -0.0261,  0.0191])\n",
      "Gradient of AB Loss for row 16: tensor([0.0421, 0.0491, 0.0153, 0.0241, 0.0514, 0.0355, 0.0398, 0.0335, 0.0517,\n",
      "        0.0428, 0.0383, 0.0300, 0.0296, 0.0347])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0448,  0.0596,  0.0317,  0.0325,  0.0422,  0.0355,  0.0428,  0.0568,\n",
      "         0.0463,  0.0520,  0.0381, -0.0383,  0.0445])\n",
      "Gradient of AB Loss for row 18: tensor([0.0411, 0.0330, 0.0167, 0.0203, 0.0382, 0.0380, 0.0293, 0.0353, 0.0525,\n",
      "        0.0351, 0.0418, 0.0314, 0.0371, 0.0474])\n",
      "Gradient of AB Loss for row 19: tensor([0.0357, 0.0519, 0.0374, 0.0258, 0.0421, 0.0243, 0.0203, 0.0345, 0.0557,\n",
      "        0.0373, 0.0424, 0.0369, 0.0368, 0.0280])\n",
      "Gradient of AB Loss for row 20: tensor([-0.4208])\n",
      "Gradient of AB Loss for row 21: tensor([0.0370, 0.0443, 0.0296, 0.0275, 0.0266, 0.0342, 0.0423, 0.0358, 0.0382,\n",
      "        0.0283, 0.0279, 0.0305, 0.0234, 0.0299])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0350,  0.0494,  0.0167,  0.0484, -0.0322,  0.0352,  0.0257,  0.0549,\n",
      "         0.0398,  0.0522,  0.0317,  0.0201, -0.0431,  0.0241])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0339, -0.0395, -0.0342,  0.0139, -0.0195,  0.0420, -0.0449,  0.0361,\n",
      "        -0.0352,  0.0413,  0.0313,  0.0240, -0.0366, -0.0384, -0.0231])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0329, -0.0286,  0.0427,  0.0319,  0.0335,  0.0156,  0.0344,  0.0369,\n",
      "         0.0369,  0.0292,  0.0322,  0.0267,  0.0367,  0.0209,  0.0298, -0.0425,\n",
      "         0.0247,  0.0196])\n",
      "Gradient of AB Loss for row 25: tensor([ 0.1534,  0.1546, -0.1108,  0.1196])\n",
      "Gradient of AB Loss for row 26: tensor([0.0266, 0.0266, 0.0280, 0.0232, 0.0243, 0.0277, 0.0298, 0.0278, 0.0205,\n",
      "        0.0275, 0.0144, 0.0302, 0.0320, 0.0327, 0.0191, 0.0170, 0.0203, 0.0343,\n",
      "        0.0239, 0.0213, 0.0171, 0.0166])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0250, -0.0514, -0.0479,  0.0327, -0.0504,  0.0428,  0.0560,  0.0268,\n",
      "         0.0449,  0.0323,  0.0375, -0.0424,  0.0404])\n",
      "Gradient of AB Loss for row 28: tensor([-0.6416])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0304, -0.0306, -0.0263, -0.0378,  0.0392,  0.0287,  0.0144, -0.0362,\n",
      "         0.0386,  0.0421,  0.0423,  0.0310,  0.0325, -0.0400,  0.0269, -0.0152,\n",
      "         0.0141])\n",
      "Gradient of AB Loss for row 30: tensor([0.0240, 0.0531, 0.0178, 0.0289, 0.0420, 0.0165, 0.0308, 0.0403, 0.0582,\n",
      "        0.0385, 0.0413, 0.0355, 0.0316, 0.0334])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0175, -0.0396,  0.0299, -0.0510,  0.0174, -0.0295,  0.0415, -0.0344,\n",
      "         0.0420, -0.0401,  0.0399,  0.0375,  0.0261, -0.0250, -0.0228])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10846911370754242\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.11154050379991531\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.22055600583553314\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0541404485702515\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.12821291387081146\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.20570850372314453\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.3217887878417969\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.13278214633464813\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.015610487200319767\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1485820859670639\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.412924999222298e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.35721588134765625\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.10982028394937515\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.1385156810283661\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.015480298548936844\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.16143706440925598\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.4151860794342497e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3408038914203644\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.09514746069908142\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.12656569480895996\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01708889566361904\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.13759300112724304\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.4324472719096093e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.36297109723091125\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.10812528431415558\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.09500487893819809\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.012961962260305882\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.09534163028001785\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.2281657912893706e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3592692017555237\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.09364646673202515\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.10528058558702469\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.01313287764787674\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10517321527004242\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.0718340659465753e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.39695093035697937\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.10289256274700165\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.11202118545770645\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.016647281125187874\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.10961693525314331\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.2094646395510722e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.35783061385154724\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09858174622058868\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.10538870841264725\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.009174204431474209\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.11151805520057678\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 9.61414614586431e-10\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3700747787952423\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.10261396318674088\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.11004241555929184\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.0081478301435709\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.11367107927799225\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.3722474268007545e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3615439236164093\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.0949576124548912\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.9686660766601562\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.49320125579833984\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.055336352437734604\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.0734209194779396\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.4247751235961914\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.05162778124213219\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.43408551812171936\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.15924488008022308\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0798628330230713\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.11924973130226135\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.22994428873062134\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.27376070618629456\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06896732747554779\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.007173276040703058\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.07263611257076263\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 9.404530487699958e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.32223695516586304\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06664881855249405\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.08895523101091385\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.0074586421251297\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09401832520961761\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 8.129364958975316e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3352883756160736\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07831820845603943\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07075099647045135\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.007684458512812853\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07160497456789017\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.4287291350001396e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.40192633867263794\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.0861268937587738\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.07635286450386047\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.006179286167025566\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.07563643902540207\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 9.251087118578027e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3412083387374878\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.0765494778752327\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.07045377045869827\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.006840507499873638\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.0685119777917862\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 9.474360185279807e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3384861350059509\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.07346682250499725\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.07840989530086517\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.0073118750005960464\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.08327994495630264\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 7.351723119164433e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.32838040590286255\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07890181988477707\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.07589605450630188\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007682152092456818\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07765968143939972\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.3189483949460623e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3291111886501312\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07775634527206421\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.10816221684217453\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.011555205099284649\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.09660324454307556\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.486511493543958e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.3241405189037323\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.07516245543956757\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.944835662841797\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.38682082295417786\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.057745207101106644\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06951502710580826\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3530697822570801\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03903067111968994\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3974098265171051\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.1377430409193039\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.0883426666259766\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2317427098751068\n",
      "ab_loss: 24.10698890686035\n",
      "Gradient of AB Loss for row 0: tensor([0.0400, 0.0485, 0.0112, 0.0268, 0.0480, 0.0323, 0.0387, 0.0406, 0.0407,\n",
      "        0.0358, 0.0533, 0.0378, 0.0325, 0.0195])\n",
      "Gradient of AB Loss for row 1: tensor([-0.5580])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0345, -0.0543,  0.0597,  0.0249,  0.0434,  0.0412,  0.0468,  0.0458,\n",
      "         0.0580,  0.0382,  0.0346, -0.0299,  0.0468])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0325, -0.0193, -0.0239, -0.0256,  0.0315,  0.0172, -0.0229,  0.0380,\n",
      "         0.0284,  0.0314,  0.0306, -0.0351, -0.0292,  0.0231, -0.0240, -0.0218,\n",
      "        -0.0108])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0387, -0.0237,  0.0215,  0.0478,  0.0294,  0.0234, -0.0242, -0.0319,\n",
      "         0.0339,  0.0369,  0.0293,  0.0427,  0.0334, -0.0268,  0.0356,  0.0171])\n",
      "Gradient of AB Loss for row 5: tensor([0.0398, 0.0463, 0.0288, 0.0265, 0.0463, 0.0401, 0.0458, 0.0365, 0.0488,\n",
      "        0.0466, 0.0358, 0.0292, 0.0331, 0.0221])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0298, -0.0283, -0.0294, -0.0362, -0.0367,  0.0288,  0.0357, -0.0401,\n",
      "         0.0425,  0.0364,  0.0255,  0.0337,  0.0187,  0.0409, -0.0325, -0.0317,\n",
      "        -0.0276])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0625, -0.0265,  0.0362,  0.0322, -0.0345,  0.0369,  0.0442,  0.0251,\n",
      "         0.0483,  0.0382,  0.0344,  0.0197, -0.0519,  0.0348])\n",
      "Gradient of AB Loss for row 8: tensor([0.0443, 0.0451, 0.0278, 0.0290, 0.0418, 0.0263, 0.0408, 0.0388, 0.0344,\n",
      "        0.0318, 0.0448, 0.0296, 0.0342, 0.0373])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0412, -0.0397, -0.0316, -0.0359,  0.0515,  0.0319,  0.0405,  0.0214,\n",
      "        -0.0269,  0.0285,  0.0408,  0.0327,  0.0306,  0.0478, -0.0369,  0.0143])\n",
      "Gradient of AB Loss for row 10: tensor([-0.3418])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0261, -0.0162, -0.0218, -0.0306, -0.0236,  0.0337, -0.0321, -0.0238,\n",
      "        -0.0246,  0.0324,  0.0121,  0.0261,  0.0284, -0.0150,  0.0300, -0.0116,\n",
      "         0.0218,  0.0324, -0.0228,  0.0198])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0364, -0.0464,  0.0391,  0.0204,  0.0562, -0.0360,  0.0384,  0.0303,\n",
      "         0.0436,  0.0387,  0.0353, -0.0332, -0.0275,  0.0367])\n",
      "Gradient of AB Loss for row 13: tensor([ 0.0395, -0.0365,  0.0462,  0.0325,  0.0222,  0.0415,  0.0250,  0.0314,\n",
      "         0.0300,  0.0470,  0.0441,  0.0380,  0.0358,  0.0235])\n",
      "Gradient of AB Loss for row 14: tensor([0.0293, 0.0494, 0.0222, 0.0368, 0.0428, 0.0331, 0.0234, 0.0439, 0.0376,\n",
      "        0.0233, 0.0368, 0.0396, 0.0298, 0.0184])\n",
      "Gradient of AB Loss for row 15: tensor([ 0.0453, -0.0581,  0.0395,  0.0153,  0.0333,  0.0420,  0.0469,  0.0340,\n",
      "         0.0385,  0.0377,  0.0409,  0.0348,  0.0436])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0359, -0.0172,  0.0214, -0.0231, -0.0435, -0.0356, -0.0198,  0.0337,\n",
      "         0.0336, -0.0298, -0.0251,  0.0441,  0.0306, -0.0208,  0.0312, -0.0163,\n",
      "         0.0355, -0.0292, -0.0258])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0482, -0.0356,  0.0614,  0.0552, -0.0408,  0.0366,  0.0421,  0.0468,\n",
      "         0.0583,  0.0499,  0.0377,  0.0221, -0.0280,  0.0260])\n",
      "Gradient of AB Loss for row 18: tensor([ 0.0419, -0.0324,  0.0387,  0.0137,  0.0470, -0.0404,  0.0461,  0.0513,\n",
      "         0.0460,  0.0465,  0.0423,  0.0252,  0.0264,  0.0348])\n",
      "Gradient of AB Loss for row 19: tensor([0.0323, 0.0357, 0.0293, 0.0432, 0.0449, 0.0406, 0.0451, 0.0466, 0.0403,\n",
      "        0.0369, 0.0264, 0.0290, 0.0188, 0.0254])\n",
      "Gradient of AB Loss for row 20: tensor([-0.6011])\n",
      "Gradient of AB Loss for row 21: tensor([0.0401, 0.0487, 0.0313, 0.0181, 0.0516, 0.0402, 0.0416, 0.0347, 0.0429,\n",
      "        0.0407, 0.0310, 0.0358, 0.0304, 0.0293])\n",
      "Gradient of AB Loss for row 22: tensor([ 0.0340, -0.0201,  0.0360,  0.0271,  0.0313,  0.0343,  0.0198,  0.0210,\n",
      "         0.0199,  0.0093,  0.0268,  0.0226,  0.0299,  0.0202,  0.0180,  0.0319,\n",
      "         0.0310,  0.0206,  0.0161,  0.0237])\n",
      "Gradient of AB Loss for row 23: tensor([-0.5705])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0273, -0.0438,  0.0311,  0.0113,  0.0486,  0.0485,  0.0336,  0.0539,\n",
      "         0.0371,  0.0391,  0.0381, -0.0334,  0.0302])\n",
      "Gradient of AB Loss for row 25: tensor([0.0330, 0.0498, 0.0299, 0.0323, 0.0495, 0.0311, 0.0310, 0.0371, 0.0343,\n",
      "        0.0272, 0.0317, 0.0374, 0.0298, 0.0335])\n",
      "Gradient of AB Loss for row 26: tensor([ 0.0441, -0.0422,  0.0456,  0.0132,  0.0277,  0.0380,  0.0277,  0.0270,\n",
      "         0.0447,  0.0413,  0.0292,  0.0318,  0.0268,  0.0434])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0423,  0.0295, -0.0320,  0.0247,  0.0259,  0.0478,  0.0491,  0.0496,\n",
      "         0.0451,  0.0186,  0.0461,  0.0312,  0.0371])\n",
      "Gradient of AB Loss for row 28: tensor([0.1622, 0.1550, 0.1231, 0.1384])\n",
      "Gradient of AB Loss for row 29: tensor([ 0.1816, -0.1463,  0.1502])\n",
      "Gradient of AB Loss for row 30: tensor([ 0.0237, -0.0324, -0.0218,  0.0296,  0.0286,  0.0258, -0.0160,  0.0239,\n",
      "         0.0188,  0.0160,  0.0368,  0.0324,  0.0153,  0.0281, -0.0177,  0.0218,\n",
      "         0.0278,  0.0185, -0.0215,  0.0181,  0.0224])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0387, -0.0233, -0.0277,  0.0315,  0.0368,  0.0284,  0.0227, -0.0261,\n",
      "         0.0417,  0.0425,  0.0414,  0.0286,  0.0243,  0.0362, -0.0327,  0.0261])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08096843957901001\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.08836271613836288\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.13798364996910095\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.859100878238678\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.09091757237911224\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.15671440958976746\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2305890917778015\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.11991896480321884\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.013280759565532207\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.13476458191871643\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.8368455645401127e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.27746111154556274\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.06909564137458801\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.13098663091659546\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.011813940480351448\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.14778243005275726\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.4406263959543253e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2780490815639496\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.0681036114692688\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.12417338788509369\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.013633225113153458\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.13217993080615997\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 9.148364843447609e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.29387953877449036\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.06849563866853714\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.11839120835065842\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.013319217599928379\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.12237589806318283\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.4046062091210842e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.2926368713378906\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07285881042480469\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.10234995931386948\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.010816939175128937\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.0987476110458374\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.1600099769637495e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.29592153429985046\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07294030487537384\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.11478245258331299\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.011792798526585102\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.1103389710187912\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.3989666092228958e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.2607513666152954\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.06519881635904312\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.13779984414577484\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.012884105555713177\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.15508434176445007\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 9.916497623052578e-10\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.2846166789531708\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07001764327287674\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.11527088284492493\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.013522590510547161\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.11378208547830582\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.6225847332407284e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.2843567430973053\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07250265777111053\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.329665422439575\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.34906327724456787\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04344470053911209\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.049292828887701035\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.29955369234085083\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.032601356506347656\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3294370174407959\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.11251813173294067\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.8897833228111267\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.0924130454659462\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.15796411037445068\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2137022167444229\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06203387305140495\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.00498581537976861\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.06496790796518326\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.0911643810729288e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.25999242067337036\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.04861544072628021\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09705326706171036\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.008432942442595959\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09724383801221848\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 8.601311884959273e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.276248574256897\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.05848558992147446\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07056017220020294\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005292245652526617\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06992966681718826\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 9.130760592057641e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3134995698928833\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.05948394164443016\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.07171843200922012\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.006987246684730053\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.07126031070947647\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 8.547771934708237e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.25678616762161255\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.05270937830209732\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.061482008546590805\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.005186708178371191\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.057214636355638504\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.2670410276527377e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.25212958455085754\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.05034278333187103\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.09567095339298248\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.007966275326907635\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09631786495447159\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.4196606112903964e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2718372941017151\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.05792272463440895\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.06868737936019897\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.006234851665794849\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07133706659078598\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 8.947590446339859e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.26977601647377014\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.05708636716008186\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07728941738605499\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.006198385264724493\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07966282218694687\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.22345691542381e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2574698030948639\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.05254001170396805\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.3318262100219727\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.27738627791404724\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.045783255249261856\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.050097089260816574\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.31861621141433716\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03301834315061569\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3189377188682556\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.10497528314590454\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.7874000072479248\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.20044733583927155\n",
      "ab_loss: 23.73241424560547\n",
      "Gradient of AB Loss for row 0: tensor([0.0356, 0.0290, 0.0119, 0.0338, 0.0338, 0.0355, 0.0401, 0.0349, 0.0188,\n",
      "        0.0391, 0.0419, 0.0315, 0.0348, 0.0420])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0328, -0.0215, -0.0445, -0.0460, -0.0460,  0.0402, -0.0261,  0.0284,\n",
      "         0.0405,  0.0329, -0.0377, -0.0390, -0.0321, -0.0241])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0293,  0.0245, -0.0476,  0.0236,  0.0354, -0.0460,  0.0264,  0.0333,\n",
      "         0.0477,  0.0409,  0.0363,  0.0331,  0.0497])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0254, -0.0275,  0.0281,  0.0160, -0.0275, -0.0225,  0.0242,  0.0087,\n",
      "         0.0253,  0.0247,  0.0278, -0.0204,  0.0258,  0.0277, -0.0163, -0.0192,\n",
      "        -0.0191, -0.0102,  0.0309,  0.0128,  0.0182, -0.0266,  0.0158])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0378, -0.0216, -0.0358, -0.0525, -0.0348,  0.0373, -0.0407,  0.0293,\n",
      "         0.0435,  0.0475,  0.0350, -0.0509,  0.0358, -0.0410])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0321, -0.0359, -0.0307,  0.0298,  0.0362,  0.0352,  0.0086,  0.0308,\n",
      "         0.0394,  0.0319,  0.0388,  0.0397,  0.0302,  0.0255,  0.0243, -0.0277,\n",
      "        -0.0349,  0.0159])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0283, -0.0188, -0.0228,  0.0220,  0.0201,  0.0281,  0.0090, -0.0221,\n",
      "         0.0347,  0.0460,  0.0350,  0.0354,  0.0280,  0.0283,  0.0244, -0.0327,\n",
      "         0.0279,  0.0184])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0183, -0.0204,  0.0187, -0.0141, -0.0271, -0.0375, -0.0182,  0.0256,\n",
      "         0.0237, -0.0186, -0.0265,  0.0278,  0.0293,  0.0246, -0.0193,  0.0255,\n",
      "        -0.0126, -0.0168,  0.0323, -0.0276,  0.0243])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0398, -0.0402,  0.0340,  0.0487, -0.0549,  0.0463,  0.0438,  0.0516,\n",
      "         0.0528,  0.0339,  0.0377, -0.0258,  0.0341])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0348, -0.0395, -0.0423, -0.0207, -0.0141, -0.0245, -0.0466, -0.0321,\n",
      "        -0.0077, -0.0334, -0.0260, -0.0364, -0.0401, -0.0349, -0.0200, -0.0302,\n",
      "        -0.0211])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0353, -0.0446, -0.0268,  0.0437,  0.0133,  0.0333, -0.0421,  0.0332,\n",
      "         0.0503,  0.0495,  0.0506,  0.0255, -0.0241,  0.0167,  0.0213])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0296, -0.0538,  0.0139,  0.0592, -0.0499,  0.0364,  0.0444,  0.0488,\n",
      "         0.0355,  0.0234, -0.0403, -0.0171,  0.0304])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0266,  0.0263, -0.0288, -0.0316, -0.0344, -0.0365, -0.0256, -0.0164,\n",
      "        -0.0135, -0.0308, -0.0184, -0.0246, -0.0188, -0.0200, -0.0177, -0.0190,\n",
      "        -0.0122, -0.0319, -0.0263, -0.0301])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0234, -0.0276, -0.0141, -0.0196, -0.0259, -0.0349, -0.0279, -0.0222,\n",
      "        -0.0244, -0.0169, -0.0308, -0.0216, -0.0254, -0.0247, -0.0195, -0.0191,\n",
      "        -0.0125, -0.0105, -0.0143,  0.0295, -0.0229, -0.0220, -0.0249])\n",
      "Gradient of AB Loss for row 14: tensor([0.0392, 0.0348, 0.0171, 0.0202, 0.0228, 0.0521, 0.0249, 0.0336, 0.0419,\n",
      "        0.0373, 0.0359, 0.0364, 0.0304, 0.0425])\n",
      "Gradient of AB Loss for row 15: tensor([0.0323, 0.0506, 0.0105, 0.0236, 0.0252, 0.0297, 0.0326, 0.0387, 0.0468,\n",
      "        0.0555, 0.0376, 0.0215, 0.0333, 0.0406])\n",
      "Gradient of AB Loss for row 16: tensor([-0.1128, -0.1188, -0.1053, -0.1181])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0255,  0.0147, -0.0154,  0.0226,  0.0217, -0.0237,  0.0173,  0.0095,\n",
      "         0.0333,  0.0160,  0.0286, -0.0215,  0.0169,  0.0280, -0.0271, -0.0136,\n",
      "        -0.0157,  0.0295,  0.0133,  0.0245, -0.0227,  0.0175])\n",
      "Gradient of AB Loss for row 18: tensor([-0.5105])\n",
      "Gradient of AB Loss for row 19: tensor([0.0279, 0.0337, 0.0376, 0.0260, 0.0396, 0.0399, 0.0299, 0.0413, 0.0273,\n",
      "        0.0437, 0.0349, 0.0275, 0.0276, 0.0385])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0423, -0.0318, -0.0434, -0.0471, -0.0385,  0.0413, -0.0361,  0.0319,\n",
      "         0.0303,  0.0460,  0.0457, -0.0382, -0.0491, -0.0311, -0.0298])\n",
      "Gradient of AB Loss for row 21: tensor([ 0.1570,  0.1515, -0.0987,  0.1324])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0383,  0.0364, -0.0368,  0.0373,  0.0236,  0.0223,  0.0517,  0.0465,\n",
      "         0.0425,  0.0566,  0.0270,  0.0331,  0.0340,  0.0363])\n",
      "Gradient of AB Loss for row 23: tensor([ 0.0370, -0.0424,  0.0447,  0.0180,  0.0375, -0.0371,  0.0445,  0.0329,\n",
      "         0.0318,  0.0500, -0.0445,  0.0302, -0.0202,  0.0221,  0.0237])\n",
      "Gradient of AB Loss for row 24: tensor([ 0.0197, -0.0156, -0.0214,  0.0349,  0.0120, -0.0264,  0.0239, -0.0108,\n",
      "        -0.0160, -0.0161, -0.0346,  0.0220,  0.0177, -0.0215, -0.0257,  0.0282,\n",
      "         0.0248, -0.0216,  0.0173, -0.0135, -0.0155, -0.0254])\n",
      "Gradient of AB Loss for row 25: tensor([ 0.1726,  0.1544, -0.0978,  0.1684])\n",
      "Gradient of AB Loss for row 26: tensor([-0.6269])\n",
      "Gradient of AB Loss for row 27: tensor([0.0378, 0.0350, 0.0127, 0.0310, 0.0446, 0.0317, 0.0317, 0.0295, 0.0277,\n",
      "        0.0446, 0.0323, 0.0255, 0.0247, 0.0466])\n",
      "Gradient of AB Loss for row 28: tensor([0.0401, 0.0402, 0.0188, 0.0279, 0.0248, 0.0379, 0.0439, 0.0347, 0.0488,\n",
      "        0.0454, 0.0418, 0.0163, 0.0314, 0.0318])\n",
      "Gradient of AB Loss for row 29: tensor([0.1344, 0.1342, 0.1555, 0.1531])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0439, -0.0408, -0.0360, -0.0187, -0.0358,  0.0437, -0.0346, -0.0216,\n",
      "         0.0320,  0.0401,  0.0212, -0.0439, -0.0207, -0.0351, -0.0189])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0237, -0.0321, -0.0374,  0.0202, -0.0126,  0.0416, -0.0342,  0.0351,\n",
      "        -0.0281,  0.0317,  0.0305,  0.0189, -0.0291, -0.0397, -0.0321])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08292680233716965\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.07453355193138123\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.14612728357315063\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.8385254144668579\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.0903550237417221\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.14592280983924866\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.22195269167423248\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.11467050015926361\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.011271106079220772\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.1363852471113205\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.358425816277986e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.2660701870918274\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07183670997619629\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.12209364026784897\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.01008796039968729\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.1387990117073059\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.2563724505199048e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2634735703468323\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.0697469711303711\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.15461485087871552\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01754993572831154\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.16925503313541412\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.1457774728995673e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.27614954113960266\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.07268675416707993\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.1332770138978958\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.01357259787619114\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.12697865068912506\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.2195113807678126e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.31511932611465454\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07405836135149002\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11611063778400421\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.011560434475541115\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.11679502576589584\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 8.903515147373753e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.2982180714607239\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.0718378946185112\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.11255688965320587\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.011214627884328365\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.10906954854726791\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.1669262223179544e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.2960347831249237\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.07486255466938019\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.1282668262720108\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.011977344751358032\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.12851572036743164\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 9.661176303410457e-10\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.2845352292060852\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.07044750452041626\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.13043470680713654\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.016820987686514854\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.1358134150505066\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.3340298865571754e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3223162591457367\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.0751340389251709\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.422703266143799\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.35743141174316406\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04511177912354469\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.05449557304382324\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.31793326139450073\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.03724193572998047\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3369792401790619\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.11536134779453278\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.8764311671257019\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.08955537527799606\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.16139580309391022\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.19622404873371124\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06593821942806244\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.005790482275187969\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.06558338552713394\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.2426721873737279e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2523908317089081\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05237970128655434\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.08265810459852219\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.007760890759527683\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.08818542957305908\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 8.955595709458919e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.29826077818870544\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06447889655828476\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.07171419262886047\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006236592307686806\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06849458813667297\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.1733402027758189e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.31421083211898804\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06343753635883331\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.06544412672519684\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.00544655229896307\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.06422708183526993\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 9.093392150383295e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.2839857339859009\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06074919551610947\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.09453948587179184\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.008114287629723549\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.08398176729679108\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 6.644196304250727e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2639891803264618\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.053202882409095764\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.07436296343803406\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.006872440688312054\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.07741124927997589\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 9.830127822851864e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.25948405265808105\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.0576632134616375\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.07724632322788239\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007203113287687302\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07394187152385712\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.655864275868396e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.26701483130455017\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.058904655277729034\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.06987419724464417\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.007259991019964218\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.06851740181446075\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 7.120982137287513e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.28202033042907715\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06157088279724121\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.3820383548736572\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.2955559194087982\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04876492917537689\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05151772126555443\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.28614333271980286\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.027850229293107986\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.31342992186546326\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.10098367929458618\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.7187492847442627\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.18367421627044678\n",
      "ab_loss: 25.143478393554688\n",
      "Gradient of AB Loss for row 0: tensor([0.0434, 0.0530, 0.0206, 0.0353, 0.0541, 0.0340, 0.0261, 0.0260, 0.0505,\n",
      "        0.0317, 0.0336, 0.0410, 0.0324, 0.0309])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0411, -0.0543, -0.0460,  0.0508,  0.0460, -0.0421,  0.0405,  0.0477,\n",
      "         0.0418,  0.0565,  0.0455,  0.0354, -0.0327,  0.0319])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0163, -0.0214, -0.0114,  0.0159, -0.0175, -0.0253, -0.0314, -0.0314,\n",
      "        -0.0205, -0.0209, -0.0288, -0.0255, -0.0194, -0.0193, -0.0185, -0.0123,\n",
      "        -0.0250, -0.0071,  0.0163, -0.0295, -0.0312, -0.0157])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0273, -0.0161, -0.0301, -0.0319, -0.0365, -0.0186, -0.0228, -0.0288,\n",
      "        -0.0235, -0.0189, -0.0355, -0.0253, -0.0126, -0.0227, -0.0114, -0.0168,\n",
      "        -0.0145,  0.0248, -0.0180, -0.0184, -0.0227])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0254, -0.0311, -0.0343, -0.0419, -0.0278,  0.0438, -0.0255,  0.0342,\n",
      "         0.0460,  0.0319,  0.0380,  0.0532, -0.0395, -0.0418, -0.0301])\n",
      "Gradient of AB Loss for row 5: tensor([0.0329, 0.0513, 0.0265, 0.0321, 0.0450, 0.0410, 0.0295, 0.0509, 0.0524,\n",
      "        0.0468, 0.0368, 0.0391, 0.0356])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0242, -0.0351, -0.0213, -0.0378,  0.0360,  0.0338,  0.0281,  0.0173,\n",
      "        -0.0274,  0.0278,  0.0435,  0.0372,  0.0449,  0.0318,  0.0389, -0.0409,\n",
      "        -0.0198,  0.0172])\n",
      "Gradient of AB Loss for row 7: tensor([0.0351, 0.0513, 0.0287, 0.0391, 0.0372, 0.0260, 0.0312, 0.0414, 0.0520,\n",
      "        0.0348, 0.0463, 0.0351, 0.0282, 0.0290])\n",
      "Gradient of AB Loss for row 8: tensor([-0.1419, -0.1022, -0.1132, -0.1148])\n",
      "Gradient of AB Loss for row 9: tensor([0.0416, 0.0412, 0.0328, 0.0346, 0.0346, 0.0351, 0.0444, 0.0366, 0.0579,\n",
      "        0.0271, 0.0367, 0.0296, 0.0312, 0.0233])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0202, -0.0270, -0.0253, -0.0207,  0.0232, -0.0253, -0.0138, -0.0271,\n",
      "         0.0118,  0.0318,  0.0247, -0.0216,  0.0239,  0.0209, -0.0190, -0.0145,\n",
      "        -0.0142,  0.0304,  0.0202, -0.0229, -0.0212])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0465,  0.0388,  0.0146,  0.0284,  0.0391,  0.0415,  0.0345,  0.0371,\n",
      "         0.0481,  0.0520,  0.0430,  0.0293, -0.0455,  0.0283])\n",
      "Gradient of AB Loss for row 12: tensor([-0.7491])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0246, -0.0585, -0.0406, -0.0354,  0.0459, -0.0463,  0.0398,  0.0386,\n",
      "         0.0524,  0.0252,  0.0261, -0.0311, -0.0292,  0.0344])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0234, -0.0272, -0.0396,  0.0178,  0.0226,  0.0196,  0.0289,  0.0464,\n",
      "         0.0336,  0.0199,  0.0336,  0.0280, -0.0211,  0.0310, -0.0238, -0.0386,\n",
      "        -0.0176])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0249, -0.0273,  0.0355,  0.0172,  0.0364, -0.0257,  0.0412,  0.0218,\n",
      "         0.0373,  0.0450,  0.0205, -0.0272, -0.0403,  0.0218,  0.0396])\n",
      "Gradient of AB Loss for row 16: tensor([-0.4950])\n",
      "Gradient of AB Loss for row 17: tensor([0.0372, 0.0480, 0.0212, 0.0331, 0.0395, 0.0359, 0.0249, 0.0320, 0.0599,\n",
      "        0.0352, 0.0362, 0.0442, 0.0221, 0.0207])\n",
      "Gradient of AB Loss for row 18: tensor([0.0317, 0.0473, 0.0275, 0.0320, 0.0443, 0.0409, 0.0283, 0.0467, 0.0410,\n",
      "        0.0399, 0.0328, 0.0333, 0.0260, 0.0240])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0383, -0.0386, -0.0269, -0.0396,  0.0365,  0.0401,  0.0135, -0.0291,\n",
      "         0.0171,  0.0465,  0.0374,  0.0349, -0.0371,  0.0280, -0.0343, -0.0263,\n",
      "         0.0206])\n",
      "Gradient of AB Loss for row 20: tensor([-0.1267, -0.1043, -0.1085, -0.1792])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0424, -0.0538, -0.0374,  0.0315, -0.0448,  0.0367,  0.0547,  0.0341,\n",
      "         0.0607,  0.0417, -0.0420, -0.0367,  0.0245])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0428, -0.0397,  0.0408,  0.0256,  0.0360, -0.0429,  0.0509,  0.0462,\n",
      "         0.0374,  0.0476,  0.0366,  0.0202, -0.0411,  0.0378])\n",
      "Gradient of AB Loss for row 23: tensor([0.0414, 0.0404, 0.0289, 0.0359, 0.0291, 0.0388, 0.0346, 0.0434, 0.0501,\n",
      "        0.0334, 0.0451, 0.0377, 0.0338, 0.0371])\n",
      "Gradient of AB Loss for row 24: tensor([0.0294, 0.0454, 0.0215, 0.0283, 0.0379, 0.0319, 0.0338, 0.0404, 0.0550,\n",
      "        0.0240, 0.0317, 0.0329, 0.0310, 0.0261])\n",
      "Gradient of AB Loss for row 25: tensor([-0.6871])\n",
      "Gradient of AB Loss for row 26: tensor([ 0.0438,  0.0410,  0.0140,  0.0266,  0.0437,  0.0278, -0.0494,  0.0323,\n",
      "         0.0627,  0.0591, -0.0308,  0.0342,  0.0357])\n",
      "Gradient of AB Loss for row 27: tensor([0.0298, 0.0354, 0.0223, 0.0454, 0.0469, 0.0403, 0.0399, 0.0349, 0.0413,\n",
      "        0.0518, 0.0405, 0.0265, 0.0211, 0.0307])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0516, -0.0350,  0.0400,  0.0354, -0.0520,  0.0273,  0.0381,  0.0377,\n",
      "         0.0293,  0.0321,  0.0346,  0.0266, -0.0336,  0.0388])\n",
      "Gradient of AB Loss for row 29: tensor([0.0284, 0.0432, 0.0231, 0.0277, 0.0343, 0.0340, 0.0338, 0.0338, 0.0434,\n",
      "        0.0271, 0.0339, 0.0490, 0.0331, 0.0233])\n",
      "Gradient of AB Loss for row 30: tensor([0.0375, 0.0406, 0.0181, 0.0341, 0.0455, 0.0396, 0.0398, 0.0373, 0.0333,\n",
      "        0.0437, 0.0354, 0.0372, 0.0232, 0.0436])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0337, -0.0222, -0.0352, -0.0228,  0.0278,  0.0357, -0.0246, -0.0206,\n",
      "        -0.0295,  0.0316,  0.0171,  0.0269,  0.0349, -0.0263,  0.0325, -0.0249,\n",
      "         0.0285,  0.0266, -0.0317, -0.0327])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10745850205421448\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.10131143778562546\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.2051701545715332\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.1180897951126099\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.13031691312789917\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.1937158852815628\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.3175448477268219\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1373702883720398\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.011055203154683113\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.14389000833034515\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.4228207501076895e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3604308068752289\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.09959526360034943\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.1306523233652115\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.0157773494720459\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.14466392993927002\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.1719439863000503e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3502439558506012\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.09861729294061661\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.15056847035884857\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.0186762697994709\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.15460547804832458\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.6701618976711075e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.37114155292510986\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.10521329939365387\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.14110097289085388\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.019647223874926567\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.13824930787086487\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.6083581133585767e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.40103617310523987\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.10732164233922958\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.16571606695652008\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.017272114753723145\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.16043201088905334\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.1132283983528168e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3925611078739166\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.11034436523914337\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.14343908429145813\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.011873298324644566\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.1385517567396164\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.551478501227166e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3682726323604584\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.0992608517408371\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.1982814073562622\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.02143266424536705\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.20450294017791748\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.4792709279731753e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.40059664845466614\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.11064748466014862\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.12975606322288513\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.01166302990168333\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.14071941375732422\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.0088427870869054e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.41716915369033813\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.1147429496049881\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 3.191706895828247\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.5431641340255737\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.058880049735307693\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.08063258975744247\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3902151882648468\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.048178721219301224\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.4211360216140747\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.155863419175148\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0716784000396729\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.1158721074461937\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.20542846620082855\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.259254515171051\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08919483423233032\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.008721433579921722\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.09146338701248169\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.1055675264159959e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2961549758911133\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06295125186443329\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.07875242829322815\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.008081345818936825\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.08103695511817932\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 8.931314576798854e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3481457233428955\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.08115019649267197\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08185738325119019\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.008304520510137081\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.07820996642112732\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.1362476515230924e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.38106799125671387\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.08266111463308334\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09039691835641861\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.00752407917752862\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.09389372915029526\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 7.977923877078297e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3404726982116699\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07803000509738922\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.07235001772642136\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.006185904610902071\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.06966733187437057\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.6071907138481833e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.33040156960487366\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.07133461534976959\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.09082407504320145\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.008808263577520847\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09468293190002441\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.7093459980799253e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.33315491676330566\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07749729603528976\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.09697486460208893\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.010272261686623096\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.09622183442115784\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 6.698280818895341e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.36013180017471313\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.08269733190536499\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.10062772035598755\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.009092886000871658\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.08982916176319122\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.368953863246986e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.32505881786346436\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.07404115796089172\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.8772592544555664\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3741610050201416\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05295560136437416\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06656017899513245\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.4382050633430481\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.04804703965783119\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.4135127067565918\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.13375258445739746\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.1129870414733887\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2309325486421585\n",
      "ab_loss: 25.69269371032715\n",
      "Gradient of AB Loss for row 0: tensor([-0.0451,  0.0542,  0.0331, -0.0442,  0.0351,  0.0388,  0.0345,  0.0322,\n",
      "         0.0421,  0.0479,  0.0472,  0.0389, -0.0353,  0.0372])\n",
      "Gradient of AB Loss for row 1: tensor([-0.6899])\n",
      "Gradient of AB Loss for row 2: tensor([-0.8250])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0296, -0.0345,  0.0358,  0.0141,  0.0407, -0.0328,  0.0220,  0.0361,\n",
      "         0.0377,  0.0340,  0.0230, -0.0325, -0.0251,  0.0182,  0.0311])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0337, -0.0575, -0.0430,  0.0541,  0.0436, -0.0227,  0.0372,  0.0343,\n",
      "         0.0450,  0.0537,  0.0435,  0.0329, -0.0311,  0.0363])\n",
      "Gradient of AB Loss for row 5: tensor([0.0456, 0.0548, 0.0226, 0.0362, 0.0335, 0.0432, 0.0356, 0.0495, 0.0567,\n",
      "        0.0516, 0.0456, 0.0325, 0.0219])\n",
      "Gradient of AB Loss for row 6: tensor([0.0314, 0.0404, 0.0226, 0.0386, 0.0395, 0.0310, 0.0378, 0.0380, 0.0260,\n",
      "        0.0425, 0.0322, 0.0231, 0.0323, 0.0418])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0314, -0.0519, -0.0315, -0.0493,  0.0350, -0.0409,  0.0260,  0.0312,\n",
      "         0.0462,  0.0339,  0.0296, -0.0466, -0.0338,  0.0259])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0360,  0.0337,  0.0234,  0.0452, -0.0409,  0.0328,  0.0345,  0.0396,\n",
      "         0.0457, -0.0347,  0.0234, -0.0271, -0.0326,  0.0303,  0.0307])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0281, -0.0297,  0.0399,  0.0321,  0.0127,  0.0364,  0.0392,  0.0291,\n",
      "         0.0314,  0.0340,  0.0418,  0.0302,  0.0232,  0.0321,  0.0223, -0.0324,\n",
      "         0.0218])\n",
      "Gradient of AB Loss for row 10: tensor([0.0358, 0.0554, 0.0343, 0.0481, 0.0377, 0.0421, 0.0465, 0.0522, 0.0414,\n",
      "        0.0402, 0.0495, 0.0418, 0.0339])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0278,  0.0360,  0.0332,  0.0161,  0.0408,  0.0430,  0.0355,  0.0247,\n",
      "         0.0373,  0.0312,  0.0379,  0.0270,  0.0334,  0.0157, -0.0227,  0.0218])\n",
      "Gradient of AB Loss for row 12: tensor([0.0326, 0.0390, 0.0145, 0.0326, 0.0361, 0.0275, 0.0327, 0.0332, 0.0323,\n",
      "        0.0230, 0.0399, 0.0319, 0.0273, 0.0364])\n",
      "Gradient of AB Loss for row 13: tensor([0.0338, 0.0388, 0.0299, 0.0157, 0.0355, 0.0286, 0.0346, 0.0349, 0.0391,\n",
      "        0.0416, 0.0334, 0.0255, 0.0419, 0.0337])\n",
      "Gradient of AB Loss for row 14: tensor([ 0.0359, -0.0523,  0.0413,  0.0205,  0.0207, -0.0330,  0.0431,  0.0377,\n",
      "         0.0386,  0.0347,  0.0414,  0.0218,  0.0304,  0.0251])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0376, -0.0386,  0.0310,  0.0200,  0.0146, -0.0243,  0.0242, -0.0383,\n",
      "         0.0260,  0.0322,  0.0310,  0.0351,  0.0298,  0.0238, -0.0322])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0316, -0.0171,  0.0353,  0.0374,  0.0166, -0.0240, -0.0263,  0.0437,\n",
      "         0.0214,  0.0383,  0.0408,  0.0324,  0.0402,  0.0165, -0.0288,  0.0230,\n",
      "         0.0373])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0295, -0.0312, -0.0377,  0.0334,  0.0277, -0.0260,  0.0296,  0.0210,\n",
      "         0.0404,  0.0433,  0.0497,  0.0389, -0.0395, -0.0299, -0.0258])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0705,  0.0654,  0.0364,  0.0420,  0.0546,  0.0415,  0.0429,  0.0282,\n",
      "         0.0338,  0.0362, -0.0359,  0.0404])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0367, -0.0351, -0.0473, -0.0436,  0.0503,  0.0484, -0.0520,  0.0294,\n",
      "         0.0325,  0.0495,  0.0309, -0.0443, -0.0384, -0.0213])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0260, -0.0285, -0.0196,  0.0432,  0.0367,  0.0221, -0.0310, -0.0297,\n",
      "         0.0368,  0.0290,  0.0254,  0.0436,  0.0258,  0.0464,  0.0133, -0.0209,\n",
      "         0.0232])\n",
      "Gradient of AB Loss for row 21: tensor([ 0.0449, -0.0363,  0.0484,  0.0133,  0.0416, -0.0338,  0.0421,  0.0347,\n",
      "         0.0326,  0.0543,  0.0213,  0.0336, -0.0427,  0.0295])\n",
      "Gradient of AB Loss for row 22: tensor([-0.6124])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0234, -0.0334, -0.0305, -0.0336,  0.0375,  0.0257,  0.0296,  0.0202,\n",
      "        -0.0326,  0.0370,  0.0371,  0.0248,  0.0396,  0.0259,  0.0380, -0.0365,\n",
      "        -0.0315,  0.0154])\n",
      "Gradient of AB Loss for row 24: tensor([0.0403, 0.0457, 0.0278, 0.0298, 0.0276, 0.0303, 0.0230, 0.0355, 0.0539,\n",
      "        0.0270, 0.0354, 0.0452, 0.0221, 0.0291])\n",
      "Gradient of AB Loss for row 25: tensor([ 0.0417, -0.0389,  0.0618,  0.0262,  0.0381,  0.0549,  0.0304,  0.0343,\n",
      "         0.0319,  0.0526,  0.0491,  0.0223,  0.0287,  0.0272])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0480, -0.0419,  0.0437,  0.0180,  0.0342,  0.0367,  0.0372,  0.0520,\n",
      "         0.0306,  0.0456,  0.0373,  0.0310, -0.0392,  0.0273])\n",
      "Gradient of AB Loss for row 27: tensor([ 0.0317, -0.0193,  0.0236,  0.0345,  0.0397,  0.0198,  0.0269,  0.0240,\n",
      "         0.0287,  0.0168,  0.0264,  0.0196,  0.0172, -0.0116,  0.0343,  0.0317,\n",
      "         0.0276,  0.0264,  0.0115])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0387, -0.0478,  0.0247,  0.0527, -0.0505,  0.0349,  0.0494,  0.0478,\n",
      "         0.0512,  0.0351,  0.0387, -0.0435,  0.0332])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0469, -0.0356, -0.0162,  0.0413,  0.0233, -0.0265,  0.0381,  0.0410,\n",
      "         0.0330,  0.0319,  0.0380,  0.0419, -0.0331, -0.0372, -0.0252, -0.0204])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0307, -0.0339,  0.0473,  0.0188,  0.0478, -0.0432,  0.0199,  0.0310,\n",
      "         0.0436,  0.0415, -0.0434,  0.0291, -0.0348,  0.0221,  0.0312])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0396, -0.0229, -0.0431, -0.0463, -0.0355,  0.0241, -0.0465,  0.0490,\n",
      "         0.0425,  0.0478,  0.0415, -0.0419, -0.0379, -0.0236])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.10455849766731262\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.10780265182256699\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1846194565296173\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0865652561187744\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.1277555525302887\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.21032273769378662\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.31432485580444336\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.153989776968956\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.014452415518462658\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.16717109084129333\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.88009630086583e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.35530680418014526\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.10802923142910004\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.1306275576353073\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.014113090932369232\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.148336723446846\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 9.952642043842275e-10\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.31742921471595764\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.09109698235988617\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1206066682934761\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.015342196449637413\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.12462976574897766\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.5516459228592794e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.33998408913612366\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09422581642866135\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.15014022588729858\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.013265157118439674\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.1496826857328415\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.5549339593690092e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.38226091861724854\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.09854213148355484\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11032900959253311\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.010110410861670971\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.11631377041339874\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.076853495263208e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.38922756910324097\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.09527979046106339\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.12100116163492203\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.014062074013054371\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.1280873417854309\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.875197996881184e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3506191372871399\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09346145391464233\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.15901514887809753\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01709732785820961\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.1768217384815216\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.598447152417748e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3469514846801758\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.09305290132761002\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.18315935134887695\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.022802550345659256\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.18018975853919983\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.506882174595603e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3587181568145752\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.08858288079500198\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.886352777481079\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.44814547896385193\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.046320974826812744\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06695403903722763\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.39663928747177124\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04923081398010254\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.4321364462375641\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.15497282147407532\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0839308500289917\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.1210395023226738\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.2076253592967987\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2789970636367798\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.07434318959712982\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.006423982325941324\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.07552409172058105\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.0154471707934931e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.31693899631500244\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06663055717945099\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09647854417562485\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.00934501551091671\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09528741240501404\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.7705747978880026e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.32392293214797974\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07178183645009995\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.11995372921228409\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.013811975717544556\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.10996068269014359\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 8.527274442116095e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.36841705441474915\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.07550929486751556\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.11429483443498611\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.011460473760962486\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.10792259126901627\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 7.720276085088074e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.345716655254364\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.0783439502120018\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08672535419464111\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.007038045208901167\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.08123769611120224\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.2117172820680366e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3349815607070923\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.07022545486688614\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.10316310822963715\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.009143632836639881\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.11177851259708405\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.0673993910970125e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3117215931415558\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06868010014295578\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.09154416620731354\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007755147758871317\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.0941706970334053\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 2.4987985014490732e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3174275755882263\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07041414827108383\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07788599282503128\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.007062562275677919\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07235664129257202\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 3.1107141307273878e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.30795103311538696\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06759423762559891\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.872295379638672\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.361831933259964\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04683345928788185\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06365765631198883\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.37050163745880127\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.0400749035179615\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3840741217136383\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.13596664369106293\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.243374824523926\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.26119929552078247\n",
      "ab_loss: 23.709930419921875\n",
      "Gradient of AB Loss for row 0: tensor([0.0366, 0.0461, 0.0161, 0.0286, 0.0356, 0.0316, 0.0330, 0.0370, 0.0459,\n",
      "        0.0429, 0.0426, 0.0263, 0.0278, 0.0372])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0382,  0.0505,  0.0142,  0.0477, -0.0427,  0.0387,  0.0419,  0.0386,\n",
      "         0.0491,  0.0271,  0.0297,  0.0314, -0.0349,  0.0289])\n",
      "Gradient of AB Loss for row 2: tensor([0.0258, 0.0467, 0.0226, 0.0339, 0.0375, 0.0350, 0.0385, 0.0473, 0.0484,\n",
      "        0.0297, 0.0349, 0.0416, 0.0189, 0.0189])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0312, -0.0344, -0.0345,  0.0115, -0.0187,  0.0469, -0.0416,  0.0303,\n",
      "        -0.0335,  0.0265,  0.0247,  0.0262, -0.0295, -0.0384, -0.0297])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0258, -0.0283,  0.0277,  0.0156,  0.0123, -0.0282,  0.0326, -0.0507,\n",
      "         0.0374,  0.0384,  0.0453,  0.0427,  0.0386,  0.0255, -0.0343])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0420, -0.0297, -0.0440, -0.0474, -0.0214,  0.0422, -0.0341,  0.0361,\n",
      "         0.0436,  0.0456,  0.0403, -0.0374, -0.0415, -0.0248])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0562, -0.0400,  0.0513,  0.0203,  0.0326, -0.0551,  0.0320,  0.0429,\n",
      "         0.0557,  0.0346,  0.0416, -0.0481, -0.0266,  0.0309])\n",
      "Gradient of AB Loss for row 7: tensor([-0.1150, -0.0539, -0.0945,  0.0907, -0.0781, -0.0717,  0.1031])\n",
      "Gradient of AB Loss for row 8: tensor([0.0388, 0.0372, 0.0115, 0.0299, 0.0276, 0.0356, 0.0412, 0.0385, 0.0357,\n",
      "        0.0366, 0.0344, 0.0262, 0.0291, 0.0421])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0293, -0.0470, -0.0304,  0.0189,  0.0279,  0.0166,  0.0239,  0.0459,\n",
      "         0.0389,  0.0334,  0.0366,  0.0470,  0.0329,  0.0376, -0.0343, -0.0311])\n",
      "Gradient of AB Loss for row 10: tensor([0.0288, 0.0309, 0.0304, 0.0436, 0.0433, 0.0330, 0.0464, 0.0254, 0.0390,\n",
      "        0.0221, 0.0313, 0.0271, 0.0341, 0.0314])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0392,  0.0395, -0.0409,  0.0321,  0.0221,  0.0421,  0.0420,  0.0435,\n",
      "         0.0535,  0.0500,  0.0361,  0.0302,  0.0346])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0577, -0.0508,  0.0382,  0.0290, -0.0343,  0.0445,  0.0450,  0.0413,\n",
      "         0.0483,  0.0472,  0.0376,  0.0163, -0.0353,  0.0471])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0156,  0.0239,  0.0276, -0.0186, -0.0185, -0.0211, -0.0247, -0.0162,\n",
      "         0.0215,  0.0204, -0.0168, -0.0228, -0.0208, -0.0262,  0.0113, -0.0215,\n",
      "        -0.0185,  0.0271, -0.0153, -0.0154, -0.0193, -0.0249, -0.0196, -0.0075,\n",
      "        -0.0236])\n",
      "Gradient of AB Loss for row 14: tensor([ 0.0167,  0.0302, -0.0211,  0.0145,  0.0261,  0.0251,  0.0287, -0.0149,\n",
      "         0.0276,  0.0200,  0.0120,  0.0355,  0.0251,  0.0158,  0.0260, -0.0141,\n",
      "         0.0245,  0.0283,  0.0292, -0.0307,  0.0194,  0.0234])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0300,  0.0296, -0.0455,  0.0508,  0.0111,  0.0445, -0.0409,  0.0217,\n",
      "         0.0334,  0.0355,  0.0381, -0.0409, -0.0248,  0.0234,  0.0324])\n",
      "Gradient of AB Loss for row 16: tensor([0.0426, 0.0361, 0.0205, 0.0464, 0.0496, 0.0225, 0.0476, 0.0371, 0.0559,\n",
      "        0.0288, 0.0366, 0.0335, 0.0293])\n",
      "Gradient of AB Loss for row 17: tensor([0.0377, 0.0340, 0.0300, 0.0238, 0.0481, 0.0416, 0.0291, 0.0422, 0.0508,\n",
      "        0.0353, 0.0334, 0.0423, 0.0387, 0.0350])\n",
      "Gradient of AB Loss for row 18: tensor([ 0.0360, -0.0393,  0.0440,  0.0411,  0.0273,  0.0342,  0.0336,  0.0439,\n",
      "         0.0304,  0.0495,  0.0233,  0.0371,  0.0239,  0.0186])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0344, -0.0201, -0.0391, -0.0352, -0.0182, -0.0204, -0.0257, -0.0256,\n",
      "        -0.0370, -0.0347, -0.0275, -0.0101, -0.0139,  0.0306, -0.0346, -0.0256,\n",
      "         0.0243])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0388,  0.0403,  0.0211,  0.0393, -0.0428,  0.0320,  0.0274,  0.0319,\n",
      "         0.0388,  0.0438,  0.0406, -0.0286, -0.0307,  0.0207,  0.0296])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0179, -0.0226,  0.0137, -0.0162, -0.0283, -0.0312, -0.0137, -0.0238,\n",
      "        -0.0190, -0.0284, -0.0195,  0.0117,  0.0231, -0.0207, -0.0129, -0.0206,\n",
      "        -0.0148,  0.0238, -0.0307, -0.0241,  0.0179])\n",
      "Gradient of AB Loss for row 22: tensor([0.0202, 0.0473, 0.0118, 0.0327, 0.0492, 0.0233, 0.0347, 0.0382, 0.0460,\n",
      "        0.0394, 0.0356, 0.0357, 0.0282, 0.0244])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0969, -0.1623, -0.1641])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0372, -0.0472,  0.0366,  0.0157,  0.0493,  0.0488,  0.0483,  0.0543,\n",
      "         0.0582,  0.0457,  0.0328, -0.0409,  0.0292])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0158,  0.0209, -0.0382,  0.0415,  0.0222,  0.0163,  0.0394,  0.0263,\n",
      "         0.0410,  0.0395,  0.0302,  0.0222,  0.0267, -0.0415, -0.0328,  0.0218,\n",
      "         0.0157])\n",
      "Gradient of AB Loss for row 26: tensor([0.0239, 0.0253, 0.0224, 0.0292, 0.0243, 0.0216, 0.0260, 0.0277, 0.0204,\n",
      "        0.0247, 0.0242, 0.0138, 0.0320, 0.0213, 0.0310, 0.0197, 0.0232, 0.0280,\n",
      "        0.0231, 0.0269, 0.0255, 0.0236, 0.0217])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0341, -0.0280,  0.0361, -0.0431,  0.0306,  0.0267, -0.0398,  0.0334,\n",
      "         0.0403,  0.0406,  0.0364,  0.0404,  0.0296, -0.0353])\n",
      "Gradient of AB Loss for row 28: tensor([-0.5323])\n",
      "Gradient of AB Loss for row 29: tensor([0.0957, 0.1771, 0.1397, 0.1282])\n",
      "Gradient of AB Loss for row 30: tensor([0.0332, 0.0369, 0.0195, 0.0301, 0.0518, 0.0396, 0.0396, 0.0421, 0.0398,\n",
      "        0.0375, 0.0294, 0.0384, 0.0276, 0.0280])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0339, -0.0318, -0.0285, -0.0351,  0.0349,  0.0441,  0.0330,  0.0189,\n",
      "        -0.0258,  0.0378,  0.0471,  0.0339,  0.0423,  0.0378,  0.0298, -0.0303,\n",
      "         0.0129])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.1043688952922821\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.11327287554740906\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.20355702936649323\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0604755878448486\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.12032826989889145\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.1770092099905014\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.285820871591568\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1684713065624237\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.01718730852007866\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.19295136630535126\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.4026477757056455e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3090885281562805\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.09069716185331345\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.2399720996618271\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.024050530046224594\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.27577701210975647\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.5459746816048892e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3407340943813324\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.09079451113939285\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1898450255393982\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.020145494490861893\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.21177899837493896\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.1059896332099584e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3526792526245117\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.10258352756500244\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.1118079274892807\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.01000720914453268\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.11741654574871063\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.0529054295105311e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.33051687479019165\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.09279173612594604\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.13789963722229004\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.014866902492940426\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.1301146298646927\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.4780234813827064e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3705732226371765\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.10058252513408661\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.14672116935253143\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.01273891981691122\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.152995765209198\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.4668235515102879e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.35240188241004944\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.0972447618842125\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.1514144390821457\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.017180250957608223\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.1610563099384308\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.4555189276066471e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.36088332533836365\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.09759770333766937\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.1474362462759018\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.012873318046331406\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.15719693899154663\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.6418415516028517e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.38803473114967346\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.10633911937475204\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.9370031356811523\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4885886311531067\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.05143190175294876\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.0688818171620369\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.38861891627311707\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04691150039434433\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.4054352343082428\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.14638139307498932\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0334196090698242\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.1030912846326828\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.18085502088069916\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.21631452441215515\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.08235571533441544\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.006743886973708868\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.08720787614583969\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 9.898686315068517e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.3044184148311615\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.062474507838487625\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09835177659988403\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.008745431900024414\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09949424117803574\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.1443692660151328e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3139619827270508\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06650616973638535\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.09163644909858704\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.009499765001237392\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08364835381507874\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 8.715990151841879e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.31232547760009766\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06653798371553421\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.08461907505989075\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.007959157228469849\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.09145592153072357\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 8.991040689743102e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3333705961704254\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07601067423820496\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.09530463069677353\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.009521019645035267\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.08404579758644104\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 9.10190756098217e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3033546805381775\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06308988481760025\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08241371810436249\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.00785452127456665\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.08738098293542862\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.4150691729497566e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3312816321849823\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.0746023878455162\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08546735346317291\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.008354824967682362\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08179337531328201\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.168609039709395e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.30641353130340576\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06719251722097397\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07294289022684097\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.006920676678419113\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.06957094371318817\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.309042343057627e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2883308529853821\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06082947552204132\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.626600980758667\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3319564461708069\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04855388030409813\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.0569283589720726\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3597487211227417\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.035190649330616\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.35396796464920044\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.10855148732662201\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.0043978691101074\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.21465076506137848\n",
      "ab_loss: 23.338773727416992\n",
      "Gradient of AB Loss for row 0: tensor([-0.0306, -0.0224,  0.0417,  0.0338,  0.0380,  0.0170, -0.0311,  0.0326,\n",
      "         0.0427,  0.0356,  0.0355,  0.0258, -0.0341,  0.0193,  0.0102,  0.0231])\n",
      "Gradient of AB Loss for row 1: tensor([0.0367, 0.0388, 0.0225, 0.0275, 0.0351, 0.0341, 0.0413, 0.0401, 0.0478,\n",
      "        0.0269, 0.0427, 0.0364, 0.0265, 0.0212])\n",
      "Gradient of AB Loss for row 2: tensor([0.0308, 0.0521, 0.0208, 0.0242, 0.0387, 0.0298, 0.0381, 0.0344, 0.0368,\n",
      "        0.0460, 0.0371, 0.0279, 0.0270, 0.0400])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0178, -0.0351, -0.0288, -0.0421, -0.0222, -0.0293,  0.0339,  0.0342,\n",
      "        -0.0174, -0.0315, -0.0327,  0.0265,  0.0381, -0.0276, -0.0186,  0.0313,\n",
      "        -0.0316, -0.0435])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0292,  0.0411,  0.0299,  0.0368, -0.0249,  0.0263,  0.0348,  0.0354,\n",
      "         0.0384, -0.0452,  0.0323, -0.0353, -0.0368,  0.0269,  0.0254])\n",
      "Gradient of AB Loss for row 5: tensor([ 0.0372, -0.0415,  0.0330,  0.0241,  0.0157,  0.0315,  0.0386,  0.0313,\n",
      "         0.0457,  0.0497,  0.0360,  0.0428,  0.0249,  0.0280])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0361, -0.0328, -0.0471, -0.0348,  0.0259, -0.0383,  0.0484,  0.0473,\n",
      "         0.0292,  0.0267, -0.0335,  0.0330,  0.0282, -0.0216])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0269, -0.0286, -0.0321, -0.0361,  0.0361,  0.0351,  0.0286,  0.0101,\n",
      "        -0.0211,  0.0345,  0.0399,  0.0383,  0.0359,  0.0345,  0.0208, -0.0374,\n",
      "        -0.0354,  0.0190])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0300, -0.0256, -0.0238,  0.0448,  0.0286,  0.0119, -0.0291,  0.0372,\n",
      "         0.0430,  0.0418,  0.0392,  0.0247,  0.0258,  0.0209,  0.0243, -0.0221,\n",
      "         0.0232])\n",
      "Gradient of AB Loss for row 9: tensor([-0.5100])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0295, -0.0414,  0.0457,  0.0208,  0.0299, -0.0303,  0.0214,  0.0296,\n",
      "         0.0343,  0.0501,  0.0251, -0.0344, -0.0318,  0.0275,  0.0371])\n",
      "Gradient of AB Loss for row 11: tensor([0.0396, 0.0560, 0.0333, 0.0296, 0.0487, 0.0299, 0.0210, 0.0395, 0.0492,\n",
      "        0.0387, 0.0374, 0.0303, 0.0397, 0.0320])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0350, -0.0231,  0.0397,  0.0396,  0.0123, -0.0267,  0.0352, -0.0431,\n",
      "         0.0311,  0.0295,  0.0234,  0.0326,  0.0184,  0.0309, -0.0388])\n",
      "Gradient of AB Loss for row 13: tensor([-0.4293])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0333, -0.0532,  0.0295, -0.0369,  0.0382,  0.0541,  0.0320,  0.0482,\n",
      "         0.0417,  0.0288, -0.0367, -0.0214,  0.0357])\n",
      "Gradient of AB Loss for row 15: tensor([-0.4487])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0381, -0.0464,  0.0523,  0.0123,  0.0360,  0.0321,  0.0393,  0.0406,\n",
      "         0.0443,  0.0278,  0.0343,  0.0288, -0.0338,  0.0323])\n",
      "Gradient of AB Loss for row 17: tensor([0.0274, 0.0329, 0.0238, 0.0321, 0.0455, 0.0432, 0.0437, 0.0480, 0.0454,\n",
      "        0.0347, 0.0373, 0.0284, 0.0326, 0.0382])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0338, -0.0336, -0.0321, -0.0505,  0.0415,  0.0198, -0.0268, -0.0338,\n",
      "         0.0396, -0.0315,  0.0368, -0.0390,  0.0216, -0.0236, -0.0140])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0330,  0.0315,  0.0334,  0.0182,  0.0329,  0.0398,  0.0266,  0.0396,\n",
      "         0.0388,  0.0272,  0.0221,  0.0298, -0.0285,  0.0237, -0.0195,  0.0128,\n",
      "         0.0247])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0162, -0.0221, -0.0176, -0.0196, -0.0215, -0.0230, -0.0272,  0.0213,\n",
      "        -0.0155, -0.0286, -0.0217, -0.0276,  0.0228, -0.0248, -0.0150,  0.0140,\n",
      "        -0.0193, -0.0091, -0.0120, -0.0104,  0.0307, -0.0238, -0.0194])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0355,  0.0547, -0.0477,  0.0423,  0.0300, -0.0601,  0.0550,  0.0343,\n",
      "         0.0591,  0.0489,  0.0349,  0.0344,  0.0412])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0524,  0.0539,  0.0342, -0.0376,  0.0328,  0.0307,  0.0306,  0.0314,\n",
      "         0.0364,  0.0293,  0.0508,  0.0385, -0.0366,  0.0355])\n",
      "Gradient of AB Loss for row 23: tensor([0.0454, 0.0376, 0.0233, 0.0261, 0.0350, 0.0344, 0.0433, 0.0390, 0.0603,\n",
      "        0.0328, 0.0441, 0.0435, 0.0344])\n",
      "Gradient of AB Loss for row 24: tensor([-0.1377, -0.1394, -0.0999, -0.1414])\n",
      "Gradient of AB Loss for row 25: tensor([0.0264, 0.0565, 0.0139, 0.0422, 0.0364, 0.0394, 0.0274, 0.0412, 0.0497,\n",
      "        0.0365, 0.0353, 0.0345, 0.0235, 0.0260])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0384, -0.0381, -0.0371, -0.0469,  0.0512, -0.0415,  0.0284,  0.0397,\n",
      "         0.0423,  0.0359,  0.0263, -0.0360, -0.0191,  0.0189])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0190, -0.0286, -0.0241, -0.0463, -0.0331, -0.0264,  0.0398,  0.0234,\n",
      "        -0.0303,  0.0339,  0.0421,  0.0403,  0.0379, -0.0322, -0.0328,  0.0248])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0290,  0.0360, -0.0464,  0.0390,  0.0142,  0.0262,  0.0504,  0.0416,\n",
      "         0.0326,  0.0513,  0.0458,  0.0388,  0.0295,  0.0277])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0303, -0.0596, -0.0359,  0.0487,  0.0432, -0.0301,  0.0395,  0.0517,\n",
      "         0.0327,  0.0505,  0.0246,  0.0325, -0.0419,  0.0300])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0466,  0.0507,  0.0264,  0.0456, -0.0473,  0.0283,  0.0325,  0.0363,\n",
      "         0.0497,  0.0412,  0.0152, -0.0405, -0.0223,  0.0313])\n",
      "Gradient of AB Loss for row 31: tensor([0.1639, 0.1453, 0.1532, 0.1037])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.1026419997215271\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.1115553230047226\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.1761862188577652\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0889194011688232\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.12695284187793732\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.1990557610988617\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.3088170886039734\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.16106462478637695\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.013325409963726997\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.18597085773944855\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.7560948251116315e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3354850113391876\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.08529616892337799\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.14479932188987732\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.013404685072600842\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.15458065271377563\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 2.3745938548813683e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.34182193875312805\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08004545420408249\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.16524355113506317\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01857391558587551\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.17181475460529327\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.4169075912562334e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3484722077846527\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.07931460440158844\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.14060033857822418\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.015057230368256569\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.14196258783340454\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.1302005997748665e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.34170001745224\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07575333118438721\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.12597309052944183\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.011630174703896046\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.12430475652217865\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.4331873465778244e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3497815430164337\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07068275660276413\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.13279376924037933\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.012661009095609188\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.13298290967941284\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.0283675022648708e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.368939071893692\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08797958493232727\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.1919419914484024\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.022320499643683434\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.2131025642156601\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.7590838785608298e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3554220199584961\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.08439268171787262\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.11218446493148804\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.013895305804908276\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.10945676267147064\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.7185824985332943e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3607143759727478\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07982572168111801\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.9423723220825195\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.41412681341171265\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.05775078758597374\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06829836219549179\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.380126416683197\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.045780833810567856\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.41798797249794006\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1569325476884842\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.1276618242263794\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.12454598397016525\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.19436155259609222\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2696945071220398\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.10711580514907837\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.01009915117174387\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.11101935058832169\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.2629881585013436e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.34509339928627014\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.07025059312582016\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.11344928294420242\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.010415381751954556\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.12034948170185089\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.210432251270049e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.35791096091270447\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07603640109300613\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.09562528878450394\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.007391383405774832\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.09382379055023193\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 7.623208730933584e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.43129414319992065\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.08571629971265793\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.10678453743457794\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.010166495107114315\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.09677676111459732\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 1.1316531045579836e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3585982918739319\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07520625740289688\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.07167226076126099\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.005759177729487419\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.07024097442626953\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.0422290808165258e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.33506062626838684\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06979789584875107\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.0911211147904396\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.006871487479656935\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09699437767267227\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.2683618599851343e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.32802075147628784\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07317856699228287\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.11820736527442932\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.010772592388093472\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.11606667190790176\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 8.605409163031652e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3628219664096832\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07547423243522644\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.08547284454107285\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.007275792770087719\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07823151350021362\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.5514223239421199e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.3427183926105499\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.07319282740354538\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 3.00917387008667\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.37843039631843567\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.058308687061071396\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.07006467878818512\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.39769235253334045\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.04383998364210129\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3741177022457123\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.13264276087284088\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.1193902492523193\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2497461885213852\n",
      "ab_loss: 23.604671478271484\n",
      "Gradient of AB Loss for row 0: tensor([0.0369, 0.0506, 0.0168, 0.0279, 0.0190, 0.0296, 0.0301, 0.0413, 0.0540,\n",
      "        0.0395, 0.0362, 0.0251, 0.0235, 0.0361])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0318, -0.0279, -0.0429, -0.0478, -0.0243, -0.0325,  0.0300, -0.0324,\n",
      "         0.0305,  0.0315,  0.0348,  0.0339, -0.0279, -0.0351, -0.0265, -0.0314,\n",
      "        -0.0241])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0380,  0.0423,  0.0268,  0.0395, -0.0383,  0.0220,  0.0278,  0.0303,\n",
      "         0.0377,  0.0399,  0.0207, -0.0390, -0.0293,  0.0187,  0.0264])\n",
      "Gradient of AB Loss for row 3: tensor([0.0398, 0.0487, 0.0141, 0.0320, 0.0238, 0.0444, 0.0307, 0.0390, 0.0553,\n",
      "        0.0493, 0.0451, 0.0225, 0.0389, 0.0372])\n",
      "Gradient of AB Loss for row 4: tensor([0.0330, 0.0549, 0.0314, 0.0216, 0.0414, 0.0225, 0.0231, 0.0357, 0.0494,\n",
      "        0.0444, 0.0294, 0.0287, 0.0208, 0.0354])\n",
      "Gradient of AB Loss for row 5: tensor([ 0.0371, -0.0415,  0.0326,  0.0249,  0.0273,  0.0449,  0.0365,  0.0369,\n",
      "         0.0333,  0.0506,  0.0408,  0.0329,  0.0305,  0.0221])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0164,  0.0164,  0.0157, -0.0134, -0.0240, -0.0283, -0.0235,  0.0177,\n",
      "         0.0202,  0.0338,  0.0122, -0.0169,  0.0273,  0.0301,  0.0318,  0.0292,\n",
      "        -0.0122,  0.0249, -0.0186, -0.0172,  0.0190,  0.0225, -0.0201, -0.0291])\n",
      "Gradient of AB Loss for row 7: tensor([0.0340, 0.0493, 0.0260, 0.0218, 0.0304, 0.0306, 0.0358, 0.0505, 0.0501,\n",
      "        0.0246, 0.0398, 0.0254, 0.0289, 0.0358])\n",
      "Gradient of AB Loss for row 8: tensor([0.0449, 0.0396, 0.0146, 0.0379, 0.0342, 0.0468, 0.0418, 0.0402, 0.0458,\n",
      "        0.0318, 0.0488, 0.0428, 0.0256, 0.0303])\n",
      "Gradient of AB Loss for row 9: tensor([0.4201])\n",
      "Gradient of AB Loss for row 10: tensor([0.0402, 0.0398, 0.0188, 0.0248, 0.0305, 0.0488, 0.0406, 0.0466, 0.0343,\n",
      "        0.0288, 0.0334, 0.0381, 0.0288, 0.0278])\n",
      "Gradient of AB Loss for row 11: tensor([0.1518, 0.1566, 0.0991, 0.1574])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0361,  0.0359,  0.0192,  0.0480, -0.0303,  0.0433,  0.0380,  0.0416,\n",
      "         0.0267,  0.0458,  0.0199,  0.0312, -0.0372,  0.0223])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0168, -0.0238, -0.0177, -0.0148, -0.0185, -0.0357, -0.0254,  0.0263,\n",
      "         0.0198, -0.0171, -0.0244, -0.0268,  0.0130, -0.0196,  0.0310, -0.0210,\n",
      "        -0.0224, -0.0272, -0.0154,  0.0169, -0.0293, -0.0250])\n",
      "Gradient of AB Loss for row 14: tensor([ 0.0333, -0.0571,  0.0520,  0.0303,  0.0390, -0.0578,  0.0304,  0.0432,\n",
      "         0.0495,  0.0533,  0.0538, -0.0398, -0.0426,  0.0429])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0335, -0.0330,  0.0291,  0.0160,  0.0383, -0.0333,  0.0457,  0.0367,\n",
      "         0.0411,  0.0381,  0.0234, -0.0333, -0.0158,  0.0161,  0.0307])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0230, -0.0275, -0.0308, -0.0313,  0.0405,  0.0238,  0.0229,  0.0324,\n",
      "         0.0393,  0.0442,  0.0408,  0.0323,  0.0375, -0.0340, -0.0362,  0.0190])\n",
      "Gradient of AB Loss for row 17: tensor([ 0.0417, -0.0428,  0.0300,  0.0279,  0.0415,  0.0358,  0.0382,  0.0447,\n",
      "         0.0427,  0.0616,  0.0270,  0.0247,  0.0197,  0.0163])\n",
      "Gradient of AB Loss for row 18: tensor([0.0350, 0.0525, 0.0263, 0.0292, 0.0482, 0.0361, 0.0357, 0.0439, 0.0446,\n",
      "        0.0305, 0.0353, 0.0340, 0.0287, 0.0304])\n",
      "Gradient of AB Loss for row 19: tensor([0.0356, 0.0467, 0.0169, 0.0372, 0.0263, 0.0281, 0.0449, 0.0358, 0.0424,\n",
      "        0.0458, 0.0400, 0.0256, 0.0353, 0.0445])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0257, -0.0282, -0.0259, -0.0376,  0.0380,  0.0113, -0.0123, -0.0232,\n",
      "         0.0233,  0.0299,  0.0310,  0.0318, -0.0376, -0.0242, -0.0289,  0.0178,\n",
      "        -0.0223])\n",
      "Gradient of AB Loss for row 21: tensor([0.0428, 0.0414, 0.0134, 0.0328, 0.0490, 0.0483, 0.0273, 0.0467, 0.0487,\n",
      "        0.0335, 0.0406, 0.0303, 0.0238, 0.0246])\n",
      "Gradient of AB Loss for row 22: tensor([0.1300, 0.1538, 0.1033, 0.1746])\n",
      "Gradient of AB Loss for row 23: tensor([-0.2877, -0.2694])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0279, -0.0317, -0.0437,  0.0319,  0.0129, -0.0245,  0.0276,  0.0333,\n",
      "         0.0318,  0.0259,  0.0267,  0.0449,  0.0235, -0.0237,  0.0180, -0.0213])\n",
      "Gradient of AB Loss for row 25: tensor([0.1092, 0.1288, 0.1360, 0.1409])\n",
      "Gradient of AB Loss for row 26: tensor([0.0348, 0.0522, 0.0141, 0.0409, 0.0409, 0.0265, 0.0297, 0.0478, 0.0601,\n",
      "        0.0455, 0.0485, 0.0281, 0.0239, 0.0198])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0324,  0.0450,  0.0172,  0.0416, -0.0416,  0.0245,  0.0214,  0.0407,\n",
      "         0.0482, -0.0390,  0.0321, -0.0305, -0.0298,  0.0232,  0.0399])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0238,  0.0328, -0.0428,  0.0448,  0.0378,  0.0200,  0.0281,  0.0333,\n",
      "         0.0320,  0.0441,  0.0398,  0.0368,  0.0348,  0.0229])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0490,  0.0249, -0.0400,  0.0344,  0.0163,  0.0218,  0.0360,  0.0327,\n",
      "         0.0482,  0.0466,  0.0421,  0.0477,  0.0273,  0.0376])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0352, -0.0583, -0.0373,  0.0403,  0.0355, -0.0298,  0.0349,  0.0361,\n",
      "         0.0309,  0.0470,  0.0396,  0.0231, -0.0415,  0.0404])\n",
      "Gradient of AB Loss for row 31: tensor([0.0291, 0.0569, 0.0170, 0.0384, 0.0360, 0.0220, 0.0286, 0.0411, 0.0379,\n",
      "        0.0495, 0.0384, 0.0320, 0.0267, 0.0330])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08384298533201218\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.08793039619922638\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.14242440462112427\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.8925008177757263\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.09649621695280075\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.17051832377910614\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.24134516716003418\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.09267851710319519\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.00870498362928629\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.105903260409832\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 9.45013622910551e-10\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.2514348328113556\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.06532883644104004\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.14138364791870117\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.014088420197367668\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.1595570594072342\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 9.67350533009892e-10\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2644730508327484\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.06712604314088821\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.13019265234470367\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.011621634475886822\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.14157961308956146\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 9.576996973237328e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.2968598008155823\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.07359763979911804\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.10157673805952072\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.009191922843456268\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.1041644886136055\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.2398977400351896e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.296516478061676\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.06774503737688065\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.1098288893699646\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.011436098255217075\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10482390969991684\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 8.700316578291734e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.30722540616989136\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07170145213603973\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.09254871308803558\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.009892952628433704\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.09391981363296509\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.312880582027276e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.2778155505657196\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.07024858891963959\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.09539280086755753\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.008985433727502823\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.10506369173526764\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.4718057883555957e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.28986606001853943\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.06970392912626266\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.09810961037874222\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.010396360419690609\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.10422518104314804\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 9.986999005562325e-10\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.30091041326522827\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07089363783597946\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.363004446029663\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3498409390449524\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.03811592981219292\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.04917297139763832\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3597850203514099\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04487720876932144\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.34979334473609924\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1237373873591423\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9283183813095093\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09398714452981949\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.17817914485931396\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2176479995250702\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.07632887363433838\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.00675260741263628\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.07673639804124832\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.923943671044981e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.29455944895744324\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.061312031000852585\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.05982280522584915\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.005744616035372019\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.06127971410751343\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 8.367688764110426e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.304232120513916\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06551160663366318\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.06268655508756638\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005231594666838646\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06140919774770737\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 8.717190858043011e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3363880217075348\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06405597925186157\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.0672479122877121\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.006519615184515715\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.06595654040575027\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 2.4350170768627777e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.29283425211906433\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06314922869205475\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.09983161091804504\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.010585379786789417\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.09401862323284149\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.109005665078655e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2707673907279968\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.055570509284734726\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.06460192054510117\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.0063761151395738125\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.07211025059223175\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.0597146493651621e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.29613298177719116\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06610376387834549\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.06963779032230377\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.006483308970928192\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.06837200373411179\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 7.38222927232357e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.2847106158733368\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06148871034383774\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.06197412312030792\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.005592344794422388\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.05840093269944191\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.169661613877111e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2744235098361969\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.059465985745191574\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.5440986156463623\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3103897273540497\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05226036533713341\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.054967038333415985\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3622419238090515\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03842195123434067\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.33543941378593445\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11243104934692383\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.7710233926773071\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.19090789556503296\n",
      "ab_loss: 23.442060470581055\n",
      "Gradient of AB Loss for row 0: tensor([0.0439, 0.0388, 0.0164, 0.0384, 0.0519, 0.0373, 0.0331, 0.0431, 0.0496,\n",
      "        0.0304, 0.0366, 0.0257, 0.0399, 0.0328])\n",
      "Gradient of AB Loss for row 1: tensor([0.0350, 0.0577, 0.0377, 0.0343, 0.0505, 0.0319, 0.0269, 0.0325, 0.0320,\n",
      "        0.0526, 0.0277, 0.0266, 0.0324])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0295, -0.0439,  0.0408,  0.0326,  0.0254,  0.0436,  0.0427,  0.0291,\n",
      "         0.0312,  0.0440,  0.0265,  0.0307, -0.0311,  0.0324])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0373,  0.0509,  0.0299,  0.0498,  0.0441,  0.0452,  0.0492,  0.0520,\n",
      "         0.0418,  0.0327,  0.0305, -0.0422,  0.0301])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0255, -0.0249, -0.0261, -0.0296, -0.0288, -0.0193,  0.0272,  0.0324,\n",
      "        -0.0207, -0.0213, -0.0191,  0.0224,  0.0276,  0.0237, -0.0212,  0.0277,\n",
      "        -0.0208, -0.0241,  0.0274,  0.0200, -0.0347,  0.0185])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0407, -0.0237, -0.0463, -0.0441, -0.0391,  0.0363, -0.0401,  0.0311,\n",
      "         0.0424,  0.0411,  0.0448, -0.0316, -0.0306, -0.0396])\n",
      "Gradient of AB Loss for row 6: tensor([-0.4945])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0288, -0.0556, -0.0383,  0.0443,  0.0168,  0.0397,  0.0413,  0.0323,\n",
      "         0.0549,  0.0265,  0.0457,  0.0165, -0.0369,  0.0379])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0375,  0.0370, -0.0418,  0.0453,  0.0185,  0.0411, -0.0337,  0.0302,\n",
      "         0.0423,  0.0261,  0.0493,  0.0266, -0.0129,  0.0243,  0.0252])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0217, -0.0311, -0.0352, -0.0419,  0.0399,  0.0285,  0.0387,  0.0144,\n",
      "        -0.0249,  0.0242,  0.0352,  0.0410,  0.0332,  0.0397, -0.0396,  0.0150])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0267, -0.0175,  0.0204,  0.0167, -0.0283, -0.0220, -0.0290,  0.0210,\n",
      "         0.0314,  0.0142,  0.0259,  0.0307,  0.0201,  0.0316, -0.0158, -0.0178,\n",
      "        -0.0165,  0.0303,  0.0259, -0.0269, -0.0279])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0363,  0.0328,  0.0430,  0.0321,  0.0350,  0.0181,  0.0310,  0.0198,\n",
      "         0.0394,  0.0438,  0.0306,  0.0317,  0.0307, -0.0288,  0.0197,  0.0196,\n",
      "         0.0373])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0413,  0.0433,  0.0232,  0.0376, -0.0374,  0.0419,  0.0446,  0.0285,\n",
      "         0.0518,  0.0447,  0.0295, -0.0328, -0.0420,  0.0447])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0292, -0.0227, -0.0206, -0.0158, -0.0387, -0.0298, -0.0296,  0.0278,\n",
      "        -0.0265, -0.0317, -0.0250, -0.0280, -0.0236, -0.0231, -0.0168, -0.0159,\n",
      "        -0.0206,  0.0144, -0.0263, -0.0326])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0270, -0.0296,  0.0396,  0.0332,  0.0150,  0.0352,  0.0420,  0.0268,\n",
      "         0.0265,  0.0212,  0.0451,  0.0323, -0.0141,  0.0324, -0.0341,  0.0237,\n",
      "        -0.0196,  0.0185])\n",
      "Gradient of AB Loss for row 15: tensor([-0.4384])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0339,  0.0407,  0.0327,  0.0280,  0.0406,  0.0404,  0.0276,  0.0451,\n",
      "         0.0419,  0.0321,  0.0258,  0.0241, -0.0259,  0.0279])\n",
      "Gradient of AB Loss for row 17: tensor([0.1104, 0.1582, 0.1548, 0.1516])\n",
      "Gradient of AB Loss for row 18: tensor([0.0377, 0.0503, 0.0319, 0.0295, 0.0326, 0.0300, 0.0346, 0.0366, 0.0425,\n",
      "        0.0377, 0.0420, 0.0341, 0.0350, 0.0229])\n",
      "Gradient of AB Loss for row 19: tensor([0.0415, 0.0320, 0.0288, 0.0269, 0.0460, 0.0405, 0.0474, 0.0340, 0.0346,\n",
      "        0.0293, 0.0423, 0.0405, 0.0289, 0.0331])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0412, -0.0339, -0.0385,  0.0132, -0.0157, -0.0400,  0.0273, -0.0313,\n",
      "         0.0360, -0.0426, -0.0417,  0.0336, -0.0328, -0.0218, -0.0161])\n",
      "Gradient of AB Loss for row 21: tensor([-0.4748])\n",
      "Gradient of AB Loss for row 22: tensor([0.0309, 0.0499, 0.0329, 0.0213, 0.0350, 0.0381, 0.0336, 0.0513, 0.0477,\n",
      "        0.0311, 0.0352, 0.0216, 0.0361, 0.0239])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0358, -0.0181, -0.0356, -0.0260,  0.0486,  0.0194, -0.0311,  0.0250,\n",
      "         0.0387,  0.0290,  0.0419,  0.0373, -0.0373, -0.0287, -0.0329])\n",
      "Gradient of AB Loss for row 24: tensor([0.0241, 0.0412, 0.0232, 0.0274, 0.0452, 0.0426, 0.0285, 0.0367, 0.0429,\n",
      "        0.0249, 0.0254, 0.0474, 0.0261, 0.0192])\n",
      "Gradient of AB Loss for row 25: tensor([0.0404, 0.0341, 0.0096, 0.0288, 0.0314, 0.0424, 0.0392, 0.0283, 0.0474,\n",
      "        0.0520, 0.0336, 0.0200, 0.0263, 0.0334])\n",
      "Gradient of AB Loss for row 26: tensor([0.0391, 0.0508, 0.0366, 0.0342, 0.0452, 0.0399, 0.0325, 0.0359, 0.0491,\n",
      "        0.0278, 0.0376, 0.0345, 0.0261, 0.0287])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0574, -0.0348,  0.0441,  0.0448, -0.0424,  0.0417,  0.0457,  0.0395,\n",
      "         0.0536,  0.0364,  0.0385,  0.0285, -0.0309,  0.0260])\n",
      "Gradient of AB Loss for row 28: tensor([0.0361, 0.0478, 0.0281, 0.0339, 0.0474, 0.0308, 0.0334, 0.0443, 0.0501,\n",
      "        0.0276, 0.0286, 0.0350, 0.0340, 0.0317])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0372, -0.0364,  0.0279,  0.0237, -0.0210,  0.0167,  0.0364,  0.0291,\n",
      "         0.0374,  0.0345,  0.0372, -0.0345,  0.0279, -0.0334,  0.0148, -0.0308])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0300, -0.0206, -0.0210, -0.0254, -0.0356, -0.0311, -0.0194,  0.0305,\n",
      "        -0.0244, -0.0203, -0.0235,  0.0242,  0.0209,  0.0253, -0.0207,  0.0336,\n",
      "        -0.0175, -0.0119, -0.0120, -0.0175,  0.0258, -0.0266, -0.0272])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0312, -0.0191, -0.0381, -0.0444, -0.0270,  0.0501,  0.0362, -0.0215,\n",
      "         0.0202,  0.0390,  0.0306,  0.0402,  0.0237, -0.0276, -0.0304, -0.0280])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.11282389611005783\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.10108719766139984\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.190560981631279\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.1179790496826172\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.12470811605453491\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.19836600124835968\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.31790488958358765\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.22112435102462769\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.019067691639065742\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.2358652651309967\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 2.2205075556058773e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3629351556301117\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.09737881273031235\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.13612960278987885\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.010405639186501503\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.1473531275987625\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.3195149417555285e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3569486141204834\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08808884024620056\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.17387671768665314\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.023013165220618248\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.2016734927892685\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.0034204578346362e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3856867551803589\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09284482151269913\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.13299642503261566\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.014237185940146446\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.13145960867404938\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.1700225233113315e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3743855357170105\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.08413185179233551\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.12969760596752167\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.01337523479014635\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.124989353120327\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.4556784666552858e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.4085374176502228\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.0875382050871849\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.1485762894153595\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.014821745455265045\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.1414777785539627\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.195829102407231e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.4000837802886963\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09644120931625366\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.12928719818592072\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.011849100701510906\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.13622787594795227\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 2.6422573018436424e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3882415294647217\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.09123723953962326\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.247613787651062\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.025130782276391983\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.2523900270462036\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.600472865348479e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.4044552147388458\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.09393879771232605\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 3.145639657974243\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.446093887090683\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.051008857786655426\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.0707525908946991\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.45714443922042847\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.05359453335404396\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.473958283662796\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.16300997138023376\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.1388853788375854\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.12427693605422974\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.20582804083824158\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2776123285293579\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.09650340676307678\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.008827729150652885\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.0995408147573471\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.2491239154144296e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.33426859974861145\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06843634694814682\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09878810495138168\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.008892695419490337\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.10338283330202103\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 9.491851749032776e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.31746894121170044\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06662212312221527\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08717533200979233\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.006672052666544914\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08484812825918198\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.2378208458230233e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.36214113235473633\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06844744086265564\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.11180032789707184\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.010298051871359348\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.10245975852012634\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 8.829149078515286e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.34467342495918274\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07380592823028564\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.08865498006343842\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.008350876159965992\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.08043669164180756\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.3563978829012058e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3171071708202362\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06654500961303711\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.1115163192152977\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.009364581666886806\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.12620218098163605\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.7985507527740197e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3282153010368347\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06990717351436615\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.11319541186094284\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.010415492579340935\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.10994970798492432\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 9.695307889856508e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.3232296109199524\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07186320424079895\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.07929036021232605\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.0065491278655827045\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07954812794923782\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.2970976515092048e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.31606435775756836\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.0698123425245285\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.7970690727233887\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.34688323736190796\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.054844930768013\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06355583667755127\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.40864408016204834\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.039987724274396896\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.4315142333507538\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.1440250277519226\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.4154324531555176\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2568966746330261\n",
      "ab_loss: 24.377832412719727\n",
      "Gradient of AB Loss for row 0: tensor([-0.0216, -0.0321, -0.0267, -0.0346,  0.0343,  0.0384,  0.0255,  0.0142,\n",
      "        -0.0301,  0.0354,  0.0465,  0.0412,  0.0445,  0.0326,  0.0344,  0.0296,\n",
      "         0.0189])\n",
      "Gradient of AB Loss for row 1: tensor([-0.0247,  0.0213, -0.0360, -0.0392,  0.0487,  0.0146,  0.0354, -0.0405,\n",
      "         0.0445,  0.0475,  0.0404,  0.0346, -0.0280,  0.0155,  0.0308])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0533, -0.0212,  0.0368,  0.0379, -0.0450,  0.0364,  0.0283,  0.0268,\n",
      "         0.0363,  0.0317,  0.0332,  0.0325, -0.0376,  0.0222])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0422, -0.0578, -0.0361,  0.0478,  0.0442, -0.0371,  0.0290,  0.0336,\n",
      "         0.0411,  0.0316,  0.0244,  0.0354, -0.0379,  0.0438])\n",
      "Gradient of AB Loss for row 4: tensor([ 0.0304, -0.0320,  0.0431,  0.0464, -0.0360,  0.0464,  0.0482,  0.0264,\n",
      "         0.0306,  0.0406,  0.0418, -0.0458,  0.0195])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0284, -0.0373,  0.0307,  0.0343,  0.0180, -0.0247,  0.0364, -0.0390,\n",
      "         0.0362,  0.0413,  0.0366,  0.0237,  0.0335,  0.0269, -0.0208])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0414, -0.0189, -0.0358, -0.0265, -0.0331,  0.0446,  0.0300,  0.0145,\n",
      "        -0.0310,  0.0278,  0.0317,  0.0325,  0.0383,  0.0297, -0.0337, -0.0279,\n",
      "        -0.0211])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0276, -0.0413,  0.0220,  0.0112,  0.0300, -0.0348,  0.0321,  0.0348,\n",
      "         0.0382,  0.0461,  0.0222, -0.0283, -0.0427,  0.0262,  0.0355])\n",
      "Gradient of AB Loss for row 8: tensor([0.0427, 0.0488, 0.0182, 0.0295, 0.0571, 0.0332, 0.0420, 0.0415, 0.0522,\n",
      "        0.0499, 0.0246, 0.0199, 0.0224, 0.0381])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0333, -0.0297, -0.0446, -0.0191,  0.0424,  0.0278, -0.0403,  0.0303,\n",
      "         0.0428,  0.0305,  0.0437, -0.0269,  0.0364,  0.0220, -0.0193,  0.0339])\n",
      "Gradient of AB Loss for row 10: tensor([ 0.0210, -0.0222, -0.0246,  0.0212, -0.0222,  0.0174,  0.0267,  0.0265,\n",
      "         0.0094,  0.0297,  0.0106,  0.0189, -0.0194, -0.0149, -0.0171,  0.0320,\n",
      "         0.0265,  0.0297, -0.0310,  0.0183,  0.0245])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0312,  0.0237,  0.0450,  0.0383,  0.0128, -0.0348,  0.0333,  0.0283,\n",
      "         0.0432,  0.0328,  0.0335,  0.0470,  0.0315,  0.0267,  0.0384,  0.0281])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0473,  0.0324, -0.0468,  0.0428,  0.0140,  0.0284,  0.0577,  0.0368,\n",
      "         0.0456,  0.0356,  0.0262,  0.0253,  0.0429])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0392, -0.0389, -0.0200, -0.0235,  0.0246,  0.0215, -0.0144, -0.0251,\n",
      "         0.0208,  0.0471,  0.0266,  0.0356, -0.0344, -0.0341, -0.0373,  0.0149,\n",
      "        -0.0297])\n",
      "Gradient of AB Loss for row 14: tensor([ 0.1955,  0.2085, -0.1203])\n",
      "Gradient of AB Loss for row 15: tensor([0.0108, 0.0477, 0.0194, 0.0366, 0.0350, 0.0328, 0.0330, 0.0362, 0.0430,\n",
      "        0.0501, 0.0410, 0.0335, 0.0216, 0.0300])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0301, -0.0193, -0.0416, -0.0469, -0.0260,  0.0357, -0.0254,  0.0315,\n",
      "         0.0346,  0.0430,  0.0438, -0.0412, -0.0378, -0.0271, -0.0304])\n",
      "Gradient of AB Loss for row 17: tensor([0.0280, 0.0508, 0.0327, 0.0245, 0.0470, 0.0334, 0.0293, 0.0410, 0.0330,\n",
      "        0.0444, 0.0429, 0.0397, 0.0312, 0.0286])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0323, -0.0469, -0.0380,  0.0471,  0.0548, -0.0383,  0.0301,  0.0262,\n",
      "         0.0471,  0.0547,  0.0284,  0.0300, -0.0405,  0.0295])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0326, -0.0368, -0.0421,  0.0150, -0.0260,  0.0272, -0.0479,  0.0391,\n",
      "        -0.0363,  0.0272,  0.0452,  0.0216, -0.0402, -0.0404, -0.0250])\n",
      "Gradient of AB Loss for row 20: tensor([0.1309, 0.1350, 0.1618, 0.1128])\n",
      "Gradient of AB Loss for row 21: tensor([ 0.1584,  0.1396, -0.1497,  0.1658])\n",
      "Gradient of AB Loss for row 22: tensor([0.0247, 0.0357, 0.0274, 0.0217, 0.0193, 0.0204, 0.0231, 0.0284, 0.0173,\n",
      "        0.0254, 0.0275, 0.0107, 0.0273, 0.0299, 0.0199, 0.0095, 0.0261, 0.0327,\n",
      "        0.0197, 0.0156, 0.0290, 0.0228, 0.0219])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0368,  0.0294, -0.0153, -0.0252, -0.0413, -0.0364, -0.0212, -0.0292,\n",
      "        -0.0255,  0.0393,  0.0247,  0.0289, -0.0388, -0.0307, -0.0245,  0.0305,\n",
      "        -0.0284, -0.0280])\n",
      "Gradient of AB Loss for row 24: tensor([0.0291, 0.0438, 0.0231, 0.0268, 0.0428, 0.0400, 0.0461, 0.0491, 0.0364,\n",
      "        0.0513, 0.0389, 0.0202, 0.0292, 0.0273])\n",
      "Gradient of AB Loss for row 25: tensor([0.0395, 0.0340, 0.0230, 0.0307, 0.0487, 0.0307, 0.0438, 0.0230, 0.0458,\n",
      "        0.0378, 0.0288, 0.0310, 0.0223, 0.0433])\n",
      "Gradient of AB Loss for row 26: tensor([ 0.0234, -0.0471,  0.0400,  0.0111,  0.0310,  0.0265,  0.0360,  0.0342,\n",
      "         0.0449,  0.0511,  0.0486,  0.0379,  0.0244,  0.0317])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0497, -0.0493,  0.0427,  0.0217, -0.0407,  0.0368,  0.0349,  0.0339,\n",
      "         0.0569,  0.0470,  0.0485,  0.0307, -0.0342,  0.0353])\n",
      "Gradient of AB Loss for row 28: tensor([0.0383, 0.0551, 0.0147, 0.0248, 0.0423, 0.0405, 0.0396, 0.0441, 0.0489,\n",
      "        0.0351, 0.0423, 0.0329, 0.0401])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0317, -0.0160, -0.0445, -0.0336, -0.0469,  0.0454,  0.0372, -0.0285,\n",
      "         0.0251,  0.0496,  0.0503,  0.0411,  0.0374,  0.0226, -0.0259,  0.0282])\n",
      "Gradient of AB Loss for row 30: tensor([0.1361, 0.1533, 0.1270, 0.1373])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0287, -0.0292,  0.0478,  0.0239,  0.0444, -0.0331,  0.0312,  0.0450,\n",
      "         0.0307,  0.0475,  0.0243, -0.0262, -0.0344,  0.0305,  0.0324])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.08362605422735214\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.07202504575252533\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.14635364711284637\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.819896399974823\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.09090103954076767\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.15200413763523102\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.23244068026542664\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1247357577085495\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.01570405811071396\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.13551509380340576\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.152031137152676e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.24904465675354004\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.06913874298334122\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.10935896635055542\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.011322315782308578\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.12289386987686157\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 9.526895938805069e-10\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.2540402114391327\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07258167862892151\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.10595674812793732\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.007811983581632376\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.10611314326524734\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 8.082688407462513e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.26732730865478516\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.0735911875963211\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.11069183796644211\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.009375609457492828\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.11137894541025162\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.4299267325768028e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.2696183919906616\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.07404250651597977\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.1062152236700058\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.011074263602495193\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10690978169441223\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.451888054226913e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.2867131531238556\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.07033264636993408\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.11757802218198776\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.010936190374195576\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.12130160629749298\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.2207292954258264e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.2550867795944214\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.06544754654169083\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.12045381218194962\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.011681945994496346\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.1266562044620514\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.0622622781397695e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.2548631727695465\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.06695706397294998\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.11465020477771759\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.009912331588566303\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.11884757876396179\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 2.2922597153751667e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.2851487398147583\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07128205895423889\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.198129892349243\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3463901877403259\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.03476157411932945\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.04949994012713432\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.31266963481903076\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.03829939663410187\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.3436780869960785\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1253507435321808\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.8278518319129944\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.08784133195877075\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.15618593990802765\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.20592676103115082\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.07487872242927551\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.005301009863615036\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.07276327908039093\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 2.3109933966480867e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.24929703772068024\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.05193737521767616\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.06616813689470291\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006395669654011726\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.06510016322135925\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.759726253602878e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.2509564161300659\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.053506895899772644\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08734395354986191\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.010858803987503052\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08785244822502136\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 9.019743285598736e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.2806006968021393\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.05677613988518715\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.0817476212978363\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.006062264088541269\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.08308599889278412\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 2.210340355190965e-09\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.2614099681377411\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.055574532598257065\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.0673251524567604\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.00564167695119977\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.06457576155662537\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 9.703795544879767e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.24705392122268677\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.05108096823096275\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.0822862908244133\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.009917241521179676\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.08685770630836487\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 9.220618157890215e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.2589977979660034\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.05910814553499222\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.06733794510364532\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.006185055710375309\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.06622666865587234\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 8.425712905157923e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.24681949615478516\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.054901957511901855\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.05825893208384514\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.004310755990445614\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.056416112929582596\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 9.495048081120672e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.22873364388942719\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.050727635622024536\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.199772596359253\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.2815582752227783\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.043969444930553436\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.04821822792291641\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.30097755789756775\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.031124021857976913\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3051762282848358\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.10051389038562775\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.5780500173568726\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.1792239099740982\n",
      "ab_loss: 25.028413772583008\n",
      "Gradient of AB Loss for row 0: tensor([-0.0492, -0.0346, -0.0364, -0.0481, -0.0292,  0.0459, -0.0298,  0.0392,\n",
      "         0.0382,  0.0387,  0.0401, -0.0357, -0.0304, -0.0276])\n",
      "Gradient of AB Loss for row 1: tensor([ 0.0411, -0.0465,  0.0569,  0.0336,  0.0522, -0.0381,  0.0213,  0.0316,\n",
      "         0.0504,  0.0237,  0.0538,  0.0266, -0.0383,  0.0393])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0108, -0.0291, -0.0256, -0.0189, -0.0171, -0.0393, -0.0338, -0.0294,\n",
      "         0.0270, -0.0171, -0.0238, -0.0214, -0.0250,  0.0302, -0.0274, -0.0261,\n",
      "        -0.0232, -0.0151,  0.0262, -0.0318, -0.0247])\n",
      "Gradient of AB Loss for row 3: tensor([0.0391, 0.0394, 0.0179, 0.0220, 0.0496, 0.0395, 0.0543, 0.0431, 0.0559,\n",
      "        0.0390, 0.0436, 0.0313, 0.0260, 0.0324])\n",
      "Gradient of AB Loss for row 4: tensor([-0.6044])\n",
      "Gradient of AB Loss for row 5: tensor([0.0389, 0.0494, 0.0308, 0.0300, 0.0418, 0.0407, 0.0389, 0.0523, 0.0270,\n",
      "        0.0622, 0.0522, 0.0342, 0.0342])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0261,  0.0287, -0.0347,  0.0381,  0.0209,  0.0491, -0.0342,  0.0281,\n",
      "         0.0442,  0.0355,  0.0526,  0.0233, -0.0346,  0.0270,  0.0297])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0392, -0.0314, -0.0506, -0.0229, -0.0446, -0.0368,  0.0424, -0.0201,\n",
      "        -0.0396,  0.0374, -0.0310,  0.0390, -0.0354, -0.0338, -0.0158])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0523,  0.0529,  0.0509, -0.0361,  0.0261,  0.0282,  0.0196,  0.0291,\n",
      "         0.0573,  0.0435,  0.0342,  0.0337, -0.0389,  0.0312])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0364, -0.0377, -0.0491, -0.0481,  0.0215,  0.0256,  0.0548,  0.0308,\n",
      "         0.0479, -0.0439,  0.0302, -0.0284,  0.0303, -0.0420, -0.0232])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0471, -0.0419, -0.0298, -0.0132, -0.0378,  0.0452,  0.0257,  0.0279,\n",
      "        -0.0395, -0.0329,  0.0248,  0.0418, -0.0237, -0.0253])\n",
      "Gradient of AB Loss for row 11: tensor([ 0.0318, -0.0336,  0.0518,  0.0149,  0.0291,  0.0484,  0.0412,  0.0404,\n",
      "         0.0551,  0.0476,  0.0418,  0.0333,  0.0287,  0.0389])\n",
      "Gradient of AB Loss for row 12: tensor([ 0.1453,  0.1014, -0.0934,  0.1467])\n",
      "Gradient of AB Loss for row 13: tensor([ 0.0278, -0.0452,  0.0194,  0.0201,  0.0342,  0.0260,  0.0414,  0.0268,\n",
      "         0.0365,  0.0424,  0.0244,  0.0278,  0.0248,  0.0396])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0304,  0.0315, -0.0380,  0.0497,  0.0236,  0.0277, -0.0458,  0.0486,\n",
      "         0.0291,  0.0587,  0.0507,  0.0355,  0.0334,  0.0237])\n",
      "Gradient of AB Loss for row 15: tensor([-0.0498,  0.0392,  0.0394,  0.0107,  0.0387,  0.0366,  0.0425,  0.0374,\n",
      "         0.0478,  0.0332,  0.0462,  0.0276,  0.0339,  0.0328])\n",
      "Gradient of AB Loss for row 16: tensor([0.2130, 0.1888, 0.1604])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0238, -0.0306, -0.0179, -0.0199, -0.0168, -0.0342, -0.0258, -0.0167,\n",
      "         0.0238, -0.0292, -0.0281, -0.0235,  0.0183, -0.0184, -0.0318, -0.0306,\n",
      "        -0.0182, -0.0225,  0.0213, -0.0311, -0.0270])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0257, -0.0446, -0.0356,  0.0483,  0.0198,  0.0402,  0.0419,  0.0477,\n",
      "         0.0533,  0.0417,  0.0444,  0.0284, -0.0289,  0.0442])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0279, -0.0254, -0.0349, -0.0450, -0.0357, -0.0380,  0.0285, -0.0392,\n",
      "         0.0272,  0.0234,  0.0313,  0.0424, -0.0365,  0.0250, -0.0293, -0.0230])\n",
      "Gradient of AB Loss for row 20: tensor([-0.0286, -0.0418, -0.0320, -0.0435,  0.0350,  0.0169, -0.0264,  0.0286,\n",
      "         0.0445,  0.0365, -0.0373,  0.0415, -0.0158, -0.0444, -0.0264])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0234, -0.0292, -0.0302, -0.0319,  0.0295,  0.0328,  0.0117, -0.0194,\n",
      "         0.0269,  0.0415,  0.0295,  0.0353,  0.0388, -0.0311,  0.0285, -0.0416,\n",
      "         0.0358])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0465, -0.0334,  0.0487,  0.0335,  0.0300,  0.0496,  0.0333,  0.0461,\n",
      "         0.0436,  0.0323,  0.0359,  0.0195, -0.0352,  0.0249])\n",
      "Gradient of AB Loss for row 23: tensor([0.0404, 0.0461, 0.0386, 0.0191, 0.0355, 0.0293, 0.0244, 0.0369, 0.0379,\n",
      "        0.0365, 0.0509, 0.0325, 0.0366, 0.0343])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0448, -0.0409,  0.0116,  0.0466, -0.0321,  0.0410,  0.0302,  0.0302,\n",
      "         0.0476,  0.0384,  0.0276, -0.0315, -0.0332,  0.0280,  0.0275])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0329,  0.0353,  0.0384,  0.0185,  0.0226,  0.0496,  0.0354,  0.0387,\n",
      "         0.0510,  0.0338,  0.0421,  0.0388,  0.0361,  0.0424])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0199,  0.0307, -0.0558,  0.0522,  0.0207,  0.0400,  0.0308,  0.0323,\n",
      "         0.0363,  0.0467,  0.0468,  0.0423,  0.0214,  0.0359])\n",
      "Gradient of AB Loss for row 27: tensor([0.0412, 0.0503, 0.0446, 0.0195, 0.0375, 0.0422, 0.0418, 0.0453, 0.0516,\n",
      "        0.0394, 0.0448, 0.0286, 0.0328, 0.0285])\n",
      "Gradient of AB Loss for row 28: tensor([0.0165, 0.0403, 0.0350, 0.0216, 0.0406, 0.0385, 0.0448, 0.0391, 0.0499,\n",
      "        0.0428, 0.0346, 0.0310, 0.0344, 0.0290, 0.0300])\n",
      "Gradient of AB Loss for row 29: tensor([-0.6833])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0172, -0.0189, -0.0226, -0.0157, -0.0160, -0.0292, -0.0296, -0.0283,\n",
      "        -0.0206,  0.0221, -0.0215, -0.0304, -0.0243, -0.0303, -0.0237, -0.0229,\n",
      "        -0.0114, -0.0094, -0.0135, -0.0165,  0.0223, -0.0265, -0.0239])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0187, -0.0162, -0.0234,  0.0139, -0.0100, -0.0204, -0.0260, -0.0298,\n",
      "        -0.0203, -0.0201, -0.0245, -0.0248, -0.0233, -0.0143, -0.0230, -0.0165,\n",
      "        -0.0139, -0.0149, -0.0123,  0.0176, -0.0208, -0.0226, -0.0293])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09782209247350693\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.08849048614501953\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.19147326052188873\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0108500719070435\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11404071748256683\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.1798848807811737\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2860291600227356\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.11602187901735306\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.013847186230123043\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.13422267138957977\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.0860370380783024e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.3102114796638489\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.08526281267404556\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.127354234457016\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.015124162659049034\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.1533074826002121\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.2024837792168341e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.3234217166900635\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.0944591537117958\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.15216882526874542\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.01874641887843609\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.17291174829006195\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.0249830983966035e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3555064797401428\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09841924160718918\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.20513510704040527\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.01750286854803562\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.2039777636528015\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.0563647734329606e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.37997186183929443\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.0934770330786705\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.10708264261484146\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.013284483924508095\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10143676400184631\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 9.509827370024482e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.4157661497592926\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.09360018372535706\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.09323018044233322\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.010542073287069798\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.09394047409296036\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 2.002041199489213e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.35706010460853577\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.09379950165748596\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.1250229775905609\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01106681115925312\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.13845907151699066\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.4426388972310633e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.40806254744529724\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.10651648044586182\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.13500507175922394\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.011483014561235905\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.13706912100315094\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.8562025250190572e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3471418619155884\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.08983432501554489\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.950554847717285\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4647544026374817\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.05713214725255966\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.07016593962907791\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3659615218639374\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04592141881585121\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.4173266887664795\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.15028263628482819\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0280966758728027\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.10712813585996628\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.18817387521266937\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.23872876167297363\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.09227559715509415\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.00794275850057602\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.09427943080663681\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 9.405417555896634e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.3247711658477783\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.06711222976446152\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.0769568383693695\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.007751265540719032\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.0819028690457344\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.0733108846139316e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3268170952796936\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.0720110759139061\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08392198383808136\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.008368777111172676\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08131424337625504\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 7.114369093841333e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3867621421813965\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.07473625242710114\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.08706506341695786\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.008390525355935097\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.08402792364358902\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 8.623407543595363e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3335639536380768\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.07386117428541183\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.06480792164802551\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.005067062098532915\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.06421858817338943\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.1048064685326153e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.313249409198761\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.0639345645904541\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.07710079103708267\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.00682586757466197\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.08567310124635696\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 2.4824158284530995e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3126881420612335\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07431147247552872\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.0800275057554245\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007125067058950663\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07841305434703827\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.9007715401642145e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.32578104734420776\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07586777955293655\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.12366081774234772\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.013312121853232384\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.11202728003263474\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 8.043554711179013e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.30146893858909607\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06352759897708893\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.901012897491455\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.36190369725227356\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04938945919275284\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06355945765972137\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.38014718890190125\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03873175010085106\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.36554446816444397\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.12049075961112976\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.964530348777771\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.21082298457622528\n",
      "ab_loss: 23.57988166809082\n",
      "Gradient of AB Loss for row 0: tensor([-0.4927])\n",
      "Gradient of AB Loss for row 1: tensor([-0.3988])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0446, -0.0461, -0.0423, -0.0349, -0.0409, -0.0444,  0.0314, -0.0284,\n",
      "         0.0330,  0.0503,  0.0477,  0.0363, -0.0298, -0.0346, -0.0252])\n",
      "Gradient of AB Loss for row 3: tensor([0.1599, 0.1238, 0.0766, 0.1077])\n",
      "Gradient of AB Loss for row 4: tensor([-0.5829])\n",
      "Gradient of AB Loss for row 5: tensor([ 0.1687, -0.1943,  0.1959])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0442, -0.0385, -0.0389,  0.0419, -0.0479,  0.0391,  0.0406,  0.0506,\n",
      "         0.0342,  0.0231,  0.0219, -0.0493,  0.0218])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0340, -0.0234,  0.0206, -0.0238, -0.0239, -0.0272, -0.0260, -0.0288,\n",
      "         0.0261, -0.0241, -0.0214,  0.0224, -0.0228, -0.0164,  0.0265, -0.0165,\n",
      "        -0.0175, -0.0161,  0.0330, -0.0270, -0.0251])\n",
      "Gradient of AB Loss for row 8: tensor([ 0.0508,  0.0307,  0.0272,  0.0413,  0.0382,  0.0256,  0.0366,  0.0461,\n",
      "         0.0387,  0.0271,  0.0358, -0.0334,  0.0261])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0378,  0.0431,  0.0177,  0.0364,  0.0456,  0.0283,  0.0398,  0.0246,\n",
      "         0.0351,  0.0407,  0.0389,  0.0274, -0.0361,  0.0403])\n",
      "Gradient of AB Loss for row 10: tensor([0.0358, 0.0533, 0.0129, 0.0325, 0.0505, 0.0265, 0.0265, 0.0357, 0.0451,\n",
      "        0.0301, 0.0307, 0.0422, 0.0340, 0.0297])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0289, -0.0623, -0.0511,  0.0437, -0.0528,  0.0437,  0.0415,  0.0439,\n",
      "         0.0407,  0.0248,  0.0329, -0.0354,  0.0306])\n",
      "Gradient of AB Loss for row 12: tensor([0.0341, 0.0236, 0.0367, 0.0285, 0.0514, 0.0378, 0.0453, 0.0242, 0.0487,\n",
      "        0.0477, 0.0318, 0.0358, 0.0423, 0.0227])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0370, -0.0330, -0.0439, -0.0211, -0.0328,  0.0354,  0.0333, -0.0253,\n",
      "         0.0311,  0.0388,  0.0392,  0.0386,  0.0299, -0.0357, -0.0341, -0.0308,\n",
      "         0.0115])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0322, -0.0320, -0.0237, -0.0339, -0.0360,  0.0332,  0.0205, -0.0206,\n",
      "         0.0298,  0.0342, -0.0293,  0.0243, -0.0323, -0.0270, -0.0365, -0.0202,\n",
      "        -0.0230])\n",
      "Gradient of AB Loss for row 15: tensor([0.0509, 0.0426, 0.0260, 0.0187, 0.0370, 0.0469, 0.0277, 0.0471, 0.0405,\n",
      "        0.0354, 0.0231, 0.0334, 0.0321, 0.0236])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0480, -0.0464,  0.0539,  0.0211,  0.0377, -0.0456,  0.0274,  0.0373,\n",
      "         0.0512,  0.0362,  0.0243, -0.0332, -0.0384,  0.0322])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0302, -0.0158, -0.0383, -0.0242,  0.0215,  0.0201, -0.0433,  0.0422,\n",
      "         0.0283,  0.0363, -0.0363,  0.0216, -0.0197,  0.0244,  0.0343])\n",
      "Gradient of AB Loss for row 18: tensor([ 0.0331, -0.0271,  0.0390,  0.0160,  0.0483, -0.0361,  0.0380,  0.0261,\n",
      "         0.0373,  0.0480,  0.0370,  0.0239,  0.0296,  0.0186])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0360,  0.0391,  0.0205,  0.0491, -0.0525,  0.0330,  0.0357,  0.0378,\n",
      "         0.0348,  0.0300, -0.0409,  0.0329, -0.0440,  0.0297])\n",
      "Gradient of AB Loss for row 20: tensor([-0.1021, -0.1720, -0.1639,  0.1139])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0275, -0.0490, -0.0395, -0.0138,  0.0446, -0.0390,  0.0376,  0.0397,\n",
      "         0.0415,  0.0311,  0.0343, -0.0320, -0.0266,  0.0403])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0249, -0.0343,  0.0328,  0.0160,  0.0528,  0.0445,  0.0340,  0.0518,\n",
      "         0.0523,  0.0444,  0.0295, -0.0358,  0.0420])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0609,  0.0558,  0.0312, -0.0408,  0.0335,  0.0280,  0.0234,  0.0436,\n",
      "         0.0518,  0.0386, -0.0401,  0.0372, -0.0385,  0.0248])\n",
      "Gradient of AB Loss for row 24: tensor([-0.4303])\n",
      "Gradient of AB Loss for row 25: tensor([-0.0327, -0.0350, -0.0295, -0.0490,  0.0405,  0.0264, -0.0272, -0.0356,\n",
      "         0.0416, -0.0263,  0.0370, -0.0394,  0.0282, -0.0407,  0.0309, -0.0207])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0202,  0.0237, -0.0134, -0.0279, -0.0312, -0.0351, -0.0217, -0.0210,\n",
      "        -0.0236, -0.0264, -0.0270, -0.0156, -0.0170, -0.0281, -0.0111, -0.0173,\n",
      "        -0.0140,  0.0259, -0.0271, -0.0217, -0.0290])\n",
      "Gradient of AB Loss for row 27: tensor([ 0.0372, -0.0352,  0.0456,  0.0265,  0.0268,  0.0460,  0.0382,  0.0414,\n",
      "         0.0338,  0.0580,  0.0313,  0.0318,  0.0276,  0.0318])\n",
      "Gradient of AB Loss for row 28: tensor([-0.5540])\n",
      "Gradient of AB Loss for row 29: tensor([0.1744, 0.1298, 0.1383, 0.1646])\n",
      "Gradient of AB Loss for row 30: tensor([0.0355, 0.0535, 0.0160, 0.0329, 0.0449, 0.0239, 0.0306, 0.0290, 0.0571,\n",
      "        0.0360, 0.0378, 0.0282, 0.0291, 0.0301])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0251, -0.0278, -0.0260,  0.0362,  0.0249,  0.0233, -0.0213,  0.0272,\n",
      "         0.0386,  0.0424,  0.0353,  0.0270,  0.0242, -0.0296,  0.0209, -0.0299,\n",
      "         0.0134])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.09078318625688553\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.07758808881044388\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.17070013284683228\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9598201513290405\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11468098312616348\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.16150148212909698\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.2795620262622833\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.15741084516048431\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.014346577227115631\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.16488876938819885\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.6382014633720132e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.28925806283950806\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.08414988219738007\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.12122654169797897\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.010847000405192375\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.14078795909881592\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.4157676142545483e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.29853472113609314\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.08453962951898575\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.1168769970536232\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.010025359690189362\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.12666818499565125\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.2755143607989794e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3444076180458069\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.09535053372383118\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.16496458649635315\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.01621943712234497\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.157884880900383\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.2257498349654838e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.3464810252189636\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.0972418412566185\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.12132513523101807\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.015444561839103699\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.11328163743019104\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.3466070480916414e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3858354985713959\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.0958433598279953\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.10829591751098633\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.009181711822748184\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.11100941896438599\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.5894554561413088e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.31818249821662903\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.08384206146001816\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.11830661445856094\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.013250213116407394\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.1260896921157837\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.2501910617856993e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.35054755210876465\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.09235287457704544\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.11681390553712845\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.012153396382927895\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.11972565948963165\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.0269022299169706e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.35793063044548035\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.09736194461584091\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.8475871086120605\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4689456820487976\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.043178215622901917\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06789298355579376\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.34609514474868774\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.045595984905958176\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.37965840101242065\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.1413814276456833\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9137139320373535\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09766875952482224\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.1718071550130844\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.2226496785879135\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.06292431801557541\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.00526259234175086\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.06714917719364166\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.1868646065948951e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.2775120437145233\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.056873999536037445\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.0749892070889473\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006737140007317066\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.07709851861000061\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.039213048947829e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.286034494638443\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.06374216824769974\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.06769100576639175\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005730136297643185\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06531933695077896\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.3454780622979001e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.3309534788131714\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06835012137889862\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.07444112747907639\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.006741783116012812\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.07584559172391891\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 8.651459548758567e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.26924630999565125\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.058573272079229355\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.06156323850154877\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.005637140944600105\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.0601816326379776\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 8.943795148930178e-10\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.2666367292404175\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.056026916950941086\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.08185907453298569\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.009444838389754295\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.09171594679355621\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.2083933853546114e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.283216655254364\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06452217698097229\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.07250525057315826\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.005324205383658409\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.0725470706820488\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.7519411477096014e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.28705286979675293\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.0626959353685379\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.0784304216504097\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.007083957549184561\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.07837594300508499\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 9.716459858921667e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.29456642270088196\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06313608586788177\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.496657133102417\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.31916236877441406\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.0442863292992115\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.0565989650785923\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3385784327983856\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.03491120785474777\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3423348665237427\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11106104403734207\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.9350558519363403\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.21512894332408905\n",
      "ab_loss: 24.691635131835938\n",
      "Gradient of AB Loss for row 0: tensor([-0.0311, -0.0241, -0.0216, -0.0383, -0.0209,  0.0284, -0.0339, -0.0232,\n",
      "        -0.0240,  0.0325,  0.0244,  0.0190, -0.0257,  0.0304, -0.0182,  0.0242,\n",
      "         0.0221, -0.0338, -0.0309])\n",
      "Gradient of AB Loss for row 1: tensor([-0.4466])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0344, -0.0218, -0.0245, -0.0393, -0.0395,  0.0367,  0.0298, -0.0282,\n",
      "         0.0311,  0.0211,  0.0471, -0.0151,  0.0312, -0.0334, -0.0375, -0.0193])\n",
      "Gradient of AB Loss for row 3: tensor([-0.5983])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0458,  0.0574,  0.0275,  0.0326,  0.0334,  0.0359,  0.0416,  0.0343,\n",
      "         0.0507,  0.0336,  0.0343,  0.0175, -0.0328,  0.0387])\n",
      "Gradient of AB Loss for row 5: tensor([-0.0259, -0.0512, -0.0323, -0.0486, -0.0428,  0.0363, -0.0281,  0.0385,\n",
      "         0.0310,  0.0513,  0.0276, -0.0389, -0.0346, -0.0214, -0.0255,  0.0306])\n",
      "Gradient of AB Loss for row 6: tensor([0.0441, 0.0407, 0.0306, 0.0365, 0.0350, 0.0283, 0.0283, 0.0352, 0.0575,\n",
      "        0.0296, 0.0278, 0.0424, 0.0369, 0.0168])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0445,  0.0480,  0.0233,  0.0183,  0.0380,  0.0271,  0.0255,  0.0362,\n",
      "         0.0476,  0.0479,  0.0420,  0.0277, -0.0396,  0.0380])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0310, -0.0273, -0.0264, -0.0183, -0.0306, -0.0198,  0.0377, -0.0228,\n",
      "        -0.0239, -0.0294,  0.0166,  0.0229, -0.0260, -0.0202, -0.0133,  0.0372,\n",
      "        -0.0279, -0.0326, -0.0249])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0412, -0.0512,  0.0404,  0.0446,  0.0371,  0.0423,  0.0344,  0.0362,\n",
      "         0.0560,  0.0484,  0.0442,  0.0254,  0.0318,  0.0345])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0221, -0.0230, -0.0333, -0.0418, -0.0375,  0.0303, -0.0297,  0.0318,\n",
      "         0.0343,  0.0427,  0.0367, -0.0352, -0.0357, -0.0279, -0.0244])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0295, -0.0488, -0.0602, -0.0348, -0.0249,  0.0367, -0.0428,  0.0401,\n",
      "         0.0527,  0.0353,  0.0324,  0.0339, -0.0340, -0.0197])\n",
      "Gradient of AB Loss for row 12: tensor([-0.0390,  0.0221,  0.0534,  0.0184,  0.0353,  0.0324,  0.0396,  0.0436,\n",
      "         0.0451,  0.0434,  0.0401,  0.0301,  0.0445,  0.0292])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0433,  0.0417,  0.0157,  0.0194,  0.0272,  0.0337,  0.0360,  0.0318,\n",
      "         0.0448,  0.0359,  0.0513,  0.0345, -0.0350,  0.0428])\n",
      "Gradient of AB Loss for row 14: tensor([0.0393, 0.0406, 0.0199, 0.0335, 0.0438, 0.0367, 0.0418, 0.0380, 0.0556,\n",
      "        0.0321, 0.0459, 0.0458, 0.0206, 0.0357])\n",
      "Gradient of AB Loss for row 15: tensor([0.0275, 0.0486, 0.0182, 0.0378, 0.0202, 0.0364, 0.0410, 0.0457, 0.0300,\n",
      "        0.0401, 0.0373, 0.0230, 0.0341, 0.0375])\n",
      "Gradient of AB Loss for row 16: tensor([ 0.0375, -0.0553,  0.0498,  0.0207,  0.0173,  0.0323,  0.0277,  0.0429,\n",
      "         0.0338,  0.0457,  0.0516,  0.0384,  0.0263,  0.0352])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0305, -0.0615, -0.0482,  0.0324, -0.0438,  0.0442,  0.0367,  0.0471,\n",
      "         0.0567,  0.0361,  0.0282, -0.0443,  0.0446])\n",
      "Gradient of AB Loss for row 18: tensor([0.0373, 0.0471, 0.0254, 0.0334, 0.0490, 0.0333, 0.0380, 0.0410, 0.0472,\n",
      "        0.0411, 0.0326, 0.0317, 0.0279, 0.0309])\n",
      "Gradient of AB Loss for row 19: tensor([0.0440, 0.0415, 0.0202, 0.0343, 0.0296, 0.0376, 0.0378, 0.0393, 0.0335,\n",
      "        0.0561, 0.0273, 0.0204, 0.0211, 0.0376])\n",
      "Gradient of AB Loss for row 20: tensor([0.0373, 0.0470, 0.0220, 0.0332, 0.0503, 0.0378, 0.0344, 0.0487, 0.0564,\n",
      "        0.0295, 0.0277, 0.0429, 0.0252, 0.0209])\n",
      "Gradient of AB Loss for row 21: tensor([0.0350, 0.0487, 0.0177, 0.0282, 0.0349, 0.0391, 0.0335, 0.0388, 0.0608,\n",
      "        0.0390, 0.0461, 0.0315, 0.0391, 0.0293])\n",
      "Gradient of AB Loss for row 22: tensor([0.0350, 0.0468, 0.0218, 0.0215, 0.0460, 0.0302, 0.0314, 0.0403, 0.0450,\n",
      "        0.0371, 0.0373, 0.0412, 0.0224, 0.0326])\n",
      "Gradient of AB Loss for row 23: tensor([-0.5448])\n",
      "Gradient of AB Loss for row 24: tensor([-0.0235, -0.0365, -0.0358, -0.0454,  0.0374,  0.0129, -0.0345, -0.0269,\n",
      "         0.0213,  0.0398,  0.0314,  0.0353, -0.0315, -0.0319, -0.0320, -0.0266,\n",
      "        -0.0239])\n",
      "Gradient of AB Loss for row 25: tensor([0.0279, 0.0382, 0.0293, 0.0311, 0.0388, 0.0334, 0.0320, 0.0443, 0.0496,\n",
      "        0.0427, 0.0528, 0.0357, 0.0364, 0.0253])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0374, -0.0417,  0.0541,  0.0374,  0.0275,  0.0521,  0.0576,  0.0545,\n",
      "         0.0452,  0.0425,  0.0231, -0.0435,  0.0513])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0322, -0.0357, -0.0262,  0.0448,  0.0353,  0.0285,  0.0110, -0.0353,\n",
      "         0.0324,  0.0407,  0.0301,  0.0389,  0.0194,  0.0299,  0.0249,  0.0308,\n",
      "         0.0290])\n",
      "Gradient of AB Loss for row 28: tensor([-0.0310, -0.0385,  0.0269,  0.0299,  0.0301,  0.0160, -0.0304,  0.0231,\n",
      "         0.0376,  0.0298,  0.0316,  0.0200,  0.0318,  0.0357,  0.0270,  0.0229,\n",
      "        -0.0330,  0.0277])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0265,  0.0575,  0.0244, -0.0635, -0.0429,  0.0488,  0.0521,  0.0627,\n",
      "         0.0436, -0.0443, -0.0306,  0.0524])\n",
      "Gradient of AB Loss for row 30: tensor([ 0.0390, -0.0472,  0.0475,  0.0107,  0.0219,  0.0382,  0.0345,  0.0338,\n",
      "         0.0350,  0.0543,  0.0534,  0.0441,  0.0312,  0.0310])\n",
      "Gradient of AB Loss for row 31: tensor([0.0315, 0.0567, 0.0167, 0.0250, 0.0401, 0.0320, 0.0331, 0.0501, 0.0495,\n",
      "        0.0334, 0.0546, 0.0439, 0.0335, 0.0433])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.0920361801981926\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.0934845507144928\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.13845641911029816\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 0.9235343337059021\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.09760327637195587\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.17219969630241394\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.23930443823337555\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.15008074045181274\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.014252373948693275\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.16707658767700195\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.621915157734577e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.301602840423584\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.06915945559740067\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.1269168257713318\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.011867580004036427\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.14033502340316772\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.0269002315155262e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.28687921166419983\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.06813577562570572\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.13993598520755768\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.014360829256474972\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.15877117216587067\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 9.579195214826086e-10\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.3144499361515045\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.0755673497915268\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.11838642507791519\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.009025095961987972\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.11102718114852905\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.2069286681182234e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.30768150091171265\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.058171916753053665\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11062784492969513\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.011633092537522316\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10973861068487167\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 9.373801734824383e-10\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3006156086921692\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.06391330808401108\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.11475104838609695\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.01424462627619505\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.11717730760574341\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.1516759768070983e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3149986267089844\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.07624396681785583\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.1719023734331131\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.013942536897957325\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.18705503642559052\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 9.926011124150591e-10\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.3409346044063568\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.0753060057759285\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.09596037119626999\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.013341270387172699\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.10217800736427307\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.2055771936303472e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.3320556879043579\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.07333957403898239\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.587507963180542\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.3461291491985321\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.04627569019794464\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.0557120144367218\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.3735763728618622\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.04330114275217056\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.37467867136001587\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.12320945411920547\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 0.9605212807655334\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.09914558380842209\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.172317773103714\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.222443625330925\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.0986470952630043\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.010221723467111588\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.10156957060098648\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 1.5103150952100464e-09\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.3020722568035126\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.060248229652643204\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.07185380905866623\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.006261071190237999\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.07115218043327332\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 7.69197927574794e-10\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.3296848237514496\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07234351336956024\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.0682150274515152\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.005919832736253738\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.06500320136547089\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 7.696134840529112e-10\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.34924620389938354\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.06353792548179626\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.08798051625490189\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.008713538758456707\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.08835896104574203\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 8.615296809288964e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.3220416009426117\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06887460500001907\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.10147114843130112\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.010586303658783436\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.09379816055297852\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.1414342804272337e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.3071691393852234\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06191326305270195\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.06915638595819473\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.006750419270247221\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.07689505070447922\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 1.6765553390030163e-09\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3082523047924042\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.07231112569570541\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.07618238031864166\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.006565649062395096\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.07997512817382812\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 1.8959520620143167e-09\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.312522292137146\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.07149134576320648\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.12519311904907227\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.012877785600721836\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.1091599091887474\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 9.835000591706944e-10\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.30441272258758545\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.06296414881944656\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.704953670501709\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3334495425224304\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.04670282453298569\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.05979492887854576\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.3670702874660492\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.0372304804623127\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.3587307631969452\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.11654030531644821\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 1.889273762702942\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.20220045745372772\n",
      "ab_loss: 24.769142150878906\n",
      "Gradient of AB Loss for row 0: tensor([-0.0345,  0.0462, -0.0353,  0.0487,  0.0086, -0.0349,  0.0260, -0.0177,\n",
      "         0.0366,  0.0500, -0.0363, -0.0364, -0.0237,  0.0300,  0.0379])\n",
      "Gradient of AB Loss for row 1: tensor([0.0355, 0.0416, 0.0319, 0.0298, 0.0461, 0.0437, 0.0490, 0.0465, 0.0472,\n",
      "        0.0246, 0.0387, 0.0377, 0.0391, 0.0240])\n",
      "Gradient of AB Loss for row 2: tensor([-0.0261,  0.0200, -0.0207, -0.0126,  0.0301, -0.0270, -0.0268, -0.0242,\n",
      "         0.0307,  0.0145,  0.0317,  0.0194, -0.0182,  0.0252, -0.0199, -0.0170,\n",
      "        -0.0177,  0.0286,  0.0298, -0.0297, -0.0280])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0247, -0.0325, -0.0251,  0.0364,  0.0085, -0.0186,  0.0325,  0.0428,\n",
      "         0.0210,  0.0394,  0.0349, -0.0360,  0.0379,  0.0193, -0.0235,  0.0239,\n",
      "        -0.0239])\n",
      "Gradient of AB Loss for row 4: tensor([-0.0305, -0.0143, -0.0201, -0.0378, -0.0307, -0.0253,  0.0225, -0.0237,\n",
      "        -0.0264, -0.0301,  0.0219,  0.0317,  0.0306, -0.0342,  0.0352, -0.0158,\n",
      "         0.0276, -0.0310, -0.0210])\n",
      "Gradient of AB Loss for row 5: tensor([0.0426, 0.0551, 0.0426, 0.0365, 0.0492, 0.0256, 0.0386, 0.0336, 0.0455,\n",
      "        0.0383, 0.0445, 0.0246, 0.0262, 0.0412])\n",
      "Gradient of AB Loss for row 6: tensor([-0.5459])\n",
      "Gradient of AB Loss for row 7: tensor([0.0273, 0.0478, 0.0291, 0.0300, 0.0379, 0.0317, 0.0310, 0.0405, 0.0522,\n",
      "        0.0443, 0.0317, 0.0300, 0.0233, 0.0325])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0458, -0.0367,  0.0510,  0.0545, -0.0439,  0.0441,  0.0393,  0.0460,\n",
      "         0.0506,  0.0448,  0.0407,  0.0215, -0.0269,  0.0464])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0446,  0.0495,  0.0212,  0.0333, -0.0403,  0.0406,  0.0339,  0.0407,\n",
      "         0.0595,  0.0422,  0.0390,  0.0327, -0.0329,  0.0320])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0215, -0.0188, -0.0400, -0.0350, -0.0337, -0.0407,  0.0278, -0.0322,\n",
      "         0.0347,  0.0293,  0.0437,  0.0439, -0.0314,  0.0206, -0.0308])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0281, -0.0567, -0.0411,  0.0350,  0.0254,  0.0416,  0.0527,  0.0444,\n",
      "         0.0457,  0.0435,  0.0334,  0.0315, -0.0364,  0.0224])\n",
      "Gradient of AB Loss for row 12: tensor([ 0.0345, -0.0420,  0.0435,  0.0192,  0.0365, -0.0337,  0.0430,  0.0335,\n",
      "         0.0395,  0.0348,  0.0600,  0.0397,  0.0209,  0.0290])\n",
      "Gradient of AB Loss for row 13: tensor([ 0.0431, -0.0316,  0.0451,  0.0290,  0.0339, -0.0390,  0.0322,  0.0394,\n",
      "         0.0429,  0.0319,  0.0515, -0.0307, -0.0293,  0.0279,  0.0271])\n",
      "Gradient of AB Loss for row 14: tensor([-0.0374, -0.0230, -0.0365, -0.0531, -0.0354,  0.0301, -0.0270,  0.0355,\n",
      "         0.0357,  0.0428,  0.0296, -0.0490, -0.0389, -0.0237, -0.0218])\n",
      "Gradient of AB Loss for row 15: tensor([0.0423, 0.0510, 0.0137, 0.0277, 0.0269, 0.0331, 0.0383, 0.0266, 0.0267,\n",
      "        0.0420, 0.0445, 0.0210, 0.0431])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0279, -0.0135, -0.0215,  0.0302, -0.0224,  0.0206,  0.0284, -0.0223,\n",
      "        -0.0264, -0.0221,  0.0185,  0.0180,  0.0195,  0.0226, -0.0302,  0.0078,\n",
      "         0.0243, -0.0181,  0.0271,  0.0250, -0.0303, -0.0271])\n",
      "Gradient of AB Loss for row 17: tensor([ 0.0284, -0.0332,  0.0488,  0.0194,  0.0300,  0.0318,  0.0445,  0.0375,\n",
      "         0.0433,  0.0540,  0.0443,  0.0440,  0.0252,  0.0231])\n",
      "Gradient of AB Loss for row 18: tensor([-0.0456, -0.0372, -0.0254,  0.0460,  0.0252,  0.0537, -0.0375,  0.0284,\n",
      "         0.0425,  0.0443,  0.0515,  0.0232, -0.0255,  0.0246,  0.0221])\n",
      "Gradient of AB Loss for row 19: tensor([-0.0323,  0.0424,  0.0114,  0.0366, -0.0406,  0.0342,  0.0401,  0.0318,\n",
      "         0.0486, -0.0418,  0.0306, -0.0285, -0.0344,  0.0182,  0.0270])\n",
      "Gradient of AB Loss for row 20: tensor([-0.1394,  0.2188, -0.1345])\n",
      "Gradient of AB Loss for row 21: tensor([0.0323, 0.0445, 0.0212, 0.0204, 0.0418, 0.0516, 0.0458, 0.0574, 0.0502,\n",
      "        0.0454, 0.0326, 0.0284, 0.0215, 0.0288])\n",
      "Gradient of AB Loss for row 22: tensor([-0.0414,  0.0431,  0.0257,  0.0263,  0.0336,  0.0317,  0.0215,  0.0463,\n",
      "         0.0559,  0.0380,  0.0407,  0.0290, -0.0346,  0.0219])\n",
      "Gradient of AB Loss for row 23: tensor([-0.0559,  0.0492,  0.0341, -0.0382,  0.0454,  0.0432,  0.0393,  0.0252,\n",
      "         0.0345,  0.0374,  0.0418,  0.0429, -0.0352,  0.0169])\n",
      "Gradient of AB Loss for row 24: tensor([0.0367, 0.0397, 0.0302, 0.0336, 0.0442, 0.0333, 0.0337, 0.0228, 0.0489,\n",
      "        0.0351, 0.0203, 0.0365, 0.0412, 0.0233])\n",
      "Gradient of AB Loss for row 25: tensor([-0.5338])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0355, -0.0174, -0.0346, -0.0464, -0.0293,  0.0414, -0.0285,  0.0363,\n",
      "         0.0355,  0.0435,  0.0508, -0.0379, -0.0361, -0.0292, -0.0220])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0347,  0.0462,  0.0154,  0.0396, -0.0414,  0.0256,  0.0253,  0.0322,\n",
      "         0.0370,  0.0556,  0.0365,  0.0236, -0.0360,  0.0377])\n",
      "Gradient of AB Loss for row 28: tensor([-0.6875])\n",
      "Gradient of AB Loss for row 29: tensor([0.1449, 0.1546, 0.1376, 0.1017])\n",
      "Gradient of AB Loss for row 30: tensor([0.0379, 0.0550, 0.0271, 0.0128, 0.0354, 0.0315, 0.0326, 0.0502, 0.0520,\n",
      "        0.0379, 0.0418, 0.0335, 0.0365, 0.0346])\n",
      "Gradient of AB Loss for row 31: tensor([-0.0247, -0.0397,  0.0516,  0.0147,  0.0418, -0.0425,  0.0398,  0.0393,\n",
      "         0.0288,  0.0478,  0.0278, -0.0344, -0.0223,  0.0340,  0.0288])\n",
      "Gradients of tensors involved in geno_loss calculation:\n",
      "Layer: embedding.token_emb.weight, Gradient Norm: 0.1042894721031189\n",
      "Layer: embedding.norm.weight, Gradient Norm: 0.10652916133403778\n",
      "Layer: embedding.norm.bias, Gradient Norm: 0.17881928384304047\n",
      "Layer: encoders.0.dense.weight, Gradient Norm: 1.0076866149902344\n",
      "Layer: encoders.0.dense.bias, Gradient Norm: 0.11952156573534012\n",
      "Layer: encoders.0.layer_norm.weight, Gradient Norm: 0.20431122183799744\n",
      "Layer: encoders.0.layer_norm.bias, Gradient Norm: 0.28546494245529175\n",
      "Layer: encoders.0.attention.heads.0.q.weight, Gradient Norm: 0.1321042776107788\n",
      "Layer: encoders.0.attention.heads.0.q.bias, Gradient Norm: 0.013566238805651665\n",
      "Layer: encoders.0.attention.heads.0.k.weight, Gradient Norm: 0.15047426521778107\n",
      "Layer: encoders.0.attention.heads.0.k.bias, Gradient Norm: 1.1693933599232764e-09\n",
      "Layer: encoders.0.attention.heads.0.v.weight, Gradient Norm: 0.31181350350379944\n",
      "Layer: encoders.0.attention.heads.0.v.bias, Gradient Norm: 0.07773460447788239\n",
      "Layer: encoders.0.attention.heads.1.q.weight, Gradient Norm: 0.14635951817035675\n",
      "Layer: encoders.0.attention.heads.1.q.bias, Gradient Norm: 0.014179401099681854\n",
      "Layer: encoders.0.attention.heads.1.k.weight, Gradient Norm: 0.16261889040470123\n",
      "Layer: encoders.0.attention.heads.1.k.bias, Gradient Norm: 1.4423086058812373e-09\n",
      "Layer: encoders.0.attention.heads.1.v.weight, Gradient Norm: 0.32080763578414917\n",
      "Layer: encoders.0.attention.heads.1.v.bias, Gradient Norm: 0.07678546756505966\n",
      "Layer: encoders.0.attention.heads.2.q.weight, Gradient Norm: 0.13264818489551544\n",
      "Layer: encoders.0.attention.heads.2.q.bias, Gradient Norm: 0.014852100051939487\n",
      "Layer: encoders.0.attention.heads.2.k.weight, Gradient Norm: 0.1429617702960968\n",
      "Layer: encoders.0.attention.heads.2.k.bias, Gradient Norm: 1.1034096969453344e-09\n",
      "Layer: encoders.0.attention.heads.2.v.weight, Gradient Norm: 0.319588840007782\n",
      "Layer: encoders.0.attention.heads.2.v.bias, Gradient Norm: 0.079549640417099\n",
      "Layer: encoders.0.attention.heads.3.q.weight, Gradient Norm: 0.15634967386722565\n",
      "Layer: encoders.0.attention.heads.3.q.bias, Gradient Norm: 0.015477817505598068\n",
      "Layer: encoders.0.attention.heads.3.k.weight, Gradient Norm: 0.14903655648231506\n",
      "Layer: encoders.0.attention.heads.3.k.bias, Gradient Norm: 1.2556706785460392e-09\n",
      "Layer: encoders.0.attention.heads.3.v.weight, Gradient Norm: 0.32874542474746704\n",
      "Layer: encoders.0.attention.heads.3.v.bias, Gradient Norm: 0.08068186789751053\n",
      "Layer: encoders.0.attention.heads.4.q.weight, Gradient Norm: 0.11004368960857391\n",
      "Layer: encoders.0.attention.heads.4.q.bias, Gradient Norm: 0.010903926566243172\n",
      "Layer: encoders.0.attention.heads.4.k.weight, Gradient Norm: 0.10955069959163666\n",
      "Layer: encoders.0.attention.heads.4.k.bias, Gradient Norm: 1.2181615716144734e-09\n",
      "Layer: encoders.0.attention.heads.4.v.weight, Gradient Norm: 0.3407960534095764\n",
      "Layer: encoders.0.attention.heads.4.v.bias, Gradient Norm: 0.08647851645946503\n",
      "Layer: encoders.0.attention.heads.5.q.weight, Gradient Norm: 0.17737123370170593\n",
      "Layer: encoders.0.attention.heads.5.q.bias, Gradient Norm: 0.0210186168551445\n",
      "Layer: encoders.0.attention.heads.5.k.weight, Gradient Norm: 0.18423466384410858\n",
      "Layer: encoders.0.attention.heads.5.k.bias, Gradient Norm: 1.2182120867620938e-09\n",
      "Layer: encoders.0.attention.heads.5.v.weight, Gradient Norm: 0.3354324996471405\n",
      "Layer: encoders.0.attention.heads.5.v.bias, Gradient Norm: 0.079355888068676\n",
      "Layer: encoders.0.attention.heads.6.q.weight, Gradient Norm: 0.12058983743190765\n",
      "Layer: encoders.0.attention.heads.6.q.bias, Gradient Norm: 0.01310251746326685\n",
      "Layer: encoders.0.attention.heads.6.k.weight, Gradient Norm: 0.12563829123973846\n",
      "Layer: encoders.0.attention.heads.6.k.bias, Gradient Norm: 1.0853843379621253e-09\n",
      "Layer: encoders.0.attention.heads.6.v.weight, Gradient Norm: 0.32587262988090515\n",
      "Layer: encoders.0.attention.heads.6.v.bias, Gradient Norm: 0.0763188973069191\n",
      "Layer: encoders.0.attention.heads.7.q.weight, Gradient Norm: 0.11776576191186905\n",
      "Layer: encoders.0.attention.heads.7.q.bias, Gradient Norm: 0.009499340318143368\n",
      "Layer: encoders.0.attention.heads.7.k.weight, Gradient Norm: 0.12412604689598083\n",
      "Layer: encoders.0.attention.heads.7.k.bias, Gradient Norm: 1.6541356062660384e-09\n",
      "Layer: encoders.0.attention.heads.7.v.weight, Gradient Norm: 0.35619667172431946\n",
      "Layer: encoders.0.attention.heads.7.v.bias, Gradient Norm: 0.08621352165937424\n",
      "Layer: encoders.0.attention.linear.weight, Gradient Norm: 2.6936283111572266\n",
      "Layer: encoders.0.attention.linear.bias, Gradient Norm: 0.4012727737426758\n",
      "Layer: encoders.0.attention.norm.weight, Gradient Norm: 0.05142292380332947\n",
      "Layer: encoders.0.attention.norm.bias, Gradient Norm: 0.06275384873151779\n",
      "Layer: encoders.0.feed_forward.0.weight, Gradient Norm: 0.39423874020576477\n",
      "Layer: encoders.0.feed_forward.0.bias, Gradient Norm: 0.0466497428715229\n",
      "Layer: encoders.0.feed_forward.2.weight, Gradient Norm: 0.41147610545158386\n",
      "Layer: encoders.0.feed_forward.2.bias, Gradient Norm: 0.14686481654644012\n",
      "Layer: encoders.1.dense.weight, Gradient Norm: 1.0020487308502197\n",
      "Layer: encoders.1.dense.bias, Gradient Norm: 0.10585226118564606\n",
      "Layer: encoders.1.layer_norm.weight, Gradient Norm: 0.19308622181415558\n",
      "Layer: encoders.1.layer_norm.bias, Gradient Norm: 0.24557113647460938\n",
      "Layer: encoders.1.attention.heads.0.q.weight, Gradient Norm: 0.0820554792881012\n",
      "Layer: encoders.1.attention.heads.0.q.bias, Gradient Norm: 0.006855988409370184\n",
      "Layer: encoders.1.attention.heads.0.k.weight, Gradient Norm: 0.08670813590288162\n",
      "Layer: encoders.1.attention.heads.0.k.bias, Gradient Norm: 7.850857186575411e-10\n",
      "Layer: encoders.1.attention.heads.0.v.weight, Gradient Norm: 0.29795849323272705\n",
      "Layer: encoders.1.attention.heads.0.v.bias, Gradient Norm: 0.062357913702726364\n",
      "Layer: encoders.1.attention.heads.1.q.weight, Gradient Norm: 0.09476820379495621\n",
      "Layer: encoders.1.attention.heads.1.q.bias, Gradient Norm: 0.009146257303655148\n",
      "Layer: encoders.1.attention.heads.1.k.weight, Gradient Norm: 0.09781195968389511\n",
      "Layer: encoders.1.attention.heads.1.k.bias, Gradient Norm: 1.1652639964054856e-09\n",
      "Layer: encoders.1.attention.heads.1.v.weight, Gradient Norm: 0.32007932662963867\n",
      "Layer: encoders.1.attention.heads.1.v.bias, Gradient Norm: 0.07304907590150833\n",
      "Layer: encoders.1.attention.heads.2.q.weight, Gradient Norm: 0.08970488607883453\n",
      "Layer: encoders.1.attention.heads.2.q.bias, Gradient Norm: 0.008519496768712997\n",
      "Layer: encoders.1.attention.heads.2.k.weight, Gradient Norm: 0.08535946905612946\n",
      "Layer: encoders.1.attention.heads.2.k.bias, Gradient Norm: 1.8479673347115977e-09\n",
      "Layer: encoders.1.attention.heads.2.v.weight, Gradient Norm: 0.32471808791160583\n",
      "Layer: encoders.1.attention.heads.2.v.bias, Gradient Norm: 0.0697992742061615\n",
      "Layer: encoders.1.attention.heads.3.q.weight, Gradient Norm: 0.09982689470052719\n",
      "Layer: encoders.1.attention.heads.3.q.bias, Gradient Norm: 0.010578922927379608\n",
      "Layer: encoders.1.attention.heads.3.k.weight, Gradient Norm: 0.10320612043142319\n",
      "Layer: encoders.1.attention.heads.3.k.bias, Gradient Norm: 9.863633243512027e-10\n",
      "Layer: encoders.1.attention.heads.3.v.weight, Gradient Norm: 0.2985963821411133\n",
      "Layer: encoders.1.attention.heads.3.v.bias, Gradient Norm: 0.06544153392314911\n",
      "Layer: encoders.1.attention.heads.4.q.weight, Gradient Norm: 0.07262425869703293\n",
      "Layer: encoders.1.attention.heads.4.q.bias, Gradient Norm: 0.007381874602288008\n",
      "Layer: encoders.1.attention.heads.4.k.weight, Gradient Norm: 0.06874188780784607\n",
      "Layer: encoders.1.attention.heads.4.k.bias, Gradient Norm: 1.5888632631799737e-09\n",
      "Layer: encoders.1.attention.heads.4.v.weight, Gradient Norm: 0.30922889709472656\n",
      "Layer: encoders.1.attention.heads.4.v.bias, Gradient Norm: 0.06546685844659805\n",
      "Layer: encoders.1.attention.heads.5.q.weight, Gradient Norm: 0.10512413084506989\n",
      "Layer: encoders.1.attention.heads.5.q.bias, Gradient Norm: 0.009727118536829948\n",
      "Layer: encoders.1.attention.heads.5.k.weight, Gradient Norm: 0.10900938510894775\n",
      "Layer: encoders.1.attention.heads.5.k.bias, Gradient Norm: 9.573370984838903e-10\n",
      "Layer: encoders.1.attention.heads.5.v.weight, Gradient Norm: 0.3097996413707733\n",
      "Layer: encoders.1.attention.heads.5.v.bias, Gradient Norm: 0.06828765571117401\n",
      "Layer: encoders.1.attention.heads.6.q.weight, Gradient Norm: 0.08157031983137131\n",
      "Layer: encoders.1.attention.heads.6.q.bias, Gradient Norm: 0.007494163233786821\n",
      "Layer: encoders.1.attention.heads.6.k.weight, Gradient Norm: 0.08437122404575348\n",
      "Layer: encoders.1.attention.heads.6.k.bias, Gradient Norm: 8.190718658873664e-10\n",
      "Layer: encoders.1.attention.heads.6.v.weight, Gradient Norm: 0.2973722815513611\n",
      "Layer: encoders.1.attention.heads.6.v.bias, Gradient Norm: 0.06516797840595245\n",
      "Layer: encoders.1.attention.heads.7.q.weight, Gradient Norm: 0.08777527511119843\n",
      "Layer: encoders.1.attention.heads.7.q.bias, Gradient Norm: 0.009515148587524891\n",
      "Layer: encoders.1.attention.heads.7.k.weight, Gradient Norm: 0.08235267549753189\n",
      "Layer: encoders.1.attention.heads.7.k.bias, Gradient Norm: 1.840665619923243e-09\n",
      "Layer: encoders.1.attention.heads.7.v.weight, Gradient Norm: 0.2694302797317505\n",
      "Layer: encoders.1.attention.heads.7.v.bias, Gradient Norm: 0.052943434566259384\n",
      "Layer: encoders.1.attention.linear.weight, Gradient Norm: 2.641633987426758\n",
      "Layer: encoders.1.attention.linear.bias, Gradient Norm: 0.3413826525211334\n",
      "Layer: encoders.1.attention.norm.weight, Gradient Norm: 0.05897018313407898\n",
      "Layer: encoders.1.attention.norm.bias, Gradient Norm: 0.06340062618255615\n",
      "Layer: encoders.1.feed_forward.0.weight, Gradient Norm: 0.38653364777565\n",
      "Layer: encoders.1.feed_forward.0.bias, Gradient Norm: 0.040363162755966187\n",
      "Layer: encoders.1.feed_forward.2.weight, Gradient Norm: 0.38383302092552185\n",
      "Layer: encoders.1.feed_forward.2.bias, Gradient Norm: 0.12584544718265533\n",
      "Layer: token_prediction_layer.weight, Gradient Norm: 2.0448267459869385\n",
      "Layer: token_prediction_layer.bias, Gradient Norm: 0.2313956320285797\n",
      "ab_loss: 24.553293228149414\n",
      "Gradient of AB Loss for row 0: tensor([-0.0489, -0.0557, -0.0385,  0.0390, -0.0494,  0.0440,  0.0529,  0.0585,\n",
      "         0.0482,  0.0421, -0.0421, -0.0283,  0.0365])\n",
      "Gradient of AB Loss for row 1: tensor([ 0.0383, -0.0398,  0.0412,  0.0193,  0.0191,  0.0316,  0.0445,  0.0414,\n",
      "         0.0524,  0.0459,  0.0395,  0.0501,  0.0337,  0.0321])\n",
      "Gradient of AB Loss for row 2: tensor([0.1611, 0.0959, 0.0883, 0.1332])\n",
      "Gradient of AB Loss for row 3: tensor([-0.0277, -0.0322, -0.0296,  0.0320,  0.0352,  0.0097, -0.0220,  0.0316,\n",
      "         0.0401,  0.0376,  0.0282,  0.0220,  0.0241,  0.0274, -0.0302,  0.0196,\n",
      "        -0.0373,  0.0330])\n",
      "Gradient of AB Loss for row 4: tensor([ 0.0272, -0.0431,  0.0477,  0.0277,  0.0383,  0.0477,  0.0355,  0.0302,\n",
      "         0.0419,  0.0447,  0.0236,  0.0395,  0.0168,  0.0395])\n",
      "Gradient of AB Loss for row 5: tensor([0.0298, 0.0435, 0.0233, 0.0269, 0.0486, 0.0279, 0.0365, 0.0418, 0.0354,\n",
      "        0.0467, 0.0324, 0.0434, 0.0361, 0.0458])\n",
      "Gradient of AB Loss for row 6: tensor([-0.0177,  0.0324, -0.0130, -0.0171, -0.0291, -0.0237,  0.0286,  0.0183,\n",
      "        -0.0176, -0.0136, -0.0299,  0.0135,  0.0080,  0.0313,  0.0213, -0.0172,\n",
      "         0.0267, -0.0142, -0.0107, -0.0089,  0.0268, -0.0261,  0.0175])\n",
      "Gradient of AB Loss for row 7: tensor([-0.0346, -0.0398, -0.0275,  0.0351,  0.0187, -0.0098,  0.0363,  0.0331,\n",
      "         0.0271,  0.0322,  0.0208,  0.0369,  0.0299, -0.0242,  0.0261, -0.0215,\n",
      "         0.0176, -0.0181])\n",
      "Gradient of AB Loss for row 8: tensor([-0.0334, -0.0290, -0.0330, -0.0337,  0.0442,  0.0235,  0.0120, -0.0286,\n",
      "         0.0277,  0.0451,  0.0312,  0.0313,  0.0322,  0.0297, -0.0312, -0.0302,\n",
      "        -0.0273,  0.0142])\n",
      "Gradient of AB Loss for row 9: tensor([-0.0481, -0.0378,  0.0510,  0.0448, -0.0407,  0.0349,  0.0410,  0.0468,\n",
      "         0.0351,  0.0396,  0.0313,  0.0208, -0.0471,  0.0292])\n",
      "Gradient of AB Loss for row 10: tensor([-0.0373,  0.0501,  0.0118,  0.0405, -0.0421,  0.0279,  0.0384,  0.0285,\n",
      "         0.0474,  0.0457,  0.0307, -0.0401, -0.0166,  0.0504])\n",
      "Gradient of AB Loss for row 11: tensor([-0.0428, -0.0287, -0.0379, -0.0492, -0.0328,  0.0458, -0.0386,  0.0363,\n",
      "         0.0399,  0.0387,  0.0467, -0.0367, -0.0445, -0.0277, -0.0207])\n",
      "Gradient of AB Loss for row 12: tensor([0.0351, 0.0518, 0.0151, 0.0367, 0.0411, 0.0243, 0.0356, 0.0403, 0.0394,\n",
      "        0.0464, 0.0270, 0.0263, 0.0271, 0.0349])\n",
      "Gradient of AB Loss for row 13: tensor([-0.0572,  0.0444,  0.0404, -0.0460,  0.0406,  0.0519,  0.0393,  0.0324,\n",
      "         0.0463,  0.0349,  0.0377,  0.0423, -0.0442,  0.0381])\n",
      "Gradient of AB Loss for row 14: tensor([0.1629, 0.1581, 0.1112, 0.1448])\n",
      "Gradient of AB Loss for row 15: tensor([0.0302, 0.0391, 0.0344, 0.0181, 0.0359, 0.0334, 0.0254, 0.0450, 0.0486,\n",
      "        0.0448, 0.0508, 0.0326, 0.0366, 0.0393])\n",
      "Gradient of AB Loss for row 16: tensor([-0.0360, -0.0261,  0.0306,  0.0220,  0.0226,  0.0353,  0.0329,  0.0289,\n",
      "         0.0402,  0.0320,  0.0350,  0.0270,  0.0301, -0.0205,  0.0275, -0.0322,\n",
      "        -0.0212,  0.0132])\n",
      "Gradient of AB Loss for row 17: tensor([-0.0397, -0.0262, -0.0420, -0.0366, -0.0325, -0.0325,  0.0350, -0.0181,\n",
      "        -0.0375,  0.0232,  0.0390, -0.0283,  0.0369, -0.0206, -0.0231, -0.0159])\n",
      "Gradient of AB Loss for row 18: tensor([0.0238, 0.0448, 0.0152, 0.0367, 0.0350, 0.0341, 0.0269, 0.0274, 0.0498,\n",
      "        0.0380, 0.0449, 0.0281, 0.0235, 0.0230])\n",
      "Gradient of AB Loss for row 19: tensor([-0.5380])\n",
      "Gradient of AB Loss for row 20: tensor([0.0331, 0.0351, 0.0213, 0.0456, 0.0355, 0.0331, 0.0388, 0.0350, 0.0477,\n",
      "        0.0366, 0.0240, 0.0360, 0.0255, 0.0246])\n",
      "Gradient of AB Loss for row 21: tensor([-0.0323, -0.0593, -0.0378, -0.0185,  0.0405, -0.0469,  0.0315,  0.0302,\n",
      "         0.0309,  0.0424,  0.0349, -0.0399, -0.0367])\n",
      "Gradient of AB Loss for row 22: tensor([0.1652, 0.1455, 0.1408, 0.1655])\n",
      "Gradient of AB Loss for row 23: tensor([0.1005, 0.1529, 0.1665, 0.1562])\n",
      "Gradient of AB Loss for row 24: tensor([0.0294, 0.0430, 0.0226, 0.0376, 0.0370, 0.0310, 0.0468, 0.0358, 0.0435,\n",
      "        0.0336, 0.0345, 0.0382, 0.0237, 0.0297])\n",
      "Gradient of AB Loss for row 25: tensor([0.0420, 0.0584, 0.0097, 0.0341, 0.0489, 0.0351, 0.0189, 0.0322, 0.0513,\n",
      "        0.0550, 0.0289, 0.0236, 0.0347, 0.0353])\n",
      "Gradient of AB Loss for row 26: tensor([-0.0319, -0.0379, -0.0299, -0.0425,  0.0431,  0.0363,  0.0223, -0.0277,\n",
      "         0.0268,  0.0484,  0.0384,  0.0302, -0.0408, -0.0260,  0.0279])\n",
      "Gradient of AB Loss for row 27: tensor([-0.0473, -0.0438,  0.0485,  0.0384, -0.0456,  0.0314,  0.0193,  0.0372,\n",
      "         0.0458,  0.0450,  0.0363,  0.0365, -0.0348,  0.0209])\n",
      "Gradient of AB Loss for row 28: tensor([ 0.0344,  0.0539,  0.0144,  0.0410,  0.0388,  0.0392,  0.0503,  0.0473,\n",
      "         0.0341, -0.0370,  0.0224,  0.0208,  0.0385])\n",
      "Gradient of AB Loss for row 29: tensor([-0.0314, -0.0284, -0.0346, -0.0571, -0.0443,  0.0292, -0.0314, -0.0449,\n",
      "         0.0378,  0.0478, -0.0370,  0.0421, -0.0397, -0.0340, -0.0384])\n",
      "Gradient of AB Loss for row 30: tensor([-0.0174, -0.0307, -0.0314, -0.0289,  0.0287,  0.0116, -0.0119, -0.0189,\n",
      "         0.0314, -0.0189,  0.0382, -0.0378,  0.0236, -0.0214,  0.0196, -0.0149])\n",
      "Gradient of AB Loss for row 31: tensor([ 0.0449, -0.0430,  0.0509,  0.0238,  0.0284,  0.0439,  0.0486,  0.0405,\n",
      "         0.0478,  0.0474,  0.0319,  0.0431,  0.0201,  0.0313])\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:13\n",
      "-=Training completed=-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 0,\n",
       " 'geno_train_losses': [0.0, 0.0],\n",
       " 'ab_train_losses': [0.0, 0.0],\n",
       " 'geno_val_losses': [1.7526093101501465, 1.7526093101501465],\n",
       " 'ab_val_losses': [5.936623992919922, 5.936623992919922],\n",
       " 'val_accs': [0.6101818181818182, 0.6101818181818182]}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.001\n",
    "bert_test = BERT_ft(len(vocabulary_geno), max_length, dim_emb, dim_hidden, attention_heads, num_encoders, drop_prob, len(vocabulary_pheno), device)\n",
    "\n",
    "test = BertTrainer_ft(bert_test, train_set, val_set, 2, batch_size, lr, device, stop_patience,  False, \"NCBI\", \"test\")\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "class BertTrainer_ft:\n",
    "    def __init__(self, model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, project_name, wandb_name):\n",
    "        self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.train_size = len(train_set)\n",
    "        self.val_set = val_set\n",
    "        self.epochs = epochs    \n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = self.train_size // self.batch_size\n",
    "        self.lr = lr\n",
    "        self.weight_decay = 0.01\n",
    "        self.current_epoch  = 0\n",
    "        self.early_stopping_counter = 0\t\n",
    "        self.patience = stop_patience\n",
    "        \n",
    "        self.wandb_mode = wandb_mode\n",
    "        self.project_name = project_name\n",
    "        self.wandb_name = wandb_name\n",
    "        \n",
    "        #self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        self.token_criterion = nn.CrossEntropyLoss(ignore_index=-1).to(device)\n",
    "        self.ab_criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self):\n",
    "        if self.wandb_mode:\n",
    "            self._init_wandb()\n",
    "        self.val_set.prepare_dataset() \n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            self.train_set.prepare_dataset()\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            avg_epoch_loss_geno, avg_epoch_loss_ab = self.train(self.current_epoch)\n",
    "            self.train_losses_geno.append(avg_epoch_loss_geno) \n",
    "            self.train_losses_ab.append(avg_epoch_loss_ab)  \n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min\")\n",
    "\n",
    "            # Validation\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            val_results = self.evaluate(self.val_loader)\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            self.val_losses_geno.append(val_results[0])\n",
    "            self.val_losses_ab.append(val_results[1])\n",
    "            self.val_accs.append(val_results[2])\n",
    "            if self.wandb_mode:\n",
    "                self._report_epoch_results()\n",
    "            criterion = self.stop_early()\n",
    "            if criterion:\n",
    "                print(f\"Training interrupted at epoch: {self.current_epoch+1}\")\n",
    "                break\n",
    "        print(f\"-=Training completed=-\")\n",
    "        results = {\n",
    "            \"best_epoch\": self.best_epoch,\n",
    "            \"geno_train_losses\": self.train_losses_geno,\n",
    "            \"ab_train_losses\": self.train_losses_ab,\n",
    "            \"geno_val_losses\": self.val_losses_geno,\n",
    "            \"ab_val_losses\": self.val_losses_ab,\n",
    "            \"val_accs\": self.val_accs\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _init_result_lists(self):\n",
    "        self.train_losses_geno = []\n",
    "        self.train_losses_ab = []\n",
    "        self.val_losses_geno = []\n",
    "        self.val_losses_ab = []\n",
    "        self.val_accs = []\n",
    "\n",
    "    def stop_early(self):\n",
    "        if self.val_losses_ab[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses_ab[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss_geno = 0\n",
    "        epoch_loss_ab = 0\n",
    "\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "            \n",
    "            ABinclusion = torch.unique(AB_idx)\n",
    "            ABinclusion = ABinclusion[ABinclusion != -1]\n",
    "            ABinclusion = ABinclusion.tolist()\n",
    "            #self.model.exclude_networks(ABinclusion)\n",
    "\n",
    "            self.optimizer.zero_grad() \n",
    "\n",
    "            token_predictions, resistance_predictions = self.model(input, attn_mask) \n",
    "            geno_loss = self.token_criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "            list_AB_predictions = []\n",
    "            for i, row in enumerate(resistance_predictions):\n",
    "                AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                current_abs = []\n",
    "                for ab in AB_list:\n",
    "                    current_abs.append(row[ab].item())\n",
    "                current_abs = torch.tensor(current_abs, requires_grad=True)\n",
    "                list_AB_predictions.append(current_abs)\n",
    "            \n",
    "            processed_tensor = [row[row != -1] for row in SR_class]\n",
    "            ab_loss = 0\n",
    "            for i, row in enumerate(processed_tensor):\n",
    "                row = torch.tensor(row, dtype=torch.float32, requires_grad=True)\n",
    "                list_AB_predictions[i] = torch.tensor(list_AB_predictions[i], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "                ab_loss += self.ab_criterion(list_AB_predictions[i], row)\n",
    "            \n",
    "            geno_loss.backward()\n",
    "                # Print gradients of tensors involved in geno_loss calculation\n",
    "            print(\"Gradients of tensors involved in geno_loss calculation:\")\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(f\"Layer: {name}, Gradient Norm: {param.grad.norm().item()}\")\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            print(f\"ab_loss: {ab_loss.item()}\")  # Print the value of ab_loss\n",
    "            \n",
    "            # Compute gradients of ab_loss\n",
    "            ab_loss.backward() \n",
    "            \n",
    "            # Print gradients of tensors involved in ab_loss calculation\n",
    "            for i, row in enumerate(processed_tensor):\n",
    "                print(f\"Gradient of AB Loss for row {i}: {list_AB_predictions[i].grad}\")\n",
    "            \n",
    "            # Update parameters\n",
    "            self.optimizer.step()\n",
    "            #self.model.reset_exclusion()   \n",
    "            \n",
    "        avg_epoch_loss_geno = epoch_loss_geno / self.num_batches\n",
    "        avg_epoch_loss_ab = epoch_loss_ab / self.num_batches\n",
    "\n",
    "        return avg_epoch_loss_geno, avg_epoch_loss_ab\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        epoch_loss_geno = 0\n",
    "        epoch_loss_ab = 0\n",
    "        total_correct = 0\n",
    "        total_sum = 0\n",
    "  \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(loader):\n",
    "                input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "\n",
    "                token_predictions, resistance_predictions = self.model(input, attn_mask) \n",
    "                geno_loss = self.token_criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "                \n",
    "                list_AB_predictions = []\n",
    "                for i, row in enumerate(resistance_predictions):\n",
    "                    AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                    current_abs = []\n",
    "                    for ab in AB_list:\n",
    "                        current_abs.append(row[ab].item())\n",
    "                    current_abs = torch.tensor(current_abs)\n",
    "                    list_AB_predictions.append(current_abs)\n",
    "                \n",
    "                processed_tensor = [row[row != -1] for row in SR_class]\n",
    "                ab_loss = 0\n",
    "                for i, row in enumerate(processed_tensor):\n",
    "                    row = row.type(torch.float32)\n",
    "                    ab_loss += self.ab_criterion(list_AB_predictions[i], row)\n",
    "\n",
    "                epoch_loss_geno += geno_loss.item()\n",
    "                epoch_loss_ab += ab_loss.item() \n",
    "                \n",
    "                list_AB_predictions = []\n",
    "                for i, row in enumerate(resistance_predictions):\n",
    "                    AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                    current_abs = []\n",
    "                    for ab in AB_list:\n",
    "                        current_abs.append(row[ab].item())\n",
    "                    current_abs = torch.tensor(current_abs)\n",
    "                    current_abs = current_abs.type(torch.int16)\n",
    "                    list_AB_predictions.append(current_abs)\n",
    "                \n",
    "                    processed_tensor = [row[row != -1] for row in SR_class]\n",
    "                for i, row in enumerate(processed_tensor):\n",
    "                    total_correct += (row == list_AB_predictions[i]).sum().item()\n",
    "                    total_sum += len(row)\n",
    "\n",
    "        avg_epoch_loss_geno = epoch_loss_geno / self.num_batches\n",
    "        avg_epoch_loss_ab = epoch_loss_ab / self.num_batches\n",
    "\n",
    "        accuracy = total_correct / total_sum\n",
    "\n",
    "        return avg_epoch_loss_geno, avg_epoch_loss_ab, accuracy\n",
    "    \n",
    "    def _save_model(self, savepath: Path):\n",
    "        torch.save(self.best_model_state, savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        \n",
    "    def _load_model(self, savepath: Path):\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "    def _init_wandb(self):\n",
    "        self.wandb_run = wandb.init(\n",
    "            project=self.project_name,\n",
    "            name=self.wandb_name,\n",
    "            \n",
    "            config={\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"num_heads\": self.model.attention_heads,\n",
    "                \"num_encoders\": self.model.num_encoders,\n",
    "                \"emb_dim\": self.model.dim_embedding,\n",
    "                'ff_dim': self.model.dim_embedding,\n",
    "                \"lr\": self.lr,\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "                \"max_seq_len\": self.model.max_length[0],\n",
    "                \"vocab_size\": len(self.train_set.vocab_geno),\n",
    "                \"num_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "            }\n",
    "        )\n",
    "        self.wandb_run.watch(self.model)\n",
    "        self.wandb_run.define_metric(\"epoch\", hidden=True)\n",
    "        self.wandb_run.define_metric(\"batch\", hidden=True)\n",
    "\n",
    "        self.wandb_run.define_metric(\"GenoLosses/geno_train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"GenoLosses/geno_val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "        self.wandb_run.define_metric(\"AB_Losses/ab_train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"AB_Losses/ab_val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_acc\", summary=\"min\", step_metric=\"epoch\")\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/final_val_loss\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_acc\")\n",
    "        self.wandb_run.define_metric(\"final_epoch\")\n",
    "\n",
    "        return self.wandb_run\n",
    "    \n",
    "    def _report_epoch_results(self):\n",
    "        wandb_dict = {\n",
    "            \"epoch\": self.current_epoch+1,\n",
    "            \"GenoLosses/geno_train_loss\": self.train_losses_geno[-1],\n",
    "            \"ABLosses/ab_train_loss\": self.train_losses_ab[-1],\n",
    "            \"GenoLosses/geno_val_loss\": self.val_losses_geno[-1],\n",
    "            \"ABLosses/ab_val_loss\": self.val_losses_ab[-1],\n",
    "            \"Accuracies/val_acc\": self.val_accs[-1],\n",
    "        }\n",
    "        self.wandb_run.log(wandb_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
