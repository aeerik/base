{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from itertools import chain \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lokalt\n",
    "data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\repo\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#station√§r\n",
    "data_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saga\n",
    "data_dir = \"/home/aeerik/data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget config file\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "max_length = [51,81]\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 32\n",
    "drop_prob = 0.2\n",
    "\n",
    "#Encoder\n",
    "dim_emb = 128\n",
    "dim_hidden = 128\n",
    "attention_heads = 8 \n",
    "\n",
    "#BERT\n",
    "num_encoders = 2\n",
    "\n",
    "#trainer\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "stop_patience = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "include_pheno = False\n",
    "vocabulary = vocab_geno(NCBI, include_pheno)\n",
    "vocab = vocab_pheno(ab_df)\n",
    "print(len(vocabulary))\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from data_preprocessing import data_loader\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "\n",
    "max_length = [51,81]\n",
    "mask_prob = 0.25\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = deepcopy(NCBI['AST_phenotypes'].tolist())\n",
    "max_seq_len = [max_length[0],max_length[1]]\n",
    "list_idx = []\n",
    "list_SR = []\n",
    "for i in range(len(sequences)):\n",
    "    current_seq = sequences[i]\n",
    "    current_idxs = []\n",
    "    current_SRs = []\n",
    "    for j in range(len(current_seq)):\n",
    "        item = current_seq[j].split('=')\n",
    "        abs = item[0]   \n",
    "        sr = item[1]\n",
    "        current_idxs.append(vocabulary_pheno.lookup_indices([abs]))\n",
    "        for k in range(len(sr)):\n",
    "            if sr == 'R':\n",
    "                current_SRs.append(1)\n",
    "            else:\n",
    "                current_SRs.append(0)\n",
    "\n",
    "    if len(current_idxs) != len(current_SRs):\n",
    "        print(\"current sequence:\",current_seq, \"\\n\", \"with length:\", len(current_seq))\n",
    "        print(\"indexes:\",current_idxs, \"with length:\", len(current_idxs))\n",
    "        print(\"suceptability\",current_SRs, \"with length:\", len(current_SRs))\n",
    "        print('error at', j)\n",
    "        print(\"--------------------\")\n",
    "    current_idxs = [int(item[0]) for item in current_idxs]\n",
    "    #for i in range(0,max_length[1] - len(current_idxs)):\n",
    "    #    current_idxs.append(-1)\n",
    "    #for i in range(0,max_length[1] - len(current_SRs)):\n",
    "    #    current_SRs.append(-1)\n",
    "    list_idx.append(current_idxs)\n",
    "    list_SR.append(current_SRs)\n",
    "for i in range(len(list_idx)):\n",
    "    if len(list_idx[i]) != len(list_SR[i]):\n",
    "        print('error at', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_loss(logits, targets, pad_index=-1):\n",
    "    #print(logits)\n",
    "    #print(targets)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "    #print(loss)\n",
    "    mask = (targets != pad_index).float()\n",
    "    #print(mask)\n",
    "    masked_loss = loss * mask\n",
    "    #print(masked_loss)\n",
    "    average_loss = masked_loss.sum() / mask.sum()\n",
    "    #print(average_loss)\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Retrieving data from: c:\\Users\\erika\\Desktop\\Exjobb\\data\n",
      "Loading data...\n",
      "Data correctly loaded, 6491 samples found\n",
      "Creating vocabulary...\n",
      "Vocabulary created with number of elements: 1227\n",
      "Number of antibiotics: 81\n",
      "Reducing samples to 1000\n",
      "Datasets has been created with 800 samples in the training set and 200 samples in the validation set\n"
     ]
    }
   ],
   "source": [
    "#Relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "from misc import get_split_indices\n",
    "from misc import export_results\n",
    "from data_preprocessing import data_loader, data_original\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from create_dataset import NCBIDataset\n",
    "from bert_builder import BERT\n",
    "from trainer import BertTrainer_ft\n",
    "from trainer import BertTrainer_pt\n",
    "from misc import get_paths\n",
    "from misc import model_loader\n",
    "\n",
    "limit_data = True\n",
    "reduced_samples = 1000\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "max_length = [51,81]\n",
    "mask_prob   = 0.15\n",
    "\n",
    "print(f\"\\n Retrieving data from: {data_dir}\")\n",
    "print(\"Loading data...\")\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_dir,ab_dir)\n",
    "NCBI_geno_only = data_original(threshold_year,data_dir, ab_dir)\n",
    "print(f\"Data correctly loaded, {len(NCBI)} samples found\")\n",
    "print(\"Creating vocabulary...\")\n",
    "vocabulary_geno = vocab_geno(NCBI_geno_only)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "print(f\"Vocabulary created with number of elements:\",len(vocabulary_geno))\n",
    "if include_pheno:\n",
    "    print(f\"Number of antibiotics:\",len(vocabulary_pheno))\n",
    "\n",
    "if limit_data:\n",
    "    print(f\"Reducing samples to {reduced_samples}\")\n",
    "    NCBI = NCBI.head(reduced_samples)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "print(f\"Datasets has been created with {len(train_set)} samples in the training set and {len(val_set)} samples in the validation set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 0])\n",
      "tensor([0, 1, 0, 1], dtype=torch.int16)\n",
      "TP: 1 FN: 2 TN: 0 FP: 1\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1], dtype=torch.int16)\n",
      "TP: 1 FN: 2 TN: 5 FP: 10\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1], dtype=torch.int16)\n",
      "TP: 3 FN: 3 TN: 10 FP: 16\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0])\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0], dtype=torch.int16)\n",
      "TP: 4 FN: 6 TN: 17 FP: 19\n",
      "tensor([1])\n",
      "tensor([0], dtype=torch.int16)\n",
      "TP: 4 FN: 7 TN: 17 FP: 19\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0], dtype=torch.int16)\n",
      "TP: 4 FN: 7 TN: 26 FP: 24\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int16)\n",
      "TP: 4 FN: 7 TN: 32 FP: 32\n",
      "tensor([0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 1], dtype=torch.int16)\n",
      "TP: 4 FN: 7 TN: 34 FP: 34\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1], dtype=torch.int16)\n",
      "TP: 5 FN: 9 TN: 39 FP: 39\n",
      "tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0])\n",
      "tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1], dtype=torch.int16)\n",
      "TP: 8 FN: 11 TN: 44 FP: 45\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1], dtype=torch.int16)\n",
      "TP: 9 FN: 11 TN: 51 FP: 51\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], dtype=torch.int16)\n",
      "TP: 11 FN: 14 TN: 59 FP: 53\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1], dtype=torch.int16)\n",
      "TP: 12 FN: 15 TN: 65 FP: 59\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
      "tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1], dtype=torch.int16)\n",
      "TP: 14 FN: 18 TN: 68 FP: 65\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=torch.int16)\n",
      "TP: 14 FN: 18 TN: 76 FP: 71\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1], dtype=torch.int16)\n",
      "TP: 15 FN: 20 TN: 81 FP: 77\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from bert_builder import BERT\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from loss_functions import custom_loss\n",
    "\n",
    "dim_emb = 128\n",
    "dim_hidden = 128 \n",
    "num_encoders = 2 \n",
    "drop_prob = 0.2     \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "bert_test = BERT(vocab_size=len(vocabulary_geno), dim_embedding = dim_emb, dim_hidden=dim_hidden, attention_heads=8, num_encoders=num_encoders, dropout_prob=drop_prob, num_ab=len(vocabulary_pheno), device=device).to(device)\n",
    "optimizer = torch.optim.AdamW(bert_test.parameters(), lr=0.001, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = -1).to(device)\n",
    "train_set.prepare_dataset()\n",
    "\n",
    "loader = DataLoader(train_set, batch_size=16, shuffle=False)\n",
    "total_correct = 0\n",
    "total_tokens = 0\n",
    "total_sum = 0\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0   \n",
    "\n",
    "for i, batch in enumerate(loader):\n",
    "    if i ==1:\n",
    "        break\n",
    "    input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "\n",
    "    token_predictions, resistance_predictions = bert_test(input, attn_mask) \n",
    "    geno_loss = criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "\n",
    "    result_list = []\n",
    "    for j in range(len(AB_idx)):\n",
    "        result_tensor = torch.full((81,), -1, device=device)  # Create tensor filled with -1 values\n",
    "        for idx, value in enumerate(AB_idx[j]):\n",
    "            if value != -1:\n",
    "                result_tensor[value.item()] = SR_class[j][idx]\n",
    "        result_list.append(result_tensor)\n",
    "    ab_loss = 0\n",
    "    pheno_loss = 0\n",
    "    for i, row in enumerate(resistance_predictions):\n",
    "        prediction = row\n",
    "        target = result_list[i]\n",
    "        ab_loss = custom_loss(prediction, target.float()) \n",
    "        pheno_loss += ab_loss\n",
    "\n",
    "    list_AB_predictions = []\n",
    "    pred_res = torch.where(resistance_predictions > 0, torch.ones_like(resistance_predictions), torch.zeros_like(resistance_predictions))\n",
    "\n",
    "    for i, row in enumerate(pred_res):\n",
    "        AB_list = 0\n",
    "        AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "        current_abs = []\n",
    "        for ab in AB_list:\n",
    "            current_abs.append(row[ab].item())\n",
    "        current_abs = torch.tensor(current_abs)\n",
    "        current_abs = current_abs.type(torch.int16)\n",
    "        list_AB_predictions.append(current_abs)\n",
    "\n",
    "        processed_tensor = [row[row != -1] for row in SR_class]\n",
    "    for i, row in enumerate(processed_tensor):\n",
    "        row = row.to(device)  # Move row tensor to the same device\n",
    "        list_AB_predictions[i] = list_AB_predictions[i].to(device)\n",
    "        print(row)\n",
    "        print(list_AB_predictions[i])\n",
    "        total_correct += (row == list_AB_predictions[i]).sum().item()\n",
    "        total_sum += len(row)\n",
    "\n",
    "        TP += torch.sum((list_AB_predictions[i] == 1) & (row == 1)).item()\n",
    "        FN += torch.sum((list_AB_predictions[i] == 0) & (row == 1)).item()\n",
    "        TN += torch.sum((list_AB_predictions[i] == 0) & (row == 0)).item()\n",
    "        FP += torch.sum((list_AB_predictions[i] == 1) & (row == 0)).item()\n",
    "        print('TP:',TP,'FN:', FN, 'TN:', TN,'FP:', FP)\n",
    "\n",
    "sensitivity = TP / (TP + FN) if TP + FN != 0 else 0  # Avoid division by zero\n",
    "specificity = TN / (TN + FP) if TN + FP != 0 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "from embedding import JointEmbedding\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_embedding, drop_prob):\n",
    "        super(AttentionHead, self).__init__()\n",
    "\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.q = nn.Linear(self.dim_embedding, self.dim_embedding)\n",
    "        self.k = nn.Linear(self.dim_embedding, self.dim_embedding)\n",
    "        self.v = nn.Linear(self.dim_embedding, self.dim_embedding)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)\n",
    "\n",
    "        scale = query.size(1) ** 0.5\n",
    "        scores = torch.matmul(query, key.transpose(-1, -2)) / scale\n",
    "\n",
    "        scores = scores.masked_fill_(attention_mask, -1e9)\n",
    "        attn = f.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, value)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, dim_embedding, drop_prob):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(dim_embedding,drop_prob) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(dim_embedding * num_heads, dim_embedding)\n",
    "        self.norm = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        s = [head(input_tensor, attention_mask) for head in self.heads]\n",
    "        scores = torch.cat(s, dim=-1)\n",
    "        scores = self.linear(scores)\n",
    "        return self.norm(scores)\n",
    "\n",
    "class resEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_embedding, attention_heads, dropout_prob):\n",
    "        super(resEncoder, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.attention_heads = attention_heads\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dense = nn.Linear(self.dim_embedding, self.dim_embedding)\n",
    "        self.layer_norm = nn.LayerNorm(self.dim_embedding)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)       \n",
    "        \n",
    "        self.attention = MultiHeadAttention(self.attention_heads, self.dim_embedding, self.dropout_prob)  \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(self.dim_embedding, self.dim_embedding),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.dim_embedding, self.dim_embedding),\n",
    "            nn.Dropout(self.dropout_prob)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(self.dim_embedding)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        x = input_tensor\n",
    "        context = self.attention(input_tensor, attention_mask)\n",
    "\n",
    "        hidden_states = self.dense(context)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        x = x + hidden_states\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        res = x\n",
    "        x = self.feed_forward(x)\n",
    "        x = x + res\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dim_embedding, dim_hidden, attention_heads, num_encoders, dropout_prob, num_ab, device):\n",
    "        super(BERT, self).__init__()\n",
    "        self.attention_heads = attention_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dim_hidden = dim_hidden    \n",
    "        self.num_encoders = num_encoders\n",
    "        self.dropout_prob = dropout_prob  \n",
    "\n",
    "        self.embedding = JointEmbedding(self.dim_embedding, self.vocab_size, self.dropout_prob)\n",
    "        self.encoders = nn.ModuleList([resEncoder(self.dim_embedding, self.attention_heads, self.dropout_prob) for _ in range(self.num_encoders)])\n",
    "        self.token_prediction_layer = nn.Linear(self.dim_embedding, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.BC = [BC_Ab(self.dim_embedding, self.dim_hidden).to(device) for _ in range(num_ab)]\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        embedded = self.embedding(input_tensor)\n",
    "        for layer in self.encoders:\n",
    "            embedded = layer(embedded, attention_mask)\n",
    "        \n",
    "        cls_tokens = embedded[:, 0, :]\n",
    "        resistance_predictions = torch.cat([network(cls_tokens) for network in self.BC], dim=1)\n",
    "        token_predictions = self.token_prediction_layer(embedded)\n",
    "        #token_predictions = self.softmax(token_predictions)\n",
    "\n",
    "        return token_predictions, resistance_predictions, cls_tokens \n",
    "    \n",
    "    def exclude_networks(self, inclusion_list: list):\n",
    "        indices_to_freeze = [i for i in range(len(self.BC)) if i not in inclusion_list]\n",
    "        for i, network in enumerate(self.BC):\n",
    "            if i in indices_to_freeze:\n",
    "                for param in network.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def reset_exclusion(self):\n",
    "        for network in self.BC:\n",
    "            for param in network.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def pretrain_freezing(self):\n",
    "        for network in self.BC:\n",
    "            for param in network.parameters():\n",
    "                param.requires_grad = False\n",
    "        print(f\"Parallell networks are frozen\")\n",
    "    \n",
    "    def finetune_unfreezeing(self):\n",
    "        for network in self.BC:\n",
    "            for param in network.parameters():\n",
    "                param.requires_grad = True\n",
    "        print(f\"Parallell networks are trainable\")\n",
    "\n",
    "class BC_Ab(nn.Module): \n",
    "    def __init__(self, emb_dim: int, hidden_dim: int):\n",
    "        super(BC_Ab, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, 1), # binary classification (S:0 | R:1)\n",
    "        )\n",
    "           \n",
    "    def forward(self, X):\n",
    "        # X is the CLS token of the BERT model\n",
    "        return self.classifier(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
