{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from itertools import chain \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lokalt\n",
    "data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saga\n",
    "data_dir = \"/home/aeerik/data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget config file\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "max_length = 51\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 32\n",
    "drop_prob = 0.2\n",
    "\n",
    "#Encoder\n",
    "enc_dim_inp = 32 \n",
    "enc_dim_out = 32 \n",
    "attention_heads = 8 \n",
    "\n",
    "#BERT\n",
    "num_encoders = 2\n",
    "\n",
    "#trainer\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "stop_patience = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>location</th>\n",
       "      <th>genes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>USA</td>\n",
       "      <td>[aadA1, sul1, tet(A)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>USA</td>\n",
       "      <td>[sul2, aph(3'')-Ib, tet(A), aph(6)-Id]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT, pmrB_Y358N=POINT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>[cyaA_S352T=POINT, uhpT_E350Q=POINT, glpT_E448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1979</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year location                                              genes\n",
       "1  [PAD]      USA                              [aadA1, sul1, tet(A)]\n",
       "4  [PAD]      USA             [sul2, aph(3'')-Ib, tet(A), aph(6)-Id]\n",
       "5  [PAD]      USA               [glpT_E448K=POINT, pmrB_Y358N=POINT]\n",
       "6  [PAD]   Sweden  [cyaA_S352T=POINT, uhpT_E350Q=POINT, glpT_E448...\n",
       "7   1979      USA                                 [glpT_E448K=POINT]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_preprocessing import data_loader\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "NCBI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1208\n"
     ]
    }
   ],
   "source": [
    "from build_vocabulary import make_vocabulary\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import NCBIDataset\n",
    "from build_vocabulary import make_vocabulary\n",
    "from data_preprocessing import data_loader\n",
    "\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "\n",
    "max_length = 51\n",
    "mask_prob = 0.50\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -1,  -1,  -1, 216, 217,  -1, 215,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "         -1,  -1,  -1,  -1,  -1,  -1])\n",
      "tensor([False, False, False,  True,  True, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([216, 217, 215])\n"
     ]
    }
   ],
   "source": [
    "print(test_set[1][1])\n",
    "token_mask = test_set[1][1] != -1\n",
    "print(token_mask)\n",
    "\n",
    "masked_idx = torch.masked_select(test_set[1][1], token_mask)\n",
    "print(masked_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "copy_dataset = Subset(test_set, range(len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "range indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m copy_dataset[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcopy_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmasked\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\utils\\data\\dataset.py:356\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m]\n",
      "\u001b[1;31mTypeError\u001b[0m: range indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "copy_dataset[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class JointEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, vocab_size, max_length, drop_prob):\n",
    "        super(JointEmbedding, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.drop_prob = drop_prob\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.token_emb = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\t\n",
    "        self.norm = nn.LayerNorm(self.embedding_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        token_embedding = self.token_emb(input_tensor)\n",
    "        token_embedding = self.norm(token_embedding)\n",
    "        token_embedding = self.dropout(token_embedding)\n",
    "\n",
    "        return token_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0570,  0.4458,  0.2012,  ..., -3.8613,  0.1622,  0.9274],\n",
       "        [-0.6363,  2.4782,  1.9085,  ..., -0.0000, -3.4922, -0.0000],\n",
       "        [-0.0058, -0.0000, -1.1447,  ...,  1.5867, -0.0000,  2.2315],\n",
       "        ...,\n",
       "        [-0.6363,  2.4782,  0.0000,  ..., -1.4776, -3.4922, -1.0516],\n",
       "        [-0.0000,  2.4782,  0.0000,  ..., -1.4776, -3.4922, -0.0000],\n",
       "        [-0.0000,  2.4782,  1.9085,  ..., -1.4776, -3.4922, -1.0516]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embedding import JointEmbedding\n",
    "embedding_dim = 32\n",
    "voca_size = len(vocabulary)\n",
    "max_length = 20\n",
    "drop_prob = 0.2\n",
    "\n",
    "emb_test = JointEmbedding(embedding_dim, voca_size, max_length, drop_prob)\n",
    "emb_test.forward(test_set[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out, drop_prob):\n",
    "        super(AttentionHead, self).__init__()\n",
    "\n",
    "        self.dim_inp = dim_inp\n",
    "        self.drop_prob = drop_prob\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.q = nn.Linear(dim_inp, dim_out)\n",
    "        self.k = nn.Linear(dim_inp, dim_out)\n",
    "        self.v = nn.Linear(dim_inp, dim_out)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)\n",
    "\n",
    "        scale = query.size(1) ** 0.5\n",
    "        scores = torch.matmul(query, key.transpose(-1, -2)) / scale\n",
    "\n",
    "        scores = scores.masked_fill_(attention_mask, -1e9)\n",
    "        attn = f.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, value)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, dim_inp, dim_out,drop_prob):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(dim_inp, dim_out,drop_prob) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(dim_out * num_heads, dim_inp)\n",
    "        self.norm = nn.LayerNorm(dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        s = [head(input_tensor, attention_mask) for head in self.heads]\n",
    "        scores = torch.cat(s, dim=-1)\n",
    "        scores = self.linear(scores)\n",
    "        return self.norm(scores)\n",
    "    \n",
    "attention_test = AttentionHead(32, 32, 0.2)\n",
    "attention_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])\n",
    "\n",
    "multihead_test = MultiHeadAttention(8, 32, 32,0.2)\n",
    "multihead_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "#dropout \n",
    "#num heads\n",
    "#dim in, dim out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out, attention_heads, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.attention_heads = attention_heads\n",
    "        self.dim_inp = dim_inp\n",
    "        self.dim_out = dim_out\n",
    "        \n",
    "        \n",
    "        self.attention = MultiHeadAttention(self.attention_heads, self.dim_inp, self.dim_out, self.dropout_prob)  # batch_size x sentence size x dim_inp\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(self.dim_inp, self.dim_out),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.dim_out, self.dim_inp),\n",
    "            nn.Dropout(self.dropout_prob)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(self.dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        context = self.attention(input_tensor, attention_mask)\n",
    "        res = self.feed_forward(context)\n",
    "        return self.norm(res)\n",
    "\n",
    "encoder_test = Encoder(enc_dim_inp, enc_dim_out, attention_heads, drop_prob)\n",
    "encoder_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, max_length, dim_inp, dim_out, attention_heads, num_encoders, dropout_prob):\n",
    "        super(BERT, self).__init__()\n",
    "        self.attention_heads = attention_heads\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_inp = dim_inp  \n",
    "        self.dim_out = dim_out\n",
    "        self.num_encoders = num_encoders\n",
    "        self.dropout_prob = dropout_prob  \n",
    "\n",
    "        self.embedding = JointEmbedding(self.dim_inp, self.vocab_size, self.max_length, self.dropout_prob)\n",
    "        self.encoders = nn.ModuleList([Encoder(self.dim_inp, self.dim_out, self.attention_heads, self.dropout_prob) for _ in range(self.num_encoders)])\n",
    "\n",
    "        self.token_prediction_layer = nn.Linear(self.dim_inp, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        embedded = self.embedding(input_tensor)\n",
    "        for layer in self.encoders:\n",
    "            embedded = layer(embedded, attention_mask)\n",
    "\n",
    "        token_predictions = self.token_prediction_layer(embedded)\n",
    "        return self.softmax(token_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test = BERT(len(vocabulary), 20, 32, 32, 8, 2, 0.2)\n",
    "bert_test.forward(test_set[1][0], test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_builder import AttentionHead\n",
    "\n",
    "attention_test = AttentionHead(32, 32, 0.2)\n",
    "attention_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_builder import MultiHeadAttention\n",
    "multihead_test = MultiHeadAttention(num_heads=8, dim_inp=32, dim_out=32, drop_prob=0.2)\n",
    "multihead_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_builder import Encoder\n",
    "\n",
    "encoder_test = Encoder(32, 32, 8, 0.2)\n",
    "encoder_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51, 1208])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import BERT\n",
    "\n",
    "bert_test = BERT(vocab_size=len(vocabulary), max_length=51, dim_inp=32, dim_out=32, attention_heads=8, num_encoders=2, dropout_prob=0.2)\n",
    "bert_test.forward(test_set[1][0], test_set[1][2]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from create_dataset import NCBIDataset\n",
    "def get_split_indices(size_to_split, val_share, random_state: int = 42):\n",
    "    indices = np.arange(size_to_split)\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_share = 1 - val_share\n",
    "    \n",
    "    train_size = int(train_share * size_to_split)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "max_length = 20\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class BertTrainer:\n",
    "    def __init__(self, model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, results_dir):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.train_size = len(train_set)\n",
    "        self.val_set = val_set\n",
    "        self.epochs = epochs    \n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = self.train_size // self.batch_size\n",
    "        self.lr = lr\n",
    "        self.weight_decay = 0.01\n",
    "        self.current_epoch  = 0\n",
    "        self.early_stopping_counter = 0\t\n",
    "        self.patience = stop_patience\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index = -1).to(device)\n",
    "\n",
    "        self.device = device\n",
    "        self.results_dir = results_dir\n",
    "\n",
    "\n",
    "    def __call__(self):      \n",
    "        self.val_set.prepare_dataset() \n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            #Training\n",
    "            self.model.train()\n",
    "            self.train_set.prepare_dataset()\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            avg_epoch_loss = self.train(self.current_epoch)\n",
    "            self.losses.append(avg_epoch_loss) \n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min\")\n",
    "            \n",
    "            #Validation\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            val_results = self.evaluate(self.val_loader, self.val_set)\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            self.val_losses.append(val_results[0])  \n",
    "            self.val_accs.append(val_results[1])\n",
    "\n",
    "            criterion = self.stop_early()\n",
    "        print(f\"-=Training completed=-\")\n",
    "        results = {\n",
    "            \"best_epoch\": self.current_epoch,\n",
    "            \"train_losses\": self.losses,\n",
    "            \"val_losses\": self.val_losses,\n",
    "            \"val_accs\": self.val_accs\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _init_result_lists(self):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accs = []\n",
    "    \n",
    "    def stop_early(self):\n",
    "        if self.val_losses[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        reporting_loss = 0\n",
    "        printing_loss = 0\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            input, token_target, attn_mask = batch\n",
    "            \n",
    "            self.optimizer.zero_grad() \n",
    "\n",
    "            tokens = self.model(input, attn_mask) \n",
    "            loss = self.criterion(tokens.transpose(-1, -2), token_target) \n",
    "            \n",
    "            epoch_loss += loss.item() \n",
    "            reporting_loss += loss.item()\n",
    "            printing_loss += loss.item()\n",
    "            \n",
    "            loss.backward() \n",
    "            self.optimizer.step()         \n",
    "        avg_epoch_loss = epoch_loss / self.num_batches\n",
    "        return avg_epoch_loss \n",
    "    \n",
    "    def evaluate(self, loader, dataset):\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0\n",
    "        total_correct = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(loader):\n",
    "                input, token_target, attn_mask = batch\n",
    "                tokens = self.model(input, attn_mask)\n",
    "                loss = self.criterion(tokens.transpose(-1, -2), token_target)\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                token_mask =  token_target != -1\n",
    "                predicted_tokens = tokens.argmax(dim=-1)\n",
    "                token_target = torch.masked_select(token_target, token_mask)\n",
    "                predicted_tokens = torch.masked_select(predicted_tokens, token_mask)\n",
    "                \n",
    "                correct = (predicted_tokens == token_target).sum().item()\n",
    "                total_correct += correct\n",
    "                total_tokens += token_target.numel() \n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(loader)\n",
    "        accuracy = total_correct / total_tokens\n",
    "\n",
    "        return avg_epoch_loss, accuracy\n",
    "    \n",
    "    def _save_model(self, savepath: Path):\n",
    "        torch.save(self.best_model_state, savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        \n",
    "        \n",
    "    def _load_model(self, savepath: Path):\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded, 1000 samples found\n",
      "length  of vocabulary: 1208\n",
      "Epoch 1/5\n",
      "Epoch completed in 0.0 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:03\n",
      "Epoch 2/5\n",
      "Epoch completed in 0.0 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:06\n",
      "Epoch 3/5\n",
      "Epoch completed in 0.0 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:09\n",
      "Epoch 4/5\n",
      "Epoch completed in 0.0 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:12\n",
      "Epoch 5/5\n",
      "Epoch completed in 0.0 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:16\n",
      "Training completed\n",
      "Model saved to c:\\Users\\erika\\Desktop\\Exjobb\\test_model.pth\n"
     ]
    }
   ],
   "source": [
    "from bert_builder import BERT\n",
    "from data_preprocessing import data_loader\n",
    "from build_vocabulary import make_vocabulary\n",
    "from misc import get_split_indices\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "reduced_samples = 1000\n",
    "NCBI = NCBI.head(reduced_samples)\n",
    "print(f\"Data loaded, {len(NCBI)} samples found\")\n",
    "print(f\"length  of vocabulary:\",len(vocabulary))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob)\n",
    "\n",
    "bert_test = BERT(len(vocabulary), max_length, enc_dim_inp, enc_dim_out, attention_heads, num_encoders, drop_prob)\n",
    "\n",
    "test = BertTrainer(bert_test, train_set, val_set, epochs, batch_size, lr, device, stop_patience, data_dir)\n",
    "test()\n",
    "test._save_model('c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\test_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from c:\\Users\\erika\\Desktop\\Exjobb\\test_model.pth\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "test._load_model('c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\test_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
