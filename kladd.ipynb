{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from itertools import chain \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lokalt\n",
    "data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\repo\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stationÃ¤r\n",
    "data_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saga\n",
    "data_dir = \"/home/aeerik/data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget config file\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "max_length = [51,37]\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 32\n",
    "drop_prob = 0.2\n",
    "\n",
    "#Encoder\n",
    "dim_emb = 128\n",
    "dim_hidden = 128\n",
    "attention_heads = 8 \n",
    "\n",
    "#BERT\n",
    "num_encoders = 2\n",
    "\n",
    "#trainer\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "stop_patience = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1208\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "include_pheno = False\n",
    "vocabulary = vocab_geno(NCBI, include_pheno)\n",
    "vocab = vocab_pheno(ab_df)\n",
    "print(len(vocabulary))\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from data_preprocessing import data_loader\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "\n",
    "max_length = [51,37]\n",
    "mask_prob = 0.25\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = deepcopy(NCBI['AST_phenotypes'].tolist())\n",
    "max_seq_len = [max_length[0],max_length[1]]\n",
    "list_idx = []\n",
    "list_SR = []\n",
    "for i in range(len(sequences)):\n",
    "    current_seq = sequences[i]\n",
    "    current_idxs = []\n",
    "    current_SRs = []\n",
    "    for j in range(len(current_seq)):\n",
    "        item = current_seq[j].split('=')\n",
    "        abs = item[0]   \n",
    "        sr = item[1]\n",
    "        current_idxs.append(vocabulary_pheno.lookup_indices([abs]))\n",
    "        for k in range(len(sr)):\n",
    "            if sr == 'R':\n",
    "                current_SRs.append(1)\n",
    "            else:\n",
    "                current_SRs.append(0)\n",
    "\n",
    "    if len(current_idxs) != len(current_SRs):\n",
    "        print(\"current sequence:\",current_seq, \"\\n\", \"with length:\", len(current_seq))\n",
    "        print(\"indexes:\",current_idxs, \"with length:\", len(current_idxs))\n",
    "        print(\"suceptability\",current_SRs, \"with length:\", len(current_SRs))\n",
    "        print('error at', j)\n",
    "        print(\"--------------------\")\n",
    "    current_idxs = [int(item[0]) for item in current_idxs]\n",
    "    #for i in range(0,max_length[1] - len(current_idxs)):\n",
    "    #    current_idxs.append(-1)\n",
    "    #for i in range(0,max_length[1] - len(current_SRs)):\n",
    "    #    current_SRs.append(-1)\n",
    "    list_idx.append(current_idxs)\n",
    "    list_SR.append(current_SRs)\n",
    "for i in range(len(list_idx)):\n",
    "    if len(list_idx[i]) != len(list_SR[i]):\n",
    "        print('error at', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "10\n",
      "10\n",
      "8\n",
      "8\n",
      "11\n",
      "7\n",
      "6\n",
      "9\n",
      "8\n",
      "10\n",
      "6\n",
      "8\n",
      "8\n",
      "7\n",
      "9\n",
      "7\n",
      "6\n",
      "7\n",
      "10\n",
      "5\n",
      "9\n",
      "6\n",
      "6\n",
      "8\n",
      "10\n",
      "6\n",
      "9\n",
      "4\n",
      "6\n",
      "8\n",
      "7\n",
      "6\n",
      "11\n",
      "8\n",
      "10\n",
      "8\n",
      "9\n",
      "7\n",
      "8\n",
      "9\n",
      "7\n",
      "10\n",
      "8\n",
      "9\n",
      "7\n",
      "5\n",
      "6\n",
      "5\n",
      "8\n",
      "7\n",
      "6\n",
      "6\n",
      "5\n",
      "7\n",
      "8\n",
      "7\n",
      "5\n",
      "9\n",
      "9\n",
      "6\n",
      "6\n",
      "12\n",
      "11\n",
      "12\n",
      "9\n",
      "7\n",
      "9\n",
      "8\n",
      "9\n",
      "8\n",
      "7\n",
      "9\n",
      "10\n",
      "10\n",
      "8\n",
      "8\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from bert_builder import BERT_ft\n",
    "\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "for i, batch in enumerate(loader):\n",
    "    if i >= 5:\n",
    "        break \n",
    "    input, token_target, attn_mask, AB_idx, SR_class  = batch\n",
    "    bert_test = BERT_ft(vocab_size=len(vocabulary_geno), max_length=51, dim_embedding = 128, dim_hidden= 128, attention_heads=8, num_encoders=2, dropout_prob=0.2, num_ab=81, device=device)\n",
    "    token_pred, res_pred = bert_test.forward(input, attn_mask)\n",
    "    res_pred = torch.where(res_pred > 0, torch.ones_like(res_pred), torch.zeros_like(res_pred))\n",
    "    list_AB_predictions = []\n",
    "    for i, row in enumerate(res_pred):\n",
    "        AB_list = 0\n",
    "        AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "        current_abs = []\n",
    "        for ab in AB_list:\n",
    "            current_abs.append(row[ab].item())\n",
    "        current_abs = torch.tensor(current_abs)\n",
    "        current_abs = current_abs.type(torch.int16)\n",
    "        list_AB_predictions.append(current_abs)\n",
    "    \n",
    "        processed_tensor = [row[row != -1] for row in SR_class]\n",
    "    for i, row in enumerate(processed_tensor):\n",
    "        correct = (row == list_AB_predictions[i]).sum().item()\n",
    "        print(correct)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mres_pred\u001b[49m[\u001b[38;5;241m14\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(test)\n\u001b[0;32m      3\u001b[0m pred_res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(test \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, torch\u001b[38;5;241m.\u001b[39mones_like(test), torch\u001b[38;5;241m.\u001b[39mzeros_like(test))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res_pred' is not defined"
     ]
    }
   ],
   "source": [
    "test = res_pred[14]\n",
    "print(test)\n",
    "pred_res = torch.where(test > 0, torch.ones_like(test), torch.zeros_like(test))\n",
    "print(pred_res)\n",
    "indicies = [1,2,3,4]\n",
    "test = pred_res[indicies]\n",
    "print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from create_dataset import NCBIDataset\n",
    "def get_split_indices(size_to_split, val_share, random_state: int = 42):\n",
    "    indices = np.arange(size_to_split)\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_share = 1 - val_share\n",
    "    \n",
    "    train_size = int(train_share * size_to_split)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "max_length = 20\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "class BertTrainer_ft:\n",
    "    def __init__(self, model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, project_name, wandb_name):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.train_size = len(train_set)\n",
    "        self.val_set = val_set\n",
    "        self.epochs = epochs    \n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = self.train_size // self.batch_size\n",
    "        self.lr = lr\n",
    "        self.weight_decay = 0.01\n",
    "        self.current_epoch  = 0\n",
    "        self.early_stopping_counter = 0\t\n",
    "        self.patience = stop_patience\n",
    "        \n",
    "        self.wandb_mode = wandb_mode\n",
    "        self.project_name = project_name\n",
    "        self.wandb_name = wandb_name\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.token_criterion = nn.CrossEntropyLoss(ignore_index = -1).to(device)\n",
    "        self.ab_criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def __call__(self):   \n",
    "        if self.wandb_mode:\n",
    "            self._init_wandb()   \n",
    "        self.val_set.prepare_dataset() \n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            #Training\n",
    "            self.model.train()\n",
    "            self.train_set.prepare_dataset()\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            avg_epoch_loss_total, avg_epoch_loss_geno, avg_epoch_loss_ab = self.train(self.current_epoch)\n",
    "            self.train_losses_total.append(avg_epoch_loss_total)\n",
    "            self.train_losses_geno.append(avg_epoch_loss_geno) \n",
    "            self.train_losses_ab.append(avg_epoch_loss_ab)  \n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min\")\n",
    "            \n",
    "            #Validation\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            val_results = self.evaluate(self.val_loader)\n",
    "            print(val_results)\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            self.val_losses_total.append(val_results[0])  \n",
    "            self.val_losses_geno.append(val_results[1])\n",
    "            self.val_losses_ab.append(val_results[2])\n",
    "            self.val_accs.append(val_results[3])\n",
    "            if self.wandb_mode:\n",
    "                self._report_epoch_results()\n",
    "            criterion = self.stop_early()\n",
    "            if criterion:\n",
    "                print(f\"Training interrupted at epoch: {self.current_epoch+1}\")\n",
    "                break\n",
    "        print(f\"-=Training completed=-\")\n",
    "        results = {\n",
    "            \"best_epoch\": self.best_epoch,\n",
    "            \"total_train_losses\": self.train_losses_total,\n",
    "            \"geno_train_losses\": self.train_losses_geno,\n",
    "            \"ab_train_losses\": self.train_losses_ab,\n",
    "            \"total_val_losses\": self.val_losses_total,\n",
    "            \"geno_val_losses\": self.val_losses_geno,\n",
    "            \"ab_val_losses\": self.val_losses_ab,\n",
    "            \"val_accs\": self.val_accs\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _init_result_lists(self):\n",
    "\n",
    "        self.train_losses_total = []\n",
    "        self.train_losses_geno = []\n",
    "        self.train_losses_ab = []\n",
    "\n",
    "        self.val_losses_total = []\n",
    "        self.val_losses_geno = []\n",
    "        self.val_losses_ab = []\n",
    "\n",
    "        self.val_accs = []\n",
    "    \n",
    "    def stop_early(self):\n",
    "        if self.val_losses_total[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses_total[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss_total = 0\n",
    "        epoch_loss_geno = 0\n",
    "        epoch_loss_ab = 0\n",
    "\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "            ABinclusion = torch.unique(AB_idx)\n",
    "            ABinclusion = ABinclusion[ABinclusion != -1]\n",
    "            ABinclusion = ABinclusion.tolist()\n",
    "            self.model.exclude_networks(ABinclusion)\n",
    "\n",
    "            self.optimizer.zero_grad() \n",
    "\n",
    "            token_predictions, resistance_predictions = self.model(input, attn_mask) \n",
    "            geno_loss = self.token_criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "            \n",
    "            list_AB_predictions = []\n",
    "            for i, row in enumerate(resistance_predictions):\n",
    "                AB_list = 0\n",
    "                AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                current_abs = []\n",
    "                for ab in AB_list:\n",
    "                    current_abs.append(row[ab].item())\n",
    "                current_abs = torch.tensor(current_abs)\n",
    "                list_AB_predictions.append(current_abs)\n",
    "            \n",
    "            processed_tensor = [row[row != -1] for row in SR_class]\n",
    "            for i, row in enumerate(processed_tensor):\n",
    "                row = row.type(torch.float32)\n",
    "                ab_loss =+ self.ab_criterion(list_AB_predictions[i], row)\n",
    "\n",
    "            avg_total_loss = geno_loss.item() + ab_loss.item() /2\n",
    "            epoch_loss_total += avg_total_loss \n",
    "            epoch_loss_geno += geno_loss.item()\n",
    "            epoch_loss_ab += ab_loss.item()\n",
    "            avg_total_loss = torch.tensor(avg_total_loss, requires_grad=True)\n",
    "            avg_total_loss.backward() \n",
    "            self.optimizer.step()\n",
    "            self.model.reset_exclusion()   \n",
    "              \n",
    "        avg_epoch_loss_total = epoch_loss_total / self.num_batches\n",
    "        avg_epoch_loss_geno = epoch_loss_geno / self.num_batches\n",
    "        avg_epoch_loss_ab = epoch_loss_ab / self.num_batches\n",
    "\n",
    "        return avg_epoch_loss_total, avg_epoch_loss_geno, avg_epoch_loss_ab\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        epoch_loss_total = 0\n",
    "        epoch_loss_geno =0\n",
    "        epoch_loss_ab = 0\n",
    "        total_correct = 0\n",
    "        total_sum = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(loader):\n",
    "                input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "\n",
    "                token_predictions, resistance_predictions = self.model(input, attn_mask) \n",
    "                geno_loss = self.token_criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "                \n",
    "                list_AB_predictions = []\n",
    "                for i, row in enumerate(resistance_predictions):\n",
    "                    AB_list = 0\n",
    "                    AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                    current_abs = []\n",
    "                    for ab in AB_list:\n",
    "                        current_abs.append(row[ab].item())\n",
    "                    current_abs = torch.tensor(current_abs)\n",
    "                    list_AB_predictions.append(current_abs)\n",
    "                \n",
    "                processed_tensor = [row[row != -1] for row in SR_class]\n",
    "                for i, row in enumerate(processed_tensor):\n",
    "                    row = row.type(torch.float32)\n",
    "                    ab_loss =+ self.ab_criterion(list_AB_predictions[i], row)\n",
    "                \n",
    "                avg_total_loss = geno_loss.item() + ab_loss.item() /2\n",
    "                epoch_loss_total += avg_total_loss \n",
    "                epoch_loss_geno += geno_loss.item()\n",
    "                epoch_loss_ab += ab_loss.item() \n",
    "                \n",
    "                list_AB_predictions = []\n",
    "                for i, row in enumerate(resistance_predictions):\n",
    "                    AB_list = 0\n",
    "                    AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                    current_abs = []\n",
    "                    for ab in AB_list:\n",
    "                        current_abs.append(row[ab].item())\n",
    "                    current_abs = torch.tensor(current_abs)\n",
    "                    current_abs = current_abs.type(torch.int16)\n",
    "                    list_AB_predictions.append(current_abs)\n",
    "                \n",
    "                    processed_tensor = [row[row != -1] for row in SR_class]\n",
    "                for i, row in enumerate(processed_tensor):\n",
    "                    total_correct += (row == list_AB_predictions[i]).sum().item()\n",
    "                    total_sum += len(row)\n",
    "\n",
    "        avg_epoch_loss_total = epoch_loss_total / self.num_batches\n",
    "        avg_epoch_loss_geno = epoch_loss_geno / self.num_batches\n",
    "        avg_epoch_loss_ab = epoch_loss_ab / self.num_batches\n",
    "\n",
    "        accuracy = total_correct / total_sum\n",
    "\n",
    "        return avg_epoch_loss_total, avg_epoch_loss_geno, avg_epoch_loss_ab, accuracy\n",
    "    \n",
    "    def _save_model(self, savepath: Path):\n",
    "        torch.save(self.best_model_state, savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        \n",
    "        \n",
    "    def _load_model(self, savepath: Path):\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "    def _init_wandb(self):\n",
    "        self.wandb_run = wandb.init(\n",
    "            project=self.project_name, # name of the project\n",
    "            name=self.wandb_name, # name of the run\n",
    "            \n",
    "            config={\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"num_heads\": self.model.attention_heads,\n",
    "                \"num_encoders\": self.model.num_encoders,\n",
    "                \"emb_dim\": self.model.dim_embedding,\n",
    "                'ff_dim': self.model.dim_embedding,\n",
    "                \"lr\": self.lr,\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "                \"max_seq_len\": self.model.max_length[0],\n",
    "                \"vocab_size\": len(self.train_set.vocab_geno),\n",
    "                \"num_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "            }\n",
    "        )\n",
    "        self.wandb_run.watch(self.model) # watch the model for gradients and parameters\n",
    "        self.wandb_run.define_metric(\"epoch\", hidden=True)\n",
    "        self.wandb_run.define_metric(\"batch\", hidden=True)\n",
    "\n",
    "        self.wandb_run.define_metric(\"TotalLosses/total_train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"TotalLosses/total_val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "        self.wandb_run.define_metric(\"GenoLosses/geno_train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"GenoLosses/geno_val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "        self.wandb_run.define_metric(\"AB_Losses/ab_train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"AB_Losses/ab_val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "\n",
    "        self.wandb_run.define_metric(\"Accuracies/val_acc\", summary=\"min\", step_metric=\"epoch\")\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/final_val_loss\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_acc\")\n",
    "        self.wandb_run.define_metric(\"final_epoch\")\n",
    "\n",
    "        return self.wandb_run\n",
    "    \n",
    "    def _report_epoch_results(self):\n",
    "        wandb_dict = {\n",
    "            \"epoch\": self.current_epoch+1,\n",
    "            \"TotalLosses/total_train_loss\": self.train_losses_total[-1],\n",
    "            \"GenoLosses/geno_train_loss\": self.train_losses_geno[-1],\n",
    "            \"ABLosses/ab_train_loss\": self.train_losses_ab[-1],\n",
    "\n",
    "            \"TotalLosses/total_val_loss\": self.val_losses_total[-1],\n",
    "            \"GenoLosses/geno_val_loss\": self.val_losses_geno[-1],\n",
    "            \"ABLosses/ab_val_loss\": self.val_losses_ab[-1],\n",
    "            \n",
    "            \"Accuracies/val_acc\": self.val_accs[-1],\n",
    "        }\n",
    "        self.wandb_run.log(wandb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "\n",
    "max_length = [51,37]\n",
    "mask_prob = 0.25\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded, 1000 samples found\n",
      "length  of token vocabulary: 472\n",
      "Epoch 1/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "(1.8301471590995788, 1.722143726348877, 0.2160068655014038, 0.624)\n",
      "Elapsed time: 00:00:04\n",
      "Epoch 2/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "(1.8301471590995788, 1.722143726348877, 0.2160068655014038, 0.624)\n",
      "Elapsed time: 00:00:09\n",
      "Epoch 3/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "(1.8301471590995788, 1.722143726348877, 0.2160068655014038, 0.624)\n",
      "Elapsed time: 00:00:13\n",
      "Epoch 4/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "(1.8301471590995788, 1.722143726348877, 0.2160068655014038, 0.624)\n",
      "Elapsed time: 00:00:19\n",
      "Epoch 5/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "(1.8301471590995788, 1.722143726348877, 0.2160068655014038, 0.624)\n",
      "Elapsed time: 00:00:24\n",
      "-=Training completed=-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 0,\n",
       " 'total_train_losses': [6.556279458999634,\n",
       "  6.592458580136299,\n",
       "  6.586696702837944,\n",
       "  6.581953485012054,\n",
       "  6.596720513105392],\n",
       " 'geno_train_losses': [6.20914400100708,\n",
       "  6.214506969451905,\n",
       "  6.219129676818848,\n",
       "  6.216217441558838,\n",
       "  6.235301609039307],\n",
       " 'ab_train_losses': [0.6942709159851074,\n",
       "  0.7559032213687896,\n",
       "  0.7351340520381927,\n",
       "  0.7314720869064331,\n",
       "  0.7228378081321716],\n",
       " 'total_val_losses': [1.8301471590995788,\n",
       "  1.8301471590995788,\n",
       "  1.8301471590995788,\n",
       "  1.8301471590995788,\n",
       "  1.8301471590995788],\n",
       " 'geno_val_losses': [1.722143726348877,\n",
       "  1.722143726348877,\n",
       "  1.722143726348877,\n",
       "  1.722143726348877,\n",
       "  1.722143726348877],\n",
       " 'ab_val_losses': [0.2160068655014038,\n",
       "  0.2160068655014038,\n",
       "  0.2160068655014038,\n",
       "  0.2160068655014038,\n",
       "  0.2160068655014038],\n",
       " 'val_accs': [0.624, 0.624, 0.624, 0.624, 0.624]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import BERT_ft\n",
    "from data_preprocessing import data_loader\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from misc import get_split_indices\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "include_pheno = True\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "reduced_samples = 1000\n",
    "NCBI = NCBI.head(reduced_samples)\n",
    "\n",
    "print(f\"Data loaded, {len(NCBI)} samples found\")\n",
    "print(f\"length  of token vocabulary:\",len(vocabulary_geno))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "\n",
    "bert_test = BERT_ft(len(vocabulary_geno), max_length, dim_emb, dim_hidden, attention_heads, num_encoders, drop_prob, len(vocabulary_pheno), device)\n",
    "\n",
    "test = BertTrainer_ft(bert_test, train_set, val_set, epochs, batch_size, lr, device, stop_patience, False, \"NCBI\", \"test\")\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
