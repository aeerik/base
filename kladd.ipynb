{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from itertools import chain \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lokalt\n",
    "data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\repo\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stationär\n",
    "data_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\data'\n",
    "ab_dir = 'c:\\\\Users\\\\erikw\\\\Desktop\\\\Exjobb kod\\\\base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saga\n",
    "data_dir = \"/home/aeerik/data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget config file\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "max_length = [51,37]\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 32\n",
    "drop_prob = 0.2\n",
    "\n",
    "#Encoder\n",
    "enc_dim_inp = 32 \n",
    "enc_dim_out = 32 \n",
    "attention_heads = 8 \n",
    "\n",
    "#BERT\n",
    "num_encoders = 2\n",
    "\n",
    "#trainer\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "stop_patience = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1208\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "include_pheno = False\n",
    "vocabulary = vocab_geno(NCBI, include_pheno)\n",
    "vocab = vocab_pheno(ab_df)\n",
    "print(len(vocabulary))\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from data_preprocessing import data_loader\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "\n",
    "data_path = data_dir\n",
    "ab_path = ab_dir\n",
    "\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_path,ab_path)\n",
    "\n",
    "max_length = [51,37]\n",
    "mask_prob = 0.25\n",
    "vocabulary_geno = vocab_geno(NCBI, include_pheno)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42,  2, 72, 69, 45, 54, 22, 66, 29, 78, 16, 30, 25,  5,  3, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1])\n",
      "[42, 2, 72, 69, 45, 54, 22, 66, 29, 78, 16, 30, 25, 5, 3]\n",
      "tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "test_tensor = test_set[5][3]\n",
    "print(test_tensor)\n",
    "test_tensor = test_tensor[test_tensor != -1]\n",
    "print(  test_tensor.tolist()  )\n",
    "\n",
    "selected = pred_res[test_tensor.tolist() ]\n",
    "print(selected)\n",
    "expected = test_set[5][4]\n",
    "expected = expected[expected != -1] \n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7668377161026001\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from bert_builder import BERT_ft\n",
    "\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "for i, batch in enumerate(loader):\n",
    "    if i >= 1:\n",
    "        break \n",
    "    input, token_target, attn_mask, AB_idx, SR_class  = batch\n",
    "    bert_test = BERT_ft(vocab_size=len(vocabulary_geno), max_length=51, dim_embedding = 128, dim_hidden= 128, attention_heads=8, num_encoders=2, dropout_prob=0.2, num_ab=81, device=device)\n",
    "    token_pred, res_pred = bert_test.forward(input, attn_mask)\n",
    "    #print(res_pred)\n",
    "    list_AB_predictions = []\n",
    "    for i, row in enumerate(res_pred):\n",
    "        #print(row)\n",
    "        AB_list = 0\n",
    "        AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "        current_abs = []\n",
    "        for ab in AB_list:\n",
    "            current_abs.append(row[ab].item())\n",
    "        current_abs = torch.tensor(current_abs)\n",
    "        list_AB_predictions.append(current_abs)\n",
    "    \n",
    "    processed_tensor = [row[row != -1] for row in SR_class]\n",
    "    for i, row in enumerate(processed_tensor):\n",
    "        row = row.type(torch.float32)\n",
    "        loss =+ loss_function(list_AB_predictions[i], row)\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3424,  0.9205, -1.0999,  0.0847,  0.5822,  0.7236,  0.3780,  0.7510,\n",
      "         0.1406,  0.4057, -0.5458, -0.5373,  0.5959,  0.6605, -0.8000,  0.0453,\n",
      "         1.0489,  0.5322,  1.2369, -0.8379, -0.2384,  0.3600, -0.0849, -0.0365,\n",
      "        -0.7926,  0.5737, -0.3591, -0.1816, -0.2066, -0.1260,  0.3713,  0.4501,\n",
      "        -0.2184, -1.7874, -0.8306, -0.2229,  0.5001, -0.0674, -0.4564, -0.1346,\n",
      "        -0.1485,  0.0665, -1.0599, -0.5795,  0.1665, -0.2995, -0.7904, -0.1770,\n",
      "        -0.6905,  0.3542, -0.3265,  0.3615, -0.3309, -0.3364,  1.0064, -1.3413,\n",
      "         0.0523,  0.0020,  0.4103,  0.8045,  1.0091, -0.4322, -0.6960,  0.1552,\n",
      "        -0.4795, -1.2025, -0.8506, -0.0682,  0.1343,  0.5795,  0.2522, -0.0998,\n",
      "        -0.8485,  0.1468, -0.0776,  0.1436,  0.2697, -0.1420, -0.3930,  0.5552,\n",
      "        -0.0430], grad_fn=<SelectBackward0>)\n",
      "tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 1., 0.])\n",
      "tensor([1., 0., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "test = res_pred[14]\n",
    "print(test)\n",
    "pred_res = torch.where(test > 0, torch.ones_like(test), torch.zeros_like(test))\n",
    "print(pred_res)\n",
    "indicies = [1,2,3,4]\n",
    "test = pred_res[indicies]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from create_dataset import NCBIDataset\n",
    "def get_split_indices(size_to_split, val_share, random_state: int = 42):\n",
    "    indices = np.arange(size_to_split)\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_share = 1 - val_share\n",
    "    \n",
    "    train_size = int(train_share * size_to_split)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "max_length = 20\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "class BertTrainer_ft:\n",
    "    def __init__(self, model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, project_name, wandb_name):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.train_size = len(train_set)\n",
    "        self.val_set = val_set\n",
    "        self.epochs = epochs    \n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = self.train_size // self.batch_size\n",
    "        self.lr = lr\n",
    "        self.weight_decay = 0.01\n",
    "        self.current_epoch  = 0\n",
    "        self.early_stopping_counter = 0\t\n",
    "        self.patience = stop_patience\n",
    "        \n",
    "        \n",
    "        self.wandb_mode = wandb_mode\n",
    "        self.project_name = project_name\n",
    "        self.wandb_name = wandb_name\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.token_criterion = nn.CrossEntropyLoss(ignore_index = -1).to(device)\n",
    "        self.ab_criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def __call__(self):   \n",
    "        if self.wandb_mode:\n",
    "            self._init_wandb()   \n",
    "        self.val_set.prepare_dataset() \n",
    "        self.val_loader = DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "        start_time = time.time()\n",
    "        self.best_val_loss = float('inf')\n",
    "        self._init_result_lists()\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            #Training\n",
    "            self.model.train()\n",
    "            self.train_set.prepare_dataset()\n",
    "            self.train_loader = DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_start_time = time.time()\n",
    "            avg_epoch_loss = self.train(self.current_epoch)\n",
    "            self.train_losses.append(avg_epoch_loss) \n",
    "            print(f\"Epoch completed in {(time.time() - epoch_start_time)/60:.1f} min\")\n",
    "            \n",
    "            #Validation\n",
    "            print(\"Evaluating on validation set...\")\n",
    "            val_results = self.evaluate(self.val_loader)\n",
    "            print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}\")\n",
    "            self.val_losses.append(val_results[0])  \n",
    "            self.val_accs.append(val_results[1])\n",
    "            if self.wandb_mode:\n",
    "                self._report_epoch_results()\n",
    "            self._report_epoch_results()\n",
    "            criterion = self.stop_early()\n",
    "            if criterion:\n",
    "                print(f\"Training interrupted at epoch: {self.current_epoch+1}\")\n",
    "                break\n",
    "\n",
    "        print(f\"-=Training completed=-\")\n",
    "        results = {\n",
    "            \"best_epoch\": self.current_epoch,\n",
    "            \"train_losses\": self.train_losses,\n",
    "            \"val_losses\": self.val_losses,\n",
    "            \"val_accs\": self.val_accs\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _init_result_lists(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accs = []\n",
    "    \n",
    "    def stop_early(self):\n",
    "        if self.val_losses[-1] < self.best_val_loss:\n",
    "            self.best_val_loss = self.val_losses[-1]\n",
    "            self.best_epoch = self.current_epoch\n",
    "            self.best_model_state = self.model.state_dict()\n",
    "            self.early_stopping_counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.early_stopping_counter += 1\n",
    "            return True if self.early_stopping_counter >= self.patience else False\n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "        time_ref = time.time()\n",
    "        \n",
    "        epoch_loss_total = 0\n",
    "        epoch_loss_geno = 0\n",
    "        epoch_loss_ab = 0\n",
    "\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            input, token_target, attn_mask, AB_idx, SR_class = batch\n",
    "\n",
    "            ABinclusion = torch.unique(AB_idx)\n",
    "            ABinclusion = ABinclusion[ABinclusion != -1]\n",
    "            ABinclusion = ABinclusion.tolist()\n",
    "            self.model.exclude_networks(ABinclusion)\n",
    "\n",
    "            self.optimizer.zero_grad() \n",
    "\n",
    "            token_predictions, resistance_predictions = self.model(input, attn_mask) \n",
    "            geno_loss = self.token_criterion(token_predictions.transpose(-1, -2), token_target) \n",
    "            \n",
    "            list_AB_predictions = []\n",
    "            for i, row in enumerate(resistance_predictions):\n",
    "                AB_list = 0\n",
    "                AB_list = [elem.item() for elem in AB_idx[i] if elem.item() != -1]\n",
    "                current_abs = []\n",
    "                for ab in AB_list:\n",
    "                    current_abs.append(row[ab].item())\n",
    "                current_abs = torch.tensor(current_abs)\n",
    "                list_AB_predictions.append(current_abs)\n",
    "            \n",
    "                processed_tensor = [row[row != -1] for row in SR_class]\n",
    "                for i, row in enumerate(processed_tensor):\n",
    "                    row = row.type(torch.float32)\n",
    "                    ab_loss =+ self.ab_criterion(list_AB_predictions[i], row)\n",
    "            \n",
    "            avg_total_loss = geno_loss.item() + ab_loss.item()\n",
    "            epoch_loss_total += avg_total_loss \n",
    "            epoch_loss_geno += geno_loss.item()\n",
    "            epoch_loss_ab += ab_loss.item()\n",
    "        \n",
    "            loss.backward() \n",
    "            self.optimizer.step()\n",
    "            self.model.reset_exclusion()         \n",
    "        avg_epoch_loss_total = epoch_loss_total / self.num_batches\n",
    "        avg_epoch_loss_geno = epoch_loss_geno / self.num_batches\n",
    "        avg_epoch_loss_ab = epoch_loss_ab / self.num_batches\n",
    "\n",
    "        return avg_epoch_loss_total, avg_epoch_loss_geno, avg_epoch_loss_ab\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0\n",
    "        total_correct = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(loader):\n",
    "                input, token_target, attn_mask = batch\n",
    "                tokens = self.model(input, attn_mask)\n",
    "                loss = self.criterion(tokens.transpose(-1, -2), token_target)\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                token_mask =  token_target != -1\n",
    "                predicted_tokens = tokens.argmax(dim=-1)\n",
    "                token_target = torch.masked_select(token_target, token_mask)\n",
    "                predicted_tokens = torch.masked_select(predicted_tokens, token_mask)\n",
    "                \n",
    "                correct = (predicted_tokens == token_target).sum().item()\n",
    "                total_correct += correct\n",
    "                total_tokens += token_target.numel() \n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(loader)\n",
    "        accuracy = total_correct / total_tokens\n",
    "\n",
    "        return avg_epoch_loss, accuracy\n",
    "    \n",
    "    def _save_model(self, savepath: Path):\n",
    "        torch.save(self.best_model_state, savepath)\n",
    "        print(f\"Model saved to {savepath}\")\n",
    "        \n",
    "        \n",
    "    def _load_model(self, savepath: Path):\n",
    "        print(f\"Loading model from {savepath}\")\n",
    "        self.model.load_state_dict(torch.load(savepath))\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "    def _init_wandb(self):\n",
    "        self.wandb_run = wandb.init(\n",
    "            project=self.project_name, # name of the project\n",
    "            name=self.wandb_name, # name of the run\n",
    "            \n",
    "            config={\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"num_heads\": self.model.attention_heads,\n",
    "                \"num_encoders\": self.model.num_encoders,\n",
    "                \"emb_dim\": self.model.dim_inp,\n",
    "                'ff_dim': self.model.dim_out,\n",
    "                \"lr\": self.lr,\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "                \"max_seq_len\": self.model.max_length[0],\n",
    "                \"vocab_size\": len(self.train_set.vocab),\n",
    "                \"num_parameters\": sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
    "            }\n",
    "        )\n",
    "        self.wandb_run.watch(self.model) # watch the model for gradients and parameters\n",
    "        self.wandb_run.define_metric(\"epoch\", hidden=True)\n",
    "        self.wandb_run.define_metric(\"batch\", hidden=True)\n",
    "\n",
    "        self.wandb_run.define_metric(\"Losses/train_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        self.wandb_run.define_metric(\"Losses/val_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "        \n",
    "        self.wandb_run.define_metric(\"Losses/final_val_loss\")\n",
    "        self.wandb_run.define_metric(\"Accuracies/final_val_acc\")\n",
    "        self.wandb_run.define_metric(\"final_epoch\")\n",
    "\n",
    "        return self.wandb_run\n",
    "    \n",
    "    def _report_epoch_results(self):\n",
    "        wandb_dict = {\n",
    "            \"epoch\": self.current_epoch+1,\n",
    "            \"Losses/train_loss\": self.train_losses[-1],\n",
    "            \"Losses/val_loss\": self.val_losses[-1],\n",
    "            \"Accuracies/val_acc\": self.val_accs[-1],\n",
    "        }\n",
    "        self.wandb_run.log(wandb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded, 1000 samples found\n",
      "length  of vocabulary: 1208\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6acd82vc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd26e33968a74d24841fde3606e1961d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.015 MB uploaded\\r'), FloatProgress(value=0.059422213793539416, max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test</strong> at: <a href='https://wandb.ai/strompfel/NCBI/runs/6acd82vc' target=\"_blank\">https://wandb.ai/strompfel/NCBI/runs/6acd82vc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240212_163013-6acd82vc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6acd82vc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0ad681bf1042a5b8ef185b01e30240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\erika\\Desktop\\Exjobb\\data\\wandb\\run-20240212_163246-r6tal2xq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/strompfel/NCBI/runs/r6tal2xq' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/strompfel/NCBI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/strompfel/NCBI' target=\"_blank\">https://wandb.ai/strompfel/NCBI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/strompfel/NCBI/runs/r6tal2xq' target=\"_blank\">https://wandb.ai/strompfel/NCBI/runs/r6tal2xq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:06\n",
      "Epoch 2/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:12\n",
      "Epoch 3/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:18\n",
      "Epoch 4/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:24\n",
      "Epoch 5/5\n",
      "Epoch completed in 0.1 min\n",
      "Evaluating on validation set...\n",
      "Elapsed time: 00:00:31\n",
      "-=Training completed=-\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 4,\n",
       " 'train_losses': [6.716255130767823,\n",
       "  5.805294799804687,\n",
       "  5.029448509216309,\n",
       "  4.463122625350952,\n",
       "  4.1071098518371585],\n",
       " 'val_losses': [6.11516353062221,\n",
       "  5.36531925201416,\n",
       "  4.712193965911865,\n",
       "  4.294982944216047,\n",
       "  4.067011458533151],\n",
       " 'val_accs': [0.24260355029585798,\n",
       "  0.23668639053254437,\n",
       "  0.1893491124260355,\n",
       "  0.1952662721893491,\n",
       "  0.2485207100591716]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import BERT\n",
    "from data_preprocessing import data_loader\n",
    "from build_vocabulary import make_vocabulary\n",
    "from misc import get_split_indices\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "include_pheno = False\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "vocabulary = make_vocabulary(NCBI, include_pheno)\n",
    "reduced_samples = 1000\n",
    "NCBI = NCBI.head(reduced_samples)\n",
    "print(f\"Data loaded, {len(NCBI)} samples found\")\n",
    "print(f\"length  of vocabulary:\",len(vocabulary))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob,include_pheno)\n",
    "\n",
    "bert_test = BERT(len(vocabulary), max_length, enc_dim_inp, enc_dim_out, attention_heads, num_encoders, drop_prob)\n",
    "\n",
    "test = BertTrainer(bert_test, train_set, val_set, epochs, batch_size, lr, device, stop_patience, True, \"NCBI\", \"test\")\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
