{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from itertools import chain \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lokalt\n",
    "data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saga\n",
    "data_dir = \"/home/aeerik/data/raw/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>location</th>\n",
       "      <th>genes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>USA</td>\n",
       "      <td>[tet(A), sul1, aadA1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>USA</td>\n",
       "      <td>[aph(6)-Id, sul2, tet(A), aph(3'')-Ib]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT, pmrB_Y358N=POINT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>[glpT_E448K=POINT, uhpT_E350Q=POINT, cyaA_S352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1979</td>\n",
       "      <td>USA</td>\n",
       "      <td>[glpT_E448K=POINT]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year location                                              genes\n",
       "1  [PAD]      USA                              [tet(A), sul1, aadA1]\n",
       "4  [PAD]      USA             [aph(6)-Id, sul2, tet(A), aph(3'')-Ib]\n",
       "5  [PAD]      USA               [glpT_E448K=POINT, pmrB_Y358N=POINT]\n",
       "6  [PAD]   Sweden  [glpT_E448K=POINT, uhpT_E350Q=POINT, cyaA_S352...\n",
       "7   1979      USA                                 [glpT_E448K=POINT]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_preprocessing import data_loader\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "NCBI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1208\n"
     ]
    }
   ],
   "source": [
    "from build_vocabulary import make_vocabulary\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import NCBIDataset\n",
    "from build_vocabulary import make_vocabulary\n",
    "from data_preprocessing import data_loader\n",
    "\n",
    "include_pheno = False\n",
    "threshold_year = 1970\n",
    "path = data_dir\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year,path)\n",
    "\n",
    "max_length = 20\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class JointEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, vocab_size, max_length, drop_prob):\n",
    "        super(JointEmbedding, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.drop_prob = drop_prob\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.token_emb = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\t\n",
    "        self.norm = nn.LayerNorm(self.embedding_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        token_embedding = self.token_emb(input_tensor)\n",
    "        token_embedding = self.norm(token_embedding)\n",
    "        token_embedding = self.dropout(token_embedding)\n",
    "\n",
    "        return token_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8331,  1.0515, -0.0000,  0.2314,  1.3775, -2.2667,  0.5048,  1.0534,\n",
       "         -0.0000,  0.0000, -0.9512, -0.0132,  0.0000, -1.1990, -0.0790, -0.6929,\n",
       "          1.5763,  1.6299, -1.0645, -0.7928, -1.1193, -0.2490,  0.3520,  0.2263,\n",
       "          0.9369,  3.2176, -0.0000,  1.3471, -3.2385, -0.2307,  0.4862, -0.9063],\n",
       "        [ 1.1834, -2.1201, -0.1374,  0.7954, -0.2633,  1.0959, -1.5578, -0.4190,\n",
       "         -0.0000, -0.0855, -2.3608,  0.5345, -0.4668,  0.6931, -0.9211, -0.0000,\n",
       "         -0.0000,  0.0000,  3.0867, -0.2822,  0.1557,  0.0000,  0.0966,  0.0080,\n",
       "          0.0000, -1.0138,  0.5358,  0.4295,  0.0000,  1.3010,  0.0628, -0.0000],\n",
       "        [-1.9641,  0.8865,  1.8891,  0.6134, -1.6141, -2.5473,  2.6708, -0.0801,\n",
       "          0.0842,  0.0000, -0.8039, -1.1668,  0.2151, -1.6891,  1.9464,  1.2084,\n",
       "          0.4367,  0.4825, -0.9501,  0.0000,  1.2927,  0.3222, -1.6566,  0.1560,\n",
       "         -0.1427, -0.3243, -1.9109,  0.0000, -0.0779, -1.3170,  1.2141,  0.4513],\n",
       "        [-1.1707,  1.2274,  0.7131,  0.3360, -0.7185,  0.1766,  1.8820, -0.0076,\n",
       "          0.0000,  0.0128, -2.8652,  0.0810, -0.0501, -1.4993, -0.0000, -1.3633,\n",
       "          0.0409, -0.0000, -1.6595,  0.1310,  0.5975,  0.6227,  0.1616,  0.0000,\n",
       "         -2.2218, -0.0794,  1.3658,  1.8048,  1.0863,  0.3941,  0.0000,  0.0618],\n",
       "        [-1.1707,  1.2274,  0.7131,  0.3360, -0.7185,  0.1766,  1.8820, -0.0076,\n",
       "          2.7385,  0.0000, -2.8652,  0.0810, -0.0501, -1.4993, -0.3248, -1.3633,\n",
       "          0.0000, -2.5912, -1.6595,  0.1310,  0.0000,  0.6227,  0.0000,  0.0000,\n",
       "         -2.2218, -0.0794,  1.3658,  1.8048,  1.0863,  0.3941,  0.2474,  0.0000],\n",
       "        [ 0.7131, -0.0079, -0.1535, -0.1755,  0.4239, -0.0000, -2.9278,  1.3096,\n",
       "          0.3426, -0.9067,  1.5262,  1.1821, -0.8459, -0.7055, -0.0000, -0.0000,\n",
       "          2.9593,  0.0000, -2.0413, -0.4553,  0.2485, -0.0000, -0.9926, -0.0000,\n",
       "         -0.7006,  1.7229,  1.1965,  0.0000, -0.6714, -0.3049, -0.3207,  0.0000],\n",
       "        [-0.0000,  1.2274,  0.7131,  0.3360, -0.7185,  0.1766,  1.8820, -0.0076,\n",
       "          2.7385,  0.0128, -2.8652,  0.0000, -0.0501, -1.4993, -0.3248, -1.3633,\n",
       "          0.0409, -2.5912, -1.6595,  0.1310,  0.5975,  0.0000,  0.1616,  0.8704,\n",
       "         -2.2218, -0.0000,  1.3658,  1.8048,  1.0863,  0.3941,  0.2474,  0.0618],\n",
       "        [ 1.1834, -2.1201, -0.1374,  0.0000, -0.2633,  1.0959, -1.5578, -0.4190,\n",
       "         -1.4857, -0.0855, -0.0000,  0.5345, -0.4668,  0.0000, -0.0000, -1.6332,\n",
       "         -2.1625,  0.8661,  0.0000, -0.2822,  0.0000,  2.5386,  0.0000,  0.0080,\n",
       "          1.0719, -1.0138,  0.5358,  0.0000,  1.0433,  1.3010,  0.0628, -0.0000],\n",
       "        [ 0.0000, -2.1201, -0.1374,  0.7954, -0.2633,  1.0959, -1.5578, -0.4190,\n",
       "         -1.4857, -0.0855, -2.3608,  0.5345, -0.4668,  0.0000, -0.9211, -1.6332,\n",
       "         -2.1625,  0.8661,  3.0867, -0.2822,  0.1557,  2.5386,  0.0966,  0.0000,\n",
       "          1.0719, -1.0138,  0.5358,  0.4295,  1.0433,  1.3010,  0.0628, -0.5891],\n",
       "        [ 1.1834, -2.1201, -0.1374,  0.7954, -0.2633,  1.0959, -1.5578, -0.4190,\n",
       "         -1.4857, -0.0855, -2.3608,  0.5345, -0.4668,  0.6931, -0.9211, -1.6332,\n",
       "         -2.1625,  0.8661,  3.0867, -0.0000,  0.0000,  2.5386,  0.0966,  0.0080,\n",
       "          1.0719, -1.0138,  0.5358,  0.4295,  0.0000,  1.3010,  0.0628, -0.5891],\n",
       "        [ 1.1834, -2.1201, -0.1374,  0.0000, -0.2633,  0.0000, -1.5578, -0.4190,\n",
       "         -1.4857, -0.0855, -2.3608,  0.5345, -0.0000,  0.0000, -0.9211, -1.6332,\n",
       "         -2.1625,  0.8661,  3.0867, -0.2822,  0.1557,  2.5386,  0.0000,  0.0080,\n",
       "          1.0719, -1.0138,  0.5358,  0.0000,  1.0433,  1.3010,  0.0000, -0.5891],\n",
       "        [ 1.1834, -2.1201, -0.1374,  0.7954, -0.2633,  1.0959, -1.5578, -0.4190,\n",
       "         -1.4857, -0.0855, -2.3608,  0.5345, -0.0000,  0.0000, -0.9211, -1.6332,\n",
       "         -2.1625,  0.8661,  3.0867, -0.2822,  0.1557,  2.5386,  0.0966,  0.0080,\n",
       "          1.0719, -0.0000,  0.0000,  0.4295,  1.0433,  1.3010,  0.0000, -0.5891],\n",
       "        [ 1.1834, -2.1201, -0.1374,  0.7954, -0.2633,  0.0000, -1.5578, -0.0000,\n",
       "         -0.0000, -0.0855, -2.3608,  0.5345, -0.0000,  0.0000, -0.9211, -1.6332,\n",
       "         -2.1625,  0.8661,  3.0867, -0.0000,  0.0000,  2.5386,  0.0966,  0.0080,\n",
       "          1.0719, -1.0138,  0.5358,  0.0000,  1.0433,  0.0000,  0.0628, -0.5891],\n",
       "        [ 1.1834, -2.1201, -0.1374,  0.7954, -0.2633,  1.0959, -1.5578, -0.4190,\n",
       "         -1.4857, -0.0855, -0.0000,  0.5345, -0.4668,  0.6931, -0.9211, -1.6332,\n",
       "         -0.0000,  0.0000,  3.0867, -0.2822,  0.0000,  2.5386,  0.0966,  0.0080,\n",
       "          1.0719, -0.0000,  0.5358,  0.4295,  1.0433,  1.3010,  0.0000, -0.5891],\n",
       "        [ 0.0000, -2.1201, -0.0000,  0.7954, -0.2633,  1.0959, -1.5578, -0.4190,\n",
       "         -0.0000, -0.0855, -2.3608,  0.5345, -0.4668,  0.6931, -0.0000, -0.0000,\n",
       "         -2.1625,  0.0000,  3.0867, -0.2822,  0.1557,  2.5386,  0.0966,  0.0080,\n",
       "          0.0000, -1.0138,  0.5358,  0.4295,  1.0433,  1.3010,  0.0628, -0.0000],\n",
       "        [ 1.1834, -2.1201, -0.1374,  0.7954, -0.2633,  1.0959, -1.5578, -0.4190,\n",
       "         -1.4857, -0.0000, -2.3608,  0.5345, -0.4668,  0.6931, -0.9211, -1.6332,\n",
       "         -2.1625,  0.8661,  3.0867, -0.2822,  0.1557,  2.5386,  0.0000,  0.0080,\n",
       "          1.0719, -1.0138,  0.0000,  0.0000,  1.0433,  1.3010,  0.0628, -0.0000],\n",
       "        [ 1.1834, -2.1201, -0.0000,  0.7954, -0.0000,  0.0000, -1.5578, -0.4190,\n",
       "         -1.4857, -0.0000, -0.0000,  0.0000, -0.4668,  0.6931, -0.9211, -0.0000,\n",
       "         -2.1625,  0.8661,  3.0867, -0.2822,  0.1557,  2.5386,  0.0000,  0.0080,\n",
       "          1.0719, -1.0138,  0.5358,  0.4295,  1.0433,  0.0000,  0.0000, -0.5891],\n",
       "        [ 1.1834, -2.1201, -0.1374,  0.7954, -0.2633,  0.0000, -1.5578, -0.0000,\n",
       "         -1.4857, -0.0855, -2.3608,  0.5345, -0.4668,  0.6931, -0.9211, -1.6332,\n",
       "         -2.1625,  0.8661,  3.0867, -0.2822,  0.1557,  2.5386,  0.0966,  0.0000,\n",
       "          1.0719, -0.0000,  0.0000,  0.4295,  1.0433,  1.3010,  0.0628, -0.5891],\n",
       "        [ 0.0000, -2.1201, -0.1374,  0.7954, -0.2633,  1.0959, -1.5578, -0.0000,\n",
       "         -1.4857, -0.0855, -2.3608,  0.5345, -0.4668,  0.6931, -0.9211, -1.6332,\n",
       "         -2.1625,  0.8661,  3.0867, -0.2822,  0.1557,  2.5386,  0.0000,  0.0080,\n",
       "          1.0719, -1.0138,  0.5358,  0.4295,  1.0433,  1.3010,  0.0628, -0.0000],\n",
       "        [ 0.0000, -2.1201, -0.0000,  0.7954, -0.0000,  1.0959, -1.5578, -0.0000,\n",
       "         -1.4857, -0.0000, -2.3608,  0.5345, -0.4668,  0.0000, -0.9211, -1.6332,\n",
       "         -2.1625,  0.8661,  3.0867, -0.2822,  0.1557,  0.0000,  0.0966,  0.0080,\n",
       "          1.0719, -1.0138,  0.5358,  0.0000,  1.0433,  1.3010,  0.0628, -0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embedding import JointEmbedding\n",
    "embedding_dim = 32\n",
    "voca_size = len(vocabulary)\n",
    "max_length = 20\n",
    "drop_prob = 0.2\n",
    "\n",
    "emb_test = JointEmbedding(embedding_dim, voca_size, max_length, drop_prob)\n",
    "emb_test.forward(test_set[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,  59, 213, 214, 215,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.2529e-02,  6.6882e-01,  1.2920e+00,  1.0694e+00, -7.1454e-02,\n",
       "          6.7509e-01, -2.0224e-02, -1.4736e+00, -2.0233e+00,  2.4986e-01,\n",
       "          2.4865e-01, -6.7741e-01,  9.8667e-01, -1.6409e+00, -9.8899e-01,\n",
       "          9.9393e-01,  2.1556e-01,  1.6299e+00, -7.0088e-01,  1.1783e+00,\n",
       "         -6.0879e-01, -1.0770e+00,  4.7027e-01,  1.0028e+00, -2.2437e-02,\n",
       "         -6.6476e-01,  1.0250e-01, -2.1046e+00,  5.9789e-01,  1.6473e+00,\n",
       "         -6.6898e-01, -2.2295e-01],\n",
       "        [-1.4151e-01,  1.1083e+00,  9.1247e-01,  1.3594e+00, -4.5214e-01,\n",
       "          5.3718e-01,  1.4342e-01, -9.5685e-01, -1.1647e+00,  5.0426e-01,\n",
       "          1.0839e+00, -1.4875e-01,  7.5383e-01, -1.1176e+00, -6.3895e-01,\n",
       "         -1.0583e-02,  7.7586e-01,  2.3473e+00, -8.6620e-01,  7.3578e-01,\n",
       "         -6.8109e-01, -1.5421e+00,  2.6042e-02,  7.8927e-01, -6.3132e-01,\n",
       "         -1.3392e+00, -1.5855e-03, -2.4988e+00,  3.9480e-01,  1.3509e+00,\n",
       "         -5.6054e-01, -7.0681e-02],\n",
       "        [-2.8954e-01,  2.0942e-01,  1.3887e+00,  1.9750e+00, -1.2974e-01,\n",
       "          1.0255e+00, -3.6718e-02, -8.8047e-01, -1.2205e+00, -1.5546e-01,\n",
       "          6.2904e-01, -6.8565e-01,  8.2731e-01, -1.0846e+00, -7.4178e-01,\n",
       "          3.0882e-01,  4.2244e-01,  1.4916e+00, -6.5450e-01,  1.1027e+00,\n",
       "         -3.8517e-01, -1.4114e+00, -6.1578e-02,  6.7990e-01, -1.2003e+00,\n",
       "         -5.7336e-01,  7.3116e-02, -2.7679e+00,  6.9696e-01,  1.3804e+00,\n",
       "         -6.0826e-01,  6.7593e-01],\n",
       "        [ 1.1795e-01,  7.3133e-01,  1.1675e+00,  1.3437e+00, -3.8598e-02,\n",
       "          6.6444e-01, -1.1734e-01, -1.1678e+00, -7.8057e-01, -2.2082e-01,\n",
       "          7.6115e-01, -6.2051e-01,  6.8475e-01, -1.5358e+00, -9.6211e-01,\n",
       "          1.1732e+00, -1.5553e-01,  1.3050e+00, -8.2314e-01,  1.5209e+00,\n",
       "         -6.0056e-01, -2.2005e+00, -3.6886e-01,  9.6079e-01, -4.2739e-01,\n",
       "         -7.8932e-01,  6.0651e-02, -1.9503e+00,  5.5802e-01,  1.9066e+00,\n",
       "         -3.6718e-01,  1.7023e-01],\n",
       "        [-5.0556e-01,  1.3961e-01,  6.9856e-01,  1.6242e+00,  8.8017e-02,\n",
       "          1.0105e+00, -8.5848e-02, -1.4803e+00, -1.2218e+00,  3.5538e-01,\n",
       "          3.4174e-02, -6.3059e-01,  8.2694e-01, -1.2416e+00, -4.8268e-01,\n",
       "          8.7477e-01,  1.4567e-02,  1.6481e+00, -1.1037e+00,  1.2700e+00,\n",
       "         -4.5421e-01, -1.3238e+00, -5.6692e-01,  1.5694e+00, -7.9720e-01,\n",
       "         -7.0495e-01,  2.6884e-01, -2.0992e+00,  1.1872e+00,  1.5596e+00,\n",
       "         -6.0302e-01,  1.3144e-01],\n",
       "        [ 2.9265e-01,  1.3961e+00,  5.3208e-01,  4.2609e-01,  5.1922e-02,\n",
       "          1.1505e+00, -1.2235e-01, -1.4507e+00, -1.4149e+00, -4.3265e-01,\n",
       "          9.6613e-02, -7.8611e-01,  9.9504e-01, -1.8091e+00, -9.9808e-01,\n",
       "          4.9158e-01,  7.5555e-01,  1.7483e+00, -1.0692e+00,  1.6955e+00,\n",
       "         -8.6647e-01, -1.0442e+00,  1.1274e+00,  1.2182e+00,  5.8283e-01,\n",
       "         -4.9720e-02, -3.6455e-01, -1.7349e+00, -1.0045e+00,  9.5028e-01,\n",
       "         -6.0174e-02, -3.0299e-01],\n",
       "        [-4.2333e-02,  5.2094e-02,  1.6587e+00,  1.8704e+00,  5.8009e-02,\n",
       "          7.4952e-01, -5.8788e-02, -9.1575e-01, -7.7820e-01,  1.5263e-01,\n",
       "          4.1227e-01, -9.9688e-01,  8.7757e-01, -1.6389e+00, -8.6438e-01,\n",
       "          6.9940e-01,  5.9882e-01,  1.7735e+00, -3.2251e-01,  1.0843e+00,\n",
       "         -5.0869e-01, -1.1676e+00,  3.6425e-01,  4.4759e-01, -1.2881e+00,\n",
       "         -5.5248e-01, -3.5103e-01, -2.6033e+00,  7.2864e-01,  1.1356e+00,\n",
       "         -6.4771e-01,  7.3461e-02],\n",
       "        [ 5.3437e-01,  5.6239e-01,  8.6262e-01,  9.4925e-01,  3.7502e-01,\n",
       "          5.3241e-01, -6.2688e-01, -5.4478e-01, -6.4874e-01, -4.8965e-01,\n",
       "          1.3962e+00, -1.7419e+00,  8.8975e-01, -2.0555e+00, -3.5224e-01,\n",
       "          9.8650e-01,  1.4383e+00,  1.8381e+00, -5.6585e-01,  1.7806e+00,\n",
       "         -1.1403e+00, -1.0499e+00,  1.9123e-01, -3.3893e-02, -1.3398e+00,\n",
       "         -7.8113e-01, -7.9303e-01, -1.4140e+00,  4.0068e-01,  8.9462e-02,\n",
       "          2.9321e-01,  4.5754e-01],\n",
       "        [-1.9687e-01,  1.1653e+00,  1.1561e+00,  1.4184e+00, -6.4889e-01,\n",
       "          6.8773e-01, -6.5720e-02, -6.4560e-01, -5.7255e-01,  5.8711e-01,\n",
       "          8.3175e-01, -4.5824e-01,  1.0461e+00, -1.3969e+00, -5.2252e-01,\n",
       "          2.5696e-01,  2.5943e-01,  2.0527e+00, -6.0345e-01,  1.3115e+00,\n",
       "         -6.3869e-01, -1.4665e+00,  4.2578e-01,  3.7954e-01, -9.2159e-01,\n",
       "         -1.7213e+00, -5.3778e-01, -2.5424e+00,  2.6626e-01,  9.4036e-01,\n",
       "         -2.3581e-01,  3.8992e-01],\n",
       "        [-1.8832e-01,  9.5847e-01,  9.0326e-01,  1.7361e+00, -6.4096e-01,\n",
       "          7.8094e-01, -5.0577e-01, -1.2473e+00, -1.0644e+00, -1.0242e-01,\n",
       "          7.6759e-01, -6.1590e-01,  1.1980e+00, -8.9774e-01, -8.5079e-01,\n",
       "         -1.3181e-01,  4.3589e-01,  1.9074e+00, -3.2954e-01,  1.1036e+00,\n",
       "         -4.5186e-01, -1.5424e+00, -7.2353e-01,  7.8221e-01, -4.9871e-01,\n",
       "         -1.0041e+00,  3.1833e-01, -2.4230e+00,  5.6586e-01,  1.6468e+00,\n",
       "         -2.1143e-01,  3.2550e-01],\n",
       "        [-4.7627e-01,  1.6680e+00,  1.0458e+00,  1.2840e+00, -6.9403e-01,\n",
       "          1.0263e+00,  1.2786e-01, -1.1338e+00, -1.1189e+00,  4.6268e-01,\n",
       "          7.9235e-01, -5.7580e-01,  1.2028e+00, -1.0116e+00, -4.2790e-01,\n",
       "          2.6363e-01,  1.2622e-01,  2.0602e+00, -1.1713e+00,  9.3687e-01,\n",
       "         -4.4573e-01, -1.6531e+00,  4.8418e-01,  3.4241e-01, -1.1119e-01,\n",
       "         -1.3724e+00, -2.7043e-01, -2.2874e+00, -1.6666e-01,  1.1043e+00,\n",
       "         -9.6975e-02,  8.5576e-02],\n",
       "        [-1.1257e-01,  1.1830e+00,  1.1743e+00,  1.8552e+00, -4.2896e-01,\n",
       "          4.5908e-01, -2.3654e-01, -1.1994e+00, -8.7200e-01, -2.1619e-02,\n",
       "          1.0927e+00, -1.5286e+00,  1.0770e+00, -1.2464e+00, -2.7460e-01,\n",
       "          5.8081e-01,  3.6422e-01,  1.7384e+00, -4.1003e-01,  1.5022e+00,\n",
       "         -6.7925e-01, -1.4006e+00,  2.9122e-01,  3.0661e-01, -7.4072e-01,\n",
       "         -1.2568e+00, -5.8206e-01, -2.1309e+00,  3.2063e-01,  8.0612e-01,\n",
       "         -2.8752e-01,  6.5692e-01],\n",
       "        [-5.7495e-01,  1.2059e+00,  1.2748e+00,  1.7104e+00, -2.6495e-01,\n",
       "          6.9724e-01, -3.1030e-01, -4.7878e-01, -7.8201e-01,  2.7596e-01,\n",
       "          9.1860e-01, -1.2320e+00,  1.0603e+00, -1.4262e+00, -2.2427e-01,\n",
       "          8.0607e-01,  5.1383e-01,  1.8491e+00, -6.2431e-01,  1.0483e+00,\n",
       "         -6.6608e-01, -1.1568e+00,  6.5248e-01, -1.9539e-02, -1.1337e+00,\n",
       "         -1.5065e+00, -5.5605e-01, -2.2487e+00,  1.1401e-01,  7.8483e-01,\n",
       "         -5.2343e-01,  8.1685e-01],\n",
       "        [-3.7375e-02,  7.5257e-01,  1.1797e+00,  1.6071e+00, -2.8515e-01,\n",
       "          7.0944e-01, -3.2539e-01, -8.5654e-01, -7.0560e-01, -1.2864e-01,\n",
       "          6.9295e-01, -1.0698e+00,  1.7633e+00, -1.5267e+00, -6.2230e-01,\n",
       "          5.6099e-01,  1.5731e-01,  1.5556e+00, -2.4633e-01,  1.8950e+00,\n",
       "         -7.4831e-01, -1.4880e+00,  2.0128e-01,  2.5286e-01, -8.1808e-01,\n",
       "         -8.6482e-01, -4.9147e-01, -2.4016e+00,  4.5860e-01,  1.0268e+00,\n",
       "         -2.7446e-01,  7.7104e-02],\n",
       "        [-4.5710e-01,  5.1119e-01,  2.0900e+00,  1.6754e+00, -1.8293e-01,\n",
       "          2.0224e-01,  4.1480e-01, -7.9246e-01, -1.3886e+00,  4.4108e-02,\n",
       "          1.2321e+00, -4.4013e-01,  1.0467e+00, -7.9910e-01, -6.1115e-01,\n",
       "          3.3680e-01, -3.2942e-01,  1.3388e+00,  2.4347e-02,  6.5967e-01,\n",
       "         -2.9412e-01, -1.6337e+00,  4.4029e-01,  3.6657e-06, -5.3464e-01,\n",
       "         -1.3272e+00, -1.8878e-01, -2.6609e+00,  8.3829e-01,  1.4811e+00,\n",
       "         -4.6090e-01, -2.3464e-01],\n",
       "        [-1.3052e-01,  6.5571e-01,  1.3505e+00,  1.5458e+00, -5.4937e-02,\n",
       "          5.3164e-01, -6.8170e-02, -7.4417e-01, -1.5426e+00, -2.7739e-02,\n",
       "          9.6667e-01, -9.5427e-01,  9.8478e-01, -1.7792e+00, -2.6499e-02,\n",
       "          5.7098e-01,  2.8889e-01,  1.6553e+00, -7.7060e-01,  1.3909e+00,\n",
       "         -6.3426e-01, -1.2836e+00,  6.6288e-01,  6.9720e-01, -7.8733e-01,\n",
       "         -9.1687e-01, -2.9383e-01, -2.4945e+00,  5.3560e-01,  1.1032e+00,\n",
       "         -4.5132e-01,  2.0237e-02],\n",
       "        [ 4.6307e-01,  6.2678e-01,  3.0264e-01,  1.4585e+00, -2.3979e-01,\n",
       "          6.8542e-01, -9.1744e-01, -6.9940e-01, -1.7084e+00,  3.8980e-01,\n",
       "          7.3741e-01, -1.2055e+00,  9.4678e-01, -1.6812e+00, -6.8222e-02,\n",
       "          5.3303e-01,  5.3068e-01,  2.2279e+00, -8.4209e-01,  1.7010e+00,\n",
       "         -7.7107e-01, -5.3435e-01,  3.0754e-01,  4.0080e-01, -1.0373e+00,\n",
       "         -1.5112e+00, -4.0723e-01, -1.6130e+00,  6.8118e-01,  1.1925e+00,\n",
       "         -3.1620e-01,  3.6732e-01],\n",
       "        [ 2.9132e-01,  5.4859e-01,  7.6279e-01,  1.1322e+00,  7.7832e-02,\n",
       "          4.3468e-01, -2.4345e-01, -6.6961e-01, -8.6247e-01, -9.9131e-02,\n",
       "          1.1428e+00, -1.0814e+00,  1.0304e+00, -1.6966e+00, -8.2885e-02,\n",
       "          2.3105e-01,  1.0244e+00,  2.1715e+00, -6.7341e-01,  1.7973e+00,\n",
       "         -9.1891e-01, -1.3720e+00,  6.0310e-01,  2.2687e-01, -1.3665e+00,\n",
       "         -1.1442e+00, -6.2055e-01, -2.0552e+00,  4.5402e-01,  1.1048e+00,\n",
       "         -7.9782e-02, -6.7550e-02],\n",
       "        [-2.5311e-02,  1.1034e+00,  1.2879e+00,  1.3083e+00, -5.4185e-01,\n",
       "          5.1385e-01,  5.8932e-02, -8.6459e-01, -1.2244e+00, -3.9089e-02,\n",
       "          7.5559e-01, -1.3038e+00,  1.3271e+00, -1.4899e+00, -5.0448e-01,\n",
       "          3.7934e-01,  3.0382e-01,  1.8004e+00, -6.1650e-01,  1.3919e+00,\n",
       "         -4.9294e-01, -1.4447e+00,  7.2334e-01,  2.9940e-01, -6.2174e-01,\n",
       "         -1.3483e+00, -2.8566e-01, -2.3476e+00,  2.7637e-01,  1.0553e+00,\n",
       "         -3.7017e-02,  6.0310e-01],\n",
       "        [ 3.9674e-01,  7.2932e-01,  1.1445e+00,  1.1403e+00, -1.8012e-01,\n",
       "          6.5988e-01,  2.9482e-02, -7.4375e-01, -1.0697e+00,  2.4430e-01,\n",
       "          1.4727e+00, -1.1134e-01,  6.7313e-01, -1.3967e+00, -5.6500e-01,\n",
       "         -2.6362e-01,  2.3827e-01,  1.8780e+00, -6.6399e-01,  1.3508e+00,\n",
       "         -7.7680e-01, -1.8172e+00,  3.8938e-01,  8.4406e-01, -7.0486e-01,\n",
       "         -1.6812e+00, -4.0235e-01, -2.2474e+00,  5.3260e-01,  1.3292e+00,\n",
       "         -5.2802e-01,  9.9535e-02]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out, drop_prob):\n",
    "        super(AttentionHead, self).__init__()\n",
    "\n",
    "        self.dim_inp = dim_inp\n",
    "        self.drop_prob = drop_prob\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.q = nn.Linear(dim_inp, dim_out)\n",
    "        self.k = nn.Linear(dim_inp, dim_out)\n",
    "        self.v = nn.Linear(dim_inp, dim_out)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)\n",
    "\n",
    "        scale = query.size(1) ** 0.5\n",
    "        scores = torch.matmul(query, key.transpose(-1, -2)) / scale\n",
    "\n",
    "        scores = scores.masked_fill_(attention_mask, -1e9)\n",
    "        attn = f.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, value)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, dim_inp, dim_out,drop_prob):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(dim_inp, dim_out,drop_prob) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(dim_out * num_heads, dim_inp)\n",
    "        self.norm = nn.LayerNorm(dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        s = [head(input_tensor, attention_mask) for head in self.heads]\n",
    "        scores = torch.cat(s, dim=-1)\n",
    "        scores = self.linear(scores)\n",
    "        return self.norm(scores)\n",
    "    \n",
    "attention_test = AttentionHead(32, 32, 0.2)\n",
    "attention_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])\n",
    "\n",
    "multihead_test = MultiHeadAttention(8, 32, 32,0.2)\n",
    "multihead_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.1706e-01, -6.7473e-02,  5.0605e-01, -9.7665e-02,  8.3774e-01,\n",
       "         -6.7473e-02, -1.4544e+00, -3.3227e-01,  1.5943e+00,  1.1752e+00,\n",
       "         -6.7473e-02,  1.2708e-01, -1.1516e+00, -6.7473e-02, -6.7473e-02,\n",
       "          7.5421e-01,  9.5889e-01, -8.1097e-01, -6.7473e-02, -6.7473e-02,\n",
       "          1.4775e+00, -3.3237e+00, -1.0164e+00, -6.7473e-02, -6.6237e-01,\n",
       "          7.5044e-01, -6.7473e-02, -3.7708e-01,  2.1856e+00, -9.8690e-01,\n",
       "         -6.7473e-02,  1.0412e-01],\n",
       "        [ 3.5822e-01, -7.5941e-02,  4.7088e-02,  7.7096e-02,  1.6481e+00,\n",
       "          1.6597e+00, -3.4294e-01, -7.8363e-01,  1.2021e+00,  4.4501e-01,\n",
       "          1.2442e-01,  5.1303e-02,  7.0817e-01, -1.8316e+00,  9.5101e-01,\n",
       "          7.7096e-02,  9.8989e-01,  5.2216e-01,  8.0485e-01, -1.4087e+00,\n",
       "          1.1965e+00, -2.3475e+00, -8.9523e-01, -1.2553e+00, -1.0449e+00,\n",
       "         -7.6363e-01,  7.7096e-02, -1.0664e+00,  1.5044e+00,  7.7096e-02,\n",
       "         -9.3702e-01,  2.3154e-01],\n",
       "        [ 6.9456e-01,  3.4231e-01, -1.4980e-01,  9.6629e-01,  2.8604e-01,\n",
       "         -1.4980e-01, -2.1237e+00,  7.3758e-01, -1.5636e-01,  6.5762e-01,\n",
       "          1.2030e+00,  3.7672e-01, -6.9698e-01,  1.5838e-01,  3.0733e-03,\n",
       "          1.1881e+00,  3.5615e-01, -1.4922e+00, -3.6855e-01, -8.4868e-01,\n",
       "          1.0929e+00, -1.8684e+00, -1.4606e+00, -1.3826e-02,  1.8716e-01,\n",
       "          1.2663e+00,  8.7720e-01, -4.3969e-01,  1.5497e+00, -1.4980e-01,\n",
       "         -2.4626e+00,  4.3781e-01],\n",
       "        [ 8.1912e-01,  2.9578e-02,  9.7413e-01,  3.8741e-01,  2.9578e-02,\n",
       "          2.9578e-02, -4.1764e-01,  2.9578e-02,  2.9578e-02,  6.9802e-01,\n",
       "          5.1732e-01, -2.7398e-01,  6.8559e-01, -8.2179e-01,  2.9578e-02,\n",
       "          1.8909e+00,  3.3079e-01, -4.7807e-01,  1.1010e+00, -9.8834e-01,\n",
       "          1.5212e+00, -2.8949e+00, -1.9352e+00,  2.5232e-01, -4.4685e-01,\n",
       "          9.7699e-02,  1.5510e+00,  3.2575e-01,  2.9578e-02, -1.3255e+00,\n",
       "         -1.7275e+00, -4.9591e-02],\n",
       "        [ 7.2460e-01, -1.2881e-01, -1.2881e-01, -1.8425e-01,  2.7240e-01,\n",
       "          6.7264e-01, -4.4864e-01, -1.2881e-01,  1.7211e+00,  1.2236e+00,\n",
       "          1.6007e-01, -6.9117e-02, -1.2881e-01, -1.6335e+00,  1.2915e+00,\n",
       "          7.6317e-01, -3.1247e-03, -1.2881e-01,  1.1890e+00, -1.2173e+00,\n",
       "          1.4343e+00, -2.7815e+00, -1.3630e+00, -6.4862e-01, -8.6720e-01,\n",
       "         -1.2881e-01, -1.2881e-01, -1.2249e-02,  1.7324e+00, -1.2881e-01,\n",
       "         -1.4670e+00,  5.4134e-01],\n",
       "        [ 3.6523e-01,  2.4134e-01, -4.9774e-02, -6.3009e-01,  9.6137e-01,\n",
       "          1.3267e+00, -1.4562e+00, -4.9774e-02,  1.1794e+00,  1.1559e+00,\n",
       "          8.5758e-01, -4.9774e-02, -4.7963e-01, -7.8904e-01,  9.3671e-01,\n",
       "          7.8331e-01,  4.8263e-01,  3.5032e-02,  7.4267e-01, -2.1687e+00,\n",
       "          1.1353e+00, -1.2898e+00, -5.0036e-01, -3.9986e-01,  2.3676e-01,\n",
       "         -4.5321e-01,  1.4734e+00, -5.1905e-01,  1.0101e+00, -1.8497e+00,\n",
       "         -2.1887e+00, -4.9774e-02],\n",
       "        [ 7.9751e-01,  1.5592e-01, -2.1102e-01,  1.1560e-02,  6.7382e-01,\n",
       "          1.5592e-01, -6.3024e-01, -8.2603e-01,  2.2975e+00,  3.9838e-01,\n",
       "          3.1666e-01, -6.9443e-02,  6.0780e-01,  1.5592e-01,  9.1583e-01,\n",
       "          8.7576e-01,  1.5592e-01,  2.2141e-01,  1.5592e-01, -1.0672e+00,\n",
       "          1.5592e-01, -3.2020e+00,  1.5592e-01, -8.6192e-01, -1.1582e+00,\n",
       "         -5.2550e-01,  1.8791e+00,  1.5592e-01,  1.5592e-01,  1.5592e-01,\n",
       "         -2.1589e+00,  1.5592e-01],\n",
       "        [ 3.0719e-01, -3.8172e-01,  1.9381e+00, -6.2545e-01,  1.8282e+00,\n",
       "          1.2353e+00, -1.3244e+00, -1.8789e+00, -2.5377e-02,  1.6484e-01,\n",
       "         -2.5377e-02,  2.7604e-01,  3.1705e-01,  3.4548e-01,  1.2862e+00,\n",
       "          1.1075e+00,  2.7207e-01, -5.6103e-01, -2.5377e-02, -1.0422e+00,\n",
       "         -4.5528e-01, -7.2938e-01, -2.5377e-02, -2.5377e-02, -1.3658e-01,\n",
       "         -2.5377e-02,  1.9611e+00, -2.5377e-02,  1.0643e-01, -1.2317e+00,\n",
       "         -2.5759e+00, -2.5377e-02],\n",
       "        [ 1.3612e-01, -9.0383e-01,  1.3612e-01, -1.1304e+00,  1.3612e-01,\n",
       "          1.8092e+00, -7.2562e-01,  1.3612e-01,  1.3142e+00,  8.5812e-01,\n",
       "          1.3612e-01,  5.2087e-01,  1.3305e+00,  1.3612e-01,  1.3612e-01,\n",
       "          5.2001e-01,  1.9669e-01,  1.1483e+00,  1.8757e-01,  1.3612e-01,\n",
       "          9.0335e-01,  1.3612e-01, -7.1713e-01,  1.3612e-01, -1.2117e+00,\n",
       "          1.3612e-01,  1.3612e-01, -1.8559e+00,  1.1012e+00, -2.9030e+00,\n",
       "         -1.9510e+00, -1.2487e-01],\n",
       "        [ 2.3260e-01, -5.6324e-01,  4.4605e-01,  6.0127e-01,  1.1362e+00,\n",
       "          2.0545e+00, -1.1599e+00,  2.6017e-02,  1.4099e+00,  5.6200e-01,\n",
       "          4.7717e-01,  2.0116e-01,  4.7642e-01, -1.4804e+00,  3.4138e-02,\n",
       "          2.2951e-01,  1.8407e-01, -6.7111e-01,  2.6314e-01, -1.5830e+00,\n",
       "          1.5586e+00, -1.8075e+00, -9.8643e-01, -1.1814e-01, -4.8079e-01,\n",
       "          3.3828e-02,  1.1392e+00,  2.6017e-02,  1.2754e+00, -1.6011e+00,\n",
       "         -1.9228e+00,  7.3405e-03],\n",
       "        [ 4.7375e-01, -1.1777e+00, -2.0567e-01, -4.0215e-01,  7.5286e-01,\n",
       "          1.3394e+00, -8.9045e-01, -2.0567e-01,  1.6411e+00,  8.6142e-01,\n",
       "          4.0342e-01, -3.4723e-01, -2.0567e-01, -2.0655e+00, -2.0567e-01,\n",
       "         -2.0567e-01,  3.5420e-01, -5.9522e-01, -2.0567e-01, -2.0567e-01,\n",
       "          1.9498e+00, -2.6542e+00, -2.0567e-01, -7.2421e-01, -2.1368e-01,\n",
       "         -2.0424e-01,  1.9140e+00, -2.4221e-01,  1.7238e+00, -2.0567e-01,\n",
       "         -2.0567e-01,  1.5980e-01],\n",
       "        [ 8.2053e-01, -3.9846e-01,  1.1105e+00, -8.3407e-01,  1.9466e-03,\n",
       "          1.0839e+00,  1.9466e-03, -1.1555e+00,  7.6470e-01,  1.0282e+00,\n",
       "         -4.7885e-01, -4.0557e-01,  5.6355e-01, -1.7745e+00,  1.6248e+00,\n",
       "          3.2160e-01,  1.0905e+00,  5.9200e-01,  6.4617e-01, -1.8409e+00,\n",
       "          1.1143e+00,  1.9466e-03,  1.9466e-03,  1.9466e-03, -8.8073e-01,\n",
       "         -1.0082e+00,  1.3365e+00, -1.5836e+00,  1.3640e+00, -1.4756e+00,\n",
       "         -1.2809e+00, -3.5434e-01],\n",
       "        [ 6.4938e-01, -3.9093e-01,  1.4940e+00, -5.8199e-01,  7.4702e-01,\n",
       "          1.9993e-01, -1.0412e+00, -8.6661e-01,  3.3689e-01,  6.7182e-01,\n",
       "          9.3669e-01,  7.3756e-02, -8.3478e-01, -1.6531e+00,  1.6341e+00,\n",
       "          1.9993e-01,  6.0596e-01,  1.9993e-01,  1.3451e+00, -1.0177e+00,\n",
       "          6.6987e-01, -1.8122e+00, -1.7262e-02, -3.4045e-01, -6.6877e-01,\n",
       "          6.8912e-01,  1.8441e+00, -3.1945e-01,  1.2825e+00, -1.4455e+00,\n",
       "         -1.6224e+00, -9.6768e-01],\n",
       "        [ 7.3971e-02, -9.5512e-01,  8.7641e-01, -1.0124e+00,  1.5487e+00,\n",
       "          1.3998e+00, -1.0702e+00, -6.7303e-01,  1.1159e+00,  7.1028e-01,\n",
       "          7.3971e-02,  7.3971e-02,  2.1027e-01, -1.5364e+00,  1.1844e+00,\n",
       "          2.8965e-01,  2.7212e-01,  2.8794e-01,  3.2670e-01, -1.9339e+00,\n",
       "          1.2552e+00, -1.0930e+00,  7.3971e-02, -5.2459e-01, -1.1810e-01,\n",
       "          1.6750e-02,  2.1320e+00, -9.6660e-01,  7.3971e-02,  7.3971e-02,\n",
       "         -2.2606e+00,  7.3971e-02],\n",
       "        [-2.3087e-02, -1.1902e-02,  1.1489e-01, -1.1902e-02,  1.2111e+00,\n",
       "          1.9120e+00, -1.1248e+00, -1.1902e-02, -1.1902e-02,  2.2365e-01,\n",
       "          1.0237e+00, -3.3209e-01, -2.3197e-01, -1.6939e+00, -1.1902e-02,\n",
       "         -1.1902e-02,  1.6468e-01,  3.3051e-01,  3.6539e-01, -2.0204e+00,\n",
       "          1.0558e+00, -1.7659e+00, -1.0214e+00, -1.1902e-02,  2.7037e-01,\n",
       "          1.6362e-01,  2.1835e+00, -9.7126e-02,  1.6315e+00, -2.0641e+00,\n",
       "         -1.1902e-02, -1.8068e-01],\n",
       "        [ 1.0906e-01,  1.3568e-01,  1.7881e-01, -8.6567e-01,  1.4942e+00,\n",
       "          1.8854e+00, -6.8819e-01, -1.2489e+00,  8.4050e-01,  6.2576e-01,\n",
       "          1.7881e-01,  2.9785e-01,  6.5057e-01, -1.3508e+00,  6.8036e-01,\n",
       "          1.7881e-01,  7.6908e-01,  5.2100e-01,  1.1167e+00, -1.9025e+00,\n",
       "          7.1974e-01,  1.7881e-01, -4.9201e-01, -6.6660e-01, -9.4537e-01,\n",
       "          1.7881e-01,  1.7388e+00, -1.2642e+00,  6.4514e-01, -1.9801e+00,\n",
       "         -1.5923e+00, -1.2738e-01],\n",
       "        [ 3.3633e-01,  1.9040e-01,  8.6643e-01,  1.0980e-01,  1.4611e+00,\n",
       "          1.4208e+00,  6.1966e-02, -1.2743e+00,  1.5154e+00,  6.1966e-02,\n",
       "          6.1966e-02, -1.7137e-02,  6.1966e-02, -2.6970e+00,  1.6321e+00,\n",
       "          6.1966e-02,  4.8474e-01, -1.6900e-01,  9.3615e-01, -2.3442e+00,\n",
       "          6.1966e-02, -1.4559e+00,  4.5515e-02,  6.1966e-02,  6.1966e-02,\n",
       "         -9.8898e-01,  6.1966e-02,  6.1966e-02,  8.2045e-01,  6.1966e-02,\n",
       "         -1.6164e+00,  6.1966e-02],\n",
       "        [-2.8521e-01,  7.5687e-01,  2.0894e+00,  1.4968e-01,  1.5840e+00,\n",
       "          1.1301e-01, -6.1059e-01, -1.7079e+00,  3.3242e-01, -5.7236e-02,\n",
       "          1.8375e-01,  1.1301e-01,  3.1452e-02, -9.1791e-01,  3.3403e-02,\n",
       "          1.9765e+00,  1.0251e+00, -5.1117e-01,  5.8037e-02, -1.2239e+00,\n",
       "          1.8773e-01,  1.1301e-01, -1.7623e-01, -1.4683e-01, -3.6032e-01,\n",
       "          1.7889e-01,  1.4011e+00,  1.1301e-01,  1.1301e-01, -2.6890e+00,\n",
       "         -1.9799e+00,  1.1301e-01],\n",
       "        [ 2.6069e-01, -3.1685e-01,  2.6069e-01, -8.8207e-01,  2.6069e-01,\n",
       "          1.2983e+00, -1.4959e+00, -1.0227e+00,  2.6069e-01,  9.3306e-01,\n",
       "          1.8460e-01,  1.0360e+00,  4.2957e-01, -8.5789e-01,  1.3593e+00,\n",
       "          2.5660e-01,  1.2992e+00,  1.0177e+00,  7.3965e-01, -2.1997e+00,\n",
       "          4.5709e-01, -3.1150e-01, -6.9646e-01, -9.4279e-01,  2.6069e-01,\n",
       "         -3.4670e-02,  1.9266e+00, -1.1129e+00,  5.3456e-01, -1.6334e+00,\n",
       "         -1.7513e+00,  4.8254e-01],\n",
       "        [ 8.4740e-01, -1.1211e-01,  3.5374e-01, -5.9582e-01,  1.2897e+00,\n",
       "          1.7609e+00, -2.5270e-01, -1.0734e+00,  6.1841e-01,  8.5350e-01,\n",
       "         -7.5694e-02, -5.4493e-02,  8.5887e-01, -2.1244e+00,  9.4903e-01,\n",
       "          9.6315e-03,  4.6121e-01,  6.1960e-01,  9.6315e-03, -1.3461e+00,\n",
       "          1.0949e+00, -1.9058e+00,  9.6315e-03, -7.8217e-01, -8.3204e-01,\n",
       "         -9.8272e-01,  1.1520e+00,  9.6315e-03,  1.8160e+00, -1.4593e+00,\n",
       "         -1.1268e+00,  9.6315e-03]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "#dropout \n",
    "#num heads\n",
    "#dim in, dim out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out, attention_heads, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.attention_heads = attention_heads\n",
    "        self.dim_inp = dim_inp\n",
    "        self.dim_out = dim_out\n",
    "        \n",
    "        \n",
    "        self.attention = MultiHeadAttention(self.attention_heads, self.dim_inp, self.dim_out, self.dropout_prob)  # batch_size x sentence size x dim_inp\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(self.dim_inp, self.dim_out),\n",
    "            nn.Dropout(self.dropout_prob),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.dim_out, self.dim_inp),\n",
    "            nn.Dropout(self.dropout_prob)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(self.dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        context = self.attention(input_tensor, attention_mask)\n",
    "        res = self.feed_forward(context)\n",
    "        return self.norm(res)\n",
    "\n",
    "encoder_test = Encoder(32, 32, 8, 0.2)\n",
    "encoder_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, max_length, dim_inp, dim_out, attention_heads, num_encoders, dropout_prob):\n",
    "        super(BERT, self).__init__()\n",
    "        self.attention_heads = attention_heads\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_inp = dim_inp  \n",
    "        self.dim_out = dim_out\n",
    "        self.num_encoders = num_encoders\n",
    "        self.dropout_prob = dropout_prob  \n",
    "\n",
    "        self.embedding = JointEmbedding(self.dim_inp, self.vocab_size, self.max_length, self.dropout_prob)\n",
    "        self.encoders = nn.ModuleList([Encoder(self.dim_inp, self.dim_out, self.attention_heads, self.dropout_prob) for _ in range(self.num_encoders)])\n",
    "\n",
    "        self.token_prediction_layer = nn.Linear(self.dim_inp, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        embedded = self.embedding(input_tensor)\n",
    "        for layer in self.encoders:\n",
    "            embedded = layer(embedded, attention_mask)\n",
    "\n",
    "        token_predictions = self.token_prediction_layer(embedded)\n",
    "        return self.softmax(token_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BERT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bert_test \u001b[38;5;241m=\u001b[39m \u001b[43mBERT\u001b[49m(\u001b[38;5;28mlen\u001b[39m(vocabulary), \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m      2\u001b[0m bert_test\u001b[38;5;241m.\u001b[39mforward(test_set[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], test_set[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BERT' is not defined"
     ]
    }
   ],
   "source": [
    "bert_test = BERT(len(vocabulary), 20, 32, 32, 8, 2, 0.2)\n",
    "bert_test.forward(test_set[1][0], test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8662e-01, -8.5011e-02,  2.2547e-01, -1.6851e-02,  4.4351e-02,\n",
       "          1.4062e-02,  1.0866e-02,  1.5393e-01, -4.3348e-01,  2.5300e-01,\n",
       "          1.9350e-01, -3.6733e-01, -5.4809e-02, -2.0595e-01,  1.3405e-01,\n",
       "          1.4189e-01,  6.8620e-02,  1.6805e-01,  4.8116e-01,  4.3453e-01,\n",
       "          3.9839e-01, -5.9823e-01, -3.6819e-01, -2.1596e-01,  8.2339e-02,\n",
       "          2.0398e-01,  1.7603e-01,  7.2742e-01, -1.4394e-03, -2.7836e-01,\n",
       "          3.4622e-01,  2.9143e-02],\n",
       "        [-1.1216e-01, -4.0262e-01,  1.4410e-01, -1.4561e-01, -2.6659e-02,\n",
       "          6.7763e-02, -1.2143e-01,  1.7912e-01, -1.8841e-01,  1.3643e-01,\n",
       "          2.9305e-01, -2.1795e-01, -1.3427e-01, -1.5704e-01, -5.3516e-03,\n",
       "          1.5155e-01, -2.9202e-01,  2.3277e-01,  2.0884e-01,  4.8633e-01,\n",
       "          2.1722e-01, -5.5967e-01, -8.6852e-02,  1.4894e-02,  9.6428e-02,\n",
       "          1.9595e-01,  1.4897e-01,  4.3382e-01,  6.8461e-02, -2.5231e-01,\n",
       "          2.1963e-01,  2.1233e-01],\n",
       "        [-3.9743e-01, -2.5733e-01,  1.3887e-01, -2.3019e-01, -9.8004e-02,\n",
       "          1.7660e-01, -2.2804e-01,  2.3970e-01, -5.6727e-01,  1.7182e-01,\n",
       "          2.2157e-01, -3.9472e-01, -1.2111e-01, -1.1600e-01, -6.5761e-02,\n",
       "          4.6153e-01, -5.2499e-01,  5.4793e-02,  1.9276e-01,  5.6845e-01,\n",
       "          2.6979e-01, -6.6000e-01, -2.2058e-01, -9.3863e-02, -3.6614e-02,\n",
       "          2.1757e-01,  2.5512e-01,  6.3608e-01,  3.4796e-02, -4.2079e-01,\n",
       "          4.7166e-01,  2.0501e-01],\n",
       "        [-4.4422e-01, -9.5608e-03,  9.6653e-02, -3.1946e-01,  2.7769e-02,\n",
       "          5.1741e-01, -1.7698e-01,  1.2593e-01, -8.3641e-01,  5.1149e-01,\n",
       "          3.0946e-03, -3.2475e-01, -4.6287e-01,  3.3998e-02, -5.1624e-02,\n",
       "          5.1686e-01, -8.2519e-01, -1.1328e-01,  1.8929e-04,  5.5630e-01,\n",
       "          7.0202e-02, -5.7813e-01, -4.1121e-01, -1.7267e-01, -1.0831e-01,\n",
       "          1.1411e-01,  1.8283e-01,  6.7467e-01, -3.8949e-02, -3.4535e-01,\n",
       "          4.9090e-01,  2.3969e-01],\n",
       "        [-4.5023e-01, -1.2833e-01,  2.7680e-01, -2.7058e-01, -1.3718e-01,\n",
       "          1.7593e-01, -2.4747e-01,  8.3858e-03, -5.9902e-01,  1.9008e-01,\n",
       "          1.5593e-01, -4.2223e-01, -1.8217e-01,  9.2038e-02,  6.1668e-03,\n",
       "          3.8434e-01, -3.6281e-01,  4.9553e-02,  3.6443e-01,  6.2250e-01,\n",
       "          2.0736e-01, -6.2415e-01, -3.1223e-01, -2.0261e-01,  3.6989e-02,\n",
       "          2.1747e-01,  2.6790e-01,  7.6841e-01,  2.0541e-02, -2.2858e-01,\n",
       "          3.8758e-01, -3.1842e-02],\n",
       "        [-4.1237e-01, -2.6856e-01,  2.2085e-01, -2.5272e-01, -8.3066e-02,\n",
       "          2.2689e-01, -1.9509e-01,  1.7100e-01, -6.5621e-01,  2.8631e-01,\n",
       "          2.2472e-01, -4.2542e-01, -2.1483e-01, -5.5445e-02, -5.2469e-03,\n",
       "          4.3063e-01, -4.6472e-01,  8.9779e-02,  3.8342e-01,  7.0502e-01,\n",
       "          3.0196e-01, -7.7965e-01, -3.2171e-01, -1.3675e-01,  4.3558e-03,\n",
       "          2.7202e-01,  2.9325e-01,  8.2259e-01,  2.2598e-02, -3.9129e-01,\n",
       "          4.4980e-01,  1.6277e-01],\n",
       "        [-4.5383e-01, -1.1992e-01,  2.9388e-01, -2.7498e-01, -1.7524e-01,\n",
       "          1.5143e-01, -2.6075e-01, -2.6756e-02, -6.0381e-01,  1.4860e-01,\n",
       "          1.4588e-01, -4.3122e-01, -1.4814e-01,  1.4179e-01,  5.8553e-03,\n",
       "          3.9416e-01, -3.1382e-01,  3.1522e-02,  4.2589e-01,  6.3338e-01,\n",
       "          2.0917e-01, -6.1628e-01, -3.0330e-01, -2.0700e-01,  3.2048e-02,\n",
       "          2.3367e-01,  2.9344e-01,  7.8499e-01,  1.9086e-02, -2.0921e-01,\n",
       "          3.6751e-01, -8.2260e-02],\n",
       "        [-3.7789e-01, -2.6475e-01,  1.4546e-01, -2.3164e-01,  5.4247e-02,\n",
       "          3.6623e-01, -1.0158e-01,  2.7146e-01, -7.2016e-01,  4.9194e-01,\n",
       "          2.0271e-01, -3.7160e-01, -3.6335e-01, -1.7563e-01,  7.6783e-03,\n",
       "          4.1613e-01, -6.1211e-01,  9.3175e-02,  2.5573e-01,  6.8880e-01,\n",
       "          2.9438e-01, -8.1612e-01, -3.9060e-01, -1.1661e-01, -2.1731e-02,\n",
       "          2.3742e-01,  2.3139e-01,  8.0865e-01,  1.7759e-04, -4.6565e-01,\n",
       "          4.9012e-01,  3.3920e-01],\n",
       "        [-2.1348e-01, -1.1316e-01,  4.9649e-02, -7.7244e-02,  2.2535e-01,\n",
       "          4.3504e-01,  1.6341e-01,  2.4774e-01, -6.8497e-01,  6.9184e-01,\n",
       "          5.2335e-02, -1.9328e-01, -4.4135e-01, -2.0562e-01,  1.0495e-01,\n",
       "          2.2468e-01, -3.6989e-01,  3.5654e-02,  3.2007e-01,  5.1652e-01,\n",
       "          2.7063e-01, -6.7231e-01, -4.6875e-01, -9.7975e-02, -6.1442e-02,\n",
       "          1.8771e-01,  1.3887e-01,  7.1479e-01, -7.1710e-02, -3.9692e-01,\n",
       "          3.2544e-01,  3.7698e-01],\n",
       "        [-1.9581e-01, -4.1363e-01,  5.6999e-02, -1.5835e-01, -7.5971e-02,\n",
       "          7.8572e-02, -1.7738e-01,  3.2353e-01, -3.0898e-01,  7.1021e-02,\n",
       "          2.9353e-01, -2.7541e-01, -3.8201e-02, -2.2724e-01, -8.9801e-02,\n",
       "          3.4456e-01, -4.5663e-01,  1.3671e-01,  1.1452e-01,  4.6998e-01,\n",
       "          2.6858e-01, -5.8829e-01, -5.1781e-02,  4.3459e-02, -1.7152e-02,\n",
       "          2.0929e-01,  2.0268e-01,  4.0983e-01,  6.5371e-02, -4.2481e-01,\n",
       "          3.5819e-01,  3.1286e-01],\n",
       "        [-2.6415e-01, -2.2264e-01,  1.4710e-01, -1.5922e-01,  1.2908e-01,\n",
       "          2.7639e-01, -4.5742e-02,  2.0162e-01, -4.4465e-01,  4.4896e-01,\n",
       "          2.0547e-01, -2.6286e-01, -3.6075e-01, -2.1384e-01,  5.3762e-02,\n",
       "          1.8641e-01, -4.4835e-01,  1.8884e-01,  1.2420e-01,  5.1200e-01,\n",
       "          2.1267e-01, -6.2980e-01, -3.2515e-01, -1.1062e-01,  7.8952e-02,\n",
       "          1.4234e-01,  9.6980e-02,  6.0364e-01,  1.7602e-02, -2.9687e-01,\n",
       "          3.5707e-01,  2.7924e-01],\n",
       "        [-1.9223e-01, -1.4446e-01,  4.7349e-02, -2.7211e-02,  2.4216e-01,\n",
       "          3.6789e-01,  1.9744e-01,  2.8626e-01, -6.1634e-01,  6.6206e-01,\n",
       "          9.0617e-02, -1.9035e-01, -3.8452e-01, -2.7609e-01,  1.2363e-01,\n",
       "          1.7319e-01, -2.7003e-01,  8.3187e-02,  3.5448e-01,  4.8935e-01,\n",
       "          3.2304e-01, -6.7969e-01, -4.5101e-01, -8.7543e-02, -4.0037e-02,\n",
       "          1.9671e-01,  1.2896e-01,  6.9896e-01, -6.3813e-02, -4.1195e-01,\n",
       "          3.1451e-01,  3.8755e-01],\n",
       "        [-2.4102e-01, -1.0008e-01,  7.0974e-02, -1.2039e-01,  1.4831e-01,\n",
       "          4.2102e-01,  9.6622e-02,  1.9305e-01, -7.1010e-01,  6.1372e-01,\n",
       "          3.5167e-02, -2.1791e-01, -4.0429e-01, -1.1089e-01,  7.6427e-02,\n",
       "          2.8348e-01, -3.8900e-01, -9.1060e-03,  3.4368e-01,  5.3864e-01,\n",
       "          2.4129e-01, -6.5116e-01, -4.4083e-01, -1.0406e-01, -7.8380e-02,\n",
       "          1.9909e-01,  1.7819e-01,  7.2136e-01, -6.7706e-02, -3.7142e-01,\n",
       "          3.2209e-01,  3.0695e-01],\n",
       "        [-1.8239e-01, -1.4778e-01,  2.2897e-02, -3.9275e-02,  2.7971e-01,\n",
       "          4.3566e-01,  2.1467e-01,  3.1236e-01, -6.5731e-01,  7.3808e-01,\n",
       "          7.8313e-02, -1.7372e-01, -4.5240e-01, -2.9607e-01,  1.1923e-01,\n",
       "          1.8762e-01, -3.5849e-01,  7.3869e-02,  3.0372e-01,  5.0171e-01,\n",
       "          3.0640e-01, -6.9884e-01, -4.7432e-01, -7.7917e-02, -5.4861e-02,\n",
       "          1.8721e-01,  1.1505e-01,  7.0098e-01, -7.1490e-02, -4.3738e-01,\n",
       "          3.3145e-01,  4.5046e-01],\n",
       "        [-3.3866e-01, -2.8881e-01,  8.0043e-02, -1.5412e-01,  2.2637e-01,\n",
       "          4.3610e-01,  3.3863e-02,  4.1709e-01, -7.1690e-01,  6.8837e-01,\n",
       "          2.3073e-01, -3.2674e-01, -4.6600e-01, -3.9053e-01,  5.3043e-02,\n",
       "          3.3465e-01, -6.4860e-01,  1.6581e-01,  1.6531e-01,  6.5092e-01,\n",
       "          3.5394e-01, -8.7769e-01, -4.6301e-01, -1.0181e-01, -1.0039e-02,\n",
       "          2.0987e-01,  1.5122e-01,  8.0192e-01, -1.3060e-02, -5.5696e-01,\n",
       "          5.3349e-01,  5.2797e-01],\n",
       "        [-3.4884e-01, -2.9421e-01,  1.0535e-01, -1.7456e-01,  1.3943e-01,\n",
       "          3.7867e-01, -2.0497e-02,  3.6638e-01, -7.0674e-01,  5.7607e-01,\n",
       "          2.2857e-01, -3.4939e-01, -3.8576e-01, -3.0672e-01,  3.2243e-02,\n",
       "          3.7155e-01, -5.9698e-01,  1.3638e-01,  2.3624e-01,  6.6626e-01,\n",
       "          3.5023e-01, -8.5715e-01, -4.1749e-01, -9.8750e-02, -2.0031e-02,\n",
       "          2.3519e-01,  1.9837e-01,  8.0430e-01, -5.5936e-03, -5.3077e-01,\n",
       "          5.1025e-01,  4.4452e-01],\n",
       "        [-3.9493e-01, -1.5374e-01,  9.7739e-02, -1.2472e-03,  3.5287e-01,\n",
       "          3.1261e-01,  1.6270e-01,  4.5070e-01, -6.0675e-01,  7.1945e-01,\n",
       "          2.3919e-01, -3.3446e-01, -3.9480e-01, -5.6587e-01,  1.5661e-01,\n",
       "          1.5284e-01, -3.5234e-01,  2.5385e-01,  1.7182e-01,  4.8414e-01,\n",
       "          4.4690e-01, -8.0606e-01, -5.4666e-01, -1.9586e-01,  6.4119e-02,\n",
       "          1.4910e-01,  5.2582e-02,  7.8825e-01, -2.3989e-02, -5.1890e-01,\n",
       "          5.4532e-01,  4.7219e-01],\n",
       "        [-3.5716e-01, -1.8119e-01,  7.2564e-03, -1.4636e-01,  3.5131e-01,\n",
       "          5.7629e-01,  9.2349e-02,  4.6596e-01, -7.8562e-01,  8.6149e-01,\n",
       "          1.5865e-01, -2.7894e-01, -6.0781e-01, -4.6236e-01,  5.5957e-02,\n",
       "          3.3237e-01, -8.1323e-01,  1.2324e-01, -5.1532e-02,  5.6395e-01,\n",
       "          2.8871e-01, -8.1898e-01, -5.3793e-01, -1.2457e-01, -4.1998e-02,\n",
       "          1.2405e-01,  6.2775e-02,  7.3976e-01, -4.1021e-02, -5.7699e-01,\n",
       "          5.8684e-01,  6.3471e-01],\n",
       "        [-4.0237e-01, -2.3212e-01,  1.3709e-01, -2.4967e-01,  7.2510e-02,\n",
       "          4.1628e-01, -1.0638e-01,  2.7161e-01, -7.6680e-01,  5.3805e-01,\n",
       "          1.8002e-01, -3.7391e-01, -4.1050e-01, -1.7057e-01,  2.6688e-03,\n",
       "          4.3966e-01, -6.8305e-01,  7.1422e-02,  2.0344e-01,  6.8810e-01,\n",
       "          2.7045e-01, -8.1252e-01, -4.1855e-01, -1.3142e-01, -3.3425e-02,\n",
       "          2.1935e-01,  2.2016e-01,  8.1530e-01, -7.1823e-03, -4.7051e-01,\n",
       "          5.1634e-01,  3.5574e-01],\n",
       "        [-1.6165e-01, -4.4484e-01,  2.4808e-02, -5.6508e-02,  2.7757e-01,\n",
       "          3.1703e-01,  1.1380e-01,  5.1460e-01, -4.6101e-01,  6.1965e-01,\n",
       "          3.2739e-01, -2.3152e-01, -3.8371e-01, -5.4833e-01,  6.3248e-02,\n",
       "          1.8184e-01, -5.1360e-01,  2.9033e-01,  1.4123e-01,  5.6912e-01,\n",
       "          4.0154e-01, -8.5047e-01, -3.3600e-01,  8.0000e-03,  3.8535e-02,\n",
       "          2.1265e-01,  9.4847e-02,  6.3540e-01,  1.7542e-02, -5.6981e-01,\n",
       "          4.3619e-01,  6.3132e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import AttentionHead\n",
    "\n",
    "attention_test = AttentionHead(32, 32, 0.2)\n",
    "attention_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1050, -0.6418, -1.4463, -0.0554,  0.3496,  1.2113,  2.5580, -1.1008,\n",
       "         -1.0964, -0.2305,  1.7568,  1.1890, -1.1085, -1.2818, -0.3666, -1.2891,\n",
       "          1.2389,  0.8882, -0.7111, -1.1318, -0.2111,  0.3702, -0.1092, -0.9846,\n",
       "          0.0066,  0.4778,  0.8445, -0.6239,  0.1886, -0.9714,  0.2523,  0.9230],\n",
       "        [ 0.9288, -1.2360, -1.2010, -0.6093, -0.1523,  0.6816,  2.7068, -1.1619,\n",
       "         -0.7128,  0.2009,  1.7770,  0.8195, -0.3951, -0.9982, -0.0635, -1.2012,\n",
       "          1.0870,  0.8328, -1.1834, -1.5427, -0.3221,  0.5871, -0.2614, -1.6084,\n",
       "         -0.1981,  0.4088,  0.6683, -0.1010,  0.4177, -0.0643,  0.5961,  1.3005],\n",
       "        [ 1.0855, -0.4137, -1.7830,  0.1882,  0.5005,  0.3475,  2.9796, -1.2225,\n",
       "         -0.0425,  0.4861,  1.4213,  0.8477, -0.4339, -1.8019, -0.1404, -1.7669,\n",
       "          1.3558, -0.9298, -0.0692, -0.6686, -0.2882, -0.0353,  0.1281, -1.0005,\n",
       "          0.0204,  0.6411,  0.6273, -0.1339, -0.0829, -0.8591, -0.0956,  1.1387],\n",
       "        [ 1.7917,  0.1199, -2.6709,  0.4180, -0.3889,  0.4356,  2.5577, -1.3475,\n",
       "         -0.3085, -0.0151,  0.9865, -0.4297,  0.6565, -1.5363,  0.0899, -0.6883,\n",
       "          0.9796, -0.1102, -0.4011, -1.1556,  0.2316,  0.9786, -0.1026, -0.3503,\n",
       "         -0.4554,  0.7677,  0.6335,  0.0638,  0.4052, -1.1321, -1.0054,  0.9820],\n",
       "        [ 1.3370, -0.3150, -1.7731,  0.1659, -0.5430,  0.5282,  2.3906, -1.1937,\n",
       "         -0.9416,  0.4719,  1.7318,  0.9538, -0.4082, -1.2881, -0.1747, -0.7758,\n",
       "          1.0742,  0.8586, -1.1545, -1.5246,  0.1903,  0.7925, -0.3344, -1.3933,\n",
       "         -0.2024,  0.3480,  0.8818,  0.2447,  0.9025, -1.0116, -0.4801,  0.6421],\n",
       "        [ 1.4023, -0.2820, -2.4061,  0.1691,  0.3784,  0.8583,  3.1752, -1.0415,\n",
       "         -0.4139, -0.5058,  1.2280,  0.4143,  0.1267, -1.1034,  0.8100, -0.3649,\n",
       "          0.8610,  0.5463, -1.0580, -0.9244, -0.5893,  0.4873,  0.6306, -0.5242,\n",
       "         -0.7654, -0.0434, -0.0141,  0.3717,  0.2000, -1.2617, -0.9732,  0.6120],\n",
       "        [ 1.3413, -1.1174, -1.4461, -0.5091,  0.4108,  0.8731,  2.4366, -1.4433,\n",
       "         -0.3339,  0.1478,  0.7448,  0.1703, -1.0584, -1.0305, -0.4362, -1.3799,\n",
       "          1.3599,  0.8806, -1.1069, -1.4619,  0.6306,  0.7443, -0.0378, -1.5689,\n",
       "          0.2245,  0.8120,  0.7084,  0.5316,  1.0149, -0.3830, -0.3776,  0.6592],\n",
       "        [ 1.0706, -0.9595, -1.2401, -0.2365, -0.2827,  0.7509,  3.1196, -1.0874,\n",
       "         -0.2473,  0.5480,  1.1209,  1.1679, -0.5981, -1.1451, -0.5201, -1.2964,\n",
       "          0.7159,  0.4220, -0.9576, -1.1452, -0.2202,  0.3104,  0.3075, -0.4054,\n",
       "         -1.3183,  0.6847,  0.7175,  0.0854,  0.4978, -1.4045,  0.1785,  1.3669],\n",
       "        [ 1.1866, -0.6536, -1.6496, -0.4965, -0.4427,  0.5593,  3.0361, -0.4375,\n",
       "         -1.1097, -0.3002,  1.3224,  0.2151, -0.0679, -1.4666,  0.1387, -0.8087,\n",
       "          0.7440,  0.9989, -1.0350, -1.2061, -0.1083,  1.1296,  0.4992, -0.5005,\n",
       "         -0.9626,  0.6778,  0.6402, -0.1262,  0.6755, -0.6520, -1.0723,  1.2724],\n",
       "        [ 1.0339, -1.1096, -1.4513, -0.1871, -0.4743,  0.5709,  2.8885, -0.9142,\n",
       "         -0.8198, -0.0984,  1.6258,  0.3438, -0.1984, -1.0136,  0.3882, -1.1956,\n",
       "          0.4404,  0.5889, -0.8661, -1.4691, -0.6150,  1.1903,  0.6802, -0.7492,\n",
       "         -1.3602,  0.6475,  0.0380,  0.0560,  0.5741, -0.3296,  0.1310,  1.6540],\n",
       "        [ 0.8222, -0.4249, -1.6774,  0.0241, -0.6420,  0.6724,  3.0502, -0.6040,\n",
       "         -1.4189,  0.5156,  1.3051,  1.0621, -0.3795, -0.8971, -0.4085, -1.2468,\n",
       "          0.4587,  1.0828, -0.8646, -1.2757,  0.1390,  0.9747,  0.3941, -0.7296,\n",
       "         -0.6813,  0.2683,  0.4928,  0.1861,  0.7553, -1.2744, -0.7156,  1.0366],\n",
       "        [ 0.7993, -0.6615, -1.3032, -0.4562, -0.1633,  0.9440,  3.0684, -0.4418,\n",
       "         -0.7270, -0.1007,  1.5649,  1.6902, -0.3860, -1.0821, -0.6945, -1.3022,\n",
       "          0.6643,  0.8463, -0.9570, -1.4625, -0.3133,  0.6456, -0.0043, -0.5960,\n",
       "         -1.0402,  0.4229,  0.4868, -0.1252,  0.6583, -0.9986, -0.0309,  1.0553],\n",
       "        [ 0.8349, -0.9501, -0.9015, -1.2194, -0.1610,  1.2135,  2.6890, -1.0410,\n",
       "         -1.1335,  0.0703,  1.9259,  1.3114, -0.9006, -0.8432, -0.1427, -0.7448,\n",
       "          1.0027,  1.2590, -0.9464, -1.1952,  0.0096,  0.6171,  0.3991, -0.8235,\n",
       "         -0.7496,  0.0426,  0.4877, -0.0081,  0.4609, -1.1579, -0.3353,  0.9297],\n",
       "        [ 1.5875, -0.5148, -1.9024,  0.7630, -0.0496,  0.5966,  2.3358, -0.9323,\n",
       "         -0.6198,  0.4963,  1.7258,  0.4125, -0.7614, -1.6616, -0.3150, -1.3335,\n",
       "          0.7753,  0.0983, -0.7406, -1.2445, -0.0647,  0.7849, -0.0419, -1.0138,\n",
       "         -0.7863,  0.5262,  0.8287,  0.3865,  0.9025, -1.1759, -0.0780,  1.0161],\n",
       "        [ 0.8830, -0.6834, -1.6007, -0.1117, -0.5051,  1.0185,  3.2390, -0.3802,\n",
       "         -1.4295,  0.0054,  1.2678,  1.3458, -0.4379, -1.0758,  0.4328, -0.4701,\n",
       "          0.6488,  1.0253, -0.7952, -1.2510, -0.4917,  0.7282,  0.2634, -0.8515,\n",
       "         -0.7486, -0.1303,  0.5221, -0.2935,  0.3369, -1.2801, -0.2083,  1.0274],\n",
       "        [ 1.0554, -0.6881, -0.6434, -0.1539, -0.5959,  0.8324,  2.3946, -1.3049,\n",
       "         -1.4148,  0.4928,  1.4287,  1.4005, -1.0705, -0.7392, -0.6917, -1.2470,\n",
       "          1.0707,  0.0820, -0.6863, -1.2369, -0.0790,  0.4493,  0.4934, -1.3373,\n",
       "         -0.8299,  0.5049,  0.6117, -0.0433,  0.2309, -0.8288,  0.6312,  1.9125],\n",
       "        [ 1.2488, -0.5626, -1.5846, -0.3843, -0.7574,  0.9170,  3.0611, -0.5815,\n",
       "         -1.5960,  0.1332,  1.3045,  0.9764, -0.4517, -0.9991,  0.2744, -0.5498,\n",
       "          0.8081,  0.9104, -0.8738, -1.1789, -0.5849,  0.9159,  0.2369, -0.9285,\n",
       "         -0.4888, -0.1122,  0.7218, -0.0095,  0.2770, -1.1839, -0.1888,  1.2309],\n",
       "        [ 0.6318, -0.8450, -1.3405, -0.3853,  0.1043,  0.8683,  3.4554, -0.7281,\n",
       "         -0.8676,  0.1930,  1.2725,  0.9902, -0.2615, -0.9865, -0.8083, -1.6366,\n",
       "          1.1099,  0.7048, -1.0683, -1.1572,  0.1131,  0.2736,  0.1349, -1.1105,\n",
       "         -0.3108,  0.3229,  0.2393,  0.0901,  0.5243, -0.6644, -0.0439,  1.1860],\n",
       "        [ 1.2376, -1.1709, -1.1315, -0.4505, -0.0162,  0.6596,  2.5103, -1.2938,\n",
       "         -0.6543,  0.2972,  1.5202,  0.5825, -0.8964, -1.2956, -0.2825, -1.3156,\n",
       "          0.9551,  0.9431, -0.7623, -1.1573, -0.6619,  0.7312,  0.2321, -0.9648,\n",
       "         -0.8609,  0.9411,  0.6117,  0.1587,  0.7095, -0.7919, -0.0849,  1.7013],\n",
       "        [ 1.3437, -0.9716, -0.6985,  0.1597, -0.7310,  0.4909,  2.5845, -1.1692,\n",
       "         -0.8966,  0.3089,  1.1868,  0.4355, -0.3584, -1.3814,  0.1673, -1.0505,\n",
       "          0.5604,  1.1717, -1.0815, -1.3498, -0.1354,  1.2185,  0.1619, -0.8396,\n",
       "         -0.6043,  0.9579,  0.5191, -0.3190,  0.5132, -1.4198, -0.5361,  1.7630]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import MultiHeadAttention\n",
    "multihead_test = MultiHeadAttention(num_heads=8, dim_inp=32, dim_out=32, drop_prob=0.2)\n",
    "multihead_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3107,  0.0484,  0.3815,  0.0484, -0.3970, -1.0474,  0.7867, -0.5924,\n",
       "         -1.7959,  0.8987,  1.7071, -0.3569, -0.5909,  0.3377,  0.0484,  0.0484,\n",
       "         -0.8283,  0.3655,  0.0484, -1.9181,  0.9739,  0.6465,  2.1926,  0.7340,\n",
       "         -0.3556,  0.0484,  0.0484, -0.2997, -1.1932,  0.8504,  1.7545, -0.2819],\n",
       "        [-0.1738,  2.4849,  2.1562, -1.1376, -0.7600, -0.3139,  1.0181, -0.2832,\n",
       "          0.8820,  0.4721, -1.4676, -1.1608, -0.1738, -2.3181, -0.5207,  1.1506,\n",
       "         -0.1738, -0.1738, -0.1738, -1.0155,  0.3465,  1.3585,  0.2631,  1.3317,\n",
       "         -0.8844, -0.7417, -0.1738, -0.0424, -0.1738,  0.1367,  0.4259, -0.1642],\n",
       "        [-2.5320,  1.8605,  1.1375,  0.5034,  0.1041, -0.7443,  0.7407, -0.0237,\n",
       "          0.1041,  0.9539,  0.1041,  0.2748, -0.7883,  0.7739,  0.1041, -0.0399,\n",
       "         -1.8362,  0.1041,  0.1041, -1.9907,  0.8502, -0.7054,  2.1488,  0.6838,\n",
       "         -1.1042,  0.6828,  0.1041, -0.4223,  0.1041, -0.6314,  0.4228, -1.0476],\n",
       "        [-2.2955,  2.1124,  0.9396, -0.7191, -0.1447,  0.1763,  1.7372, -0.4026,\n",
       "          0.8312,  0.1603,  0.4010,  0.5161,  0.1603, -0.7054,  0.1603,  0.6500,\n",
       "         -1.9547, -0.9854,  0.1603, -1.1342,  0.1603,  1.4878,  0.1603,  1.2409,\n",
       "         -0.7434, -1.2030, -0.6459,  0.6683, -1.5848,  0.3681,  0.1603,  0.2677],\n",
       "        [-2.2473,  2.1645,  1.8596,  0.5747,  0.0223,  0.0223,  0.9374,  0.8683,\n",
       "          0.1975,  1.1666,  0.8233,  0.8221, -0.1673, -0.0753,  0.4628,  0.0223,\n",
       "         -2.0694,  0.9373, -0.8630, -1.0688,  0.0242,  0.3101,  0.0223, -0.1648,\n",
       "         -0.4280,  0.7753, -1.7840, -1.0921, -0.7255, -0.6528, -0.7391,  0.0649],\n",
       "        [ 0.0278,  2.4061,  0.2267,  0.0278, -0.1093, -1.7639,  1.0110, -0.0833,\n",
       "         -1.6312,  0.1001,  1.2349, -0.0078,  0.0278,  0.3485,  0.5494, -0.1992,\n",
       "         -0.9777, -0.4292, -0.0033, -1.7330,  1.3444,  1.0449,  1.7853,  0.8166,\n",
       "         -0.9570,  0.2266, -1.3578, -0.9749, -1.4037,  0.0278,  0.7700, -0.3442],\n",
       "        [-2.6784,  2.7428,  0.1547, -0.8046, -0.7564, -0.7018, -0.6099,  0.3718,\n",
       "         -0.0132,  0.5279,  0.9274,  0.1547,  0.1547,  0.6833,  1.1044, -0.0752,\n",
       "          0.1406,  0.9670, -1.4583,  0.1547, -0.4763, -0.2651,  1.9358,  0.5842,\n",
       "          0.3050,  0.3082, -1.5781, -1.1114, -0.6117, -0.3899,  0.4680, -0.1550],\n",
       "        [-2.4232,  2.5393,  2.1115, -0.3294, -0.8896, -0.2066,  1.1586,  0.1139,\n",
       "         -0.0244, -0.0244,  0.8463,  0.7403,  0.1663, -0.0244,  0.4853, -0.0244,\n",
       "         -2.5322, -0.0244, -0.4372, -1.3157, -0.0244, -0.0244,  0.7807,  0.3266,\n",
       "          0.1492, -0.3385, -0.0244, -0.5308, -1.1789, -0.0244,  0.3428,  0.6407],\n",
       "        [-2.0822,  0.3036,  1.6491, -0.2535, -0.3663, -1.1091,  0.8815, -0.2533,\n",
       "         -1.5715,  1.0720,  0.3036, -0.6862, -0.6419,  0.5996,  0.3036, -0.7173,\n",
       "         -1.4748,  0.7201, -0.6549, -1.6820,  1.0961,  0.4760,  0.3036,  0.7549,\n",
       "          0.3036,  1.1189, -1.0706,  0.3028, -0.6605,  0.3331,  2.3980,  0.3036],\n",
       "        [-2.5962,  2.3084,  0.2212,  0.0156,  0.1612, -0.8361,  1.5062, -0.4501,\n",
       "         -0.2330,  0.4499,  0.1050, -0.3830, -0.0599, -0.0942,  0.7119,  0.2212,\n",
       "         -1.3227, -1.3898, -0.2446,  0.2212,  1.2417,  0.2212,  1.5031,  0.9759,\n",
       "         -1.1850, -0.8007, -0.3043, -0.5644, -1.4801,  0.9306,  1.2580, -0.1081],\n",
       "        [ 0.0947,  2.2771,  1.2563,  0.0947,  0.1529, -1.2514,  1.6059, -0.1739,\n",
       "         -0.2530,  0.0947,  0.5914,  0.1589,  0.0947,  0.0248,  0.1302,  0.0611,\n",
       "         -2.3182, -1.1733, -0.2661, -1.1725,  1.0463,  0.6728,  1.5108,  1.2607,\n",
       "         -1.5348, -0.1018, -1.8114,  0.0947, -1.0636, -0.3263,  0.0947,  0.1289],\n",
       "        [-2.6550,  2.4546,  0.8139,  0.6577, -0.4026, -0.5859,  0.0666,  0.8248,\n",
       "         -0.5387,  0.0666,  0.3314,  0.4798,  0.7899,  0.0666,  1.0370, -1.0123,\n",
       "          0.0666,  0.0666,  0.0666,  0.0666,  0.5931,  0.0666,  1.3737, -0.2352,\n",
       "          0.4784, -0.2988, -1.9267, -1.4081, -2.1153,  0.6786,  0.0666,  0.0666],\n",
       "        [-0.1342,  1.6816,  0.5583, -0.1288, -0.1342, -1.3812,  2.0672, -1.0232,\n",
       "         -0.9158,  0.5593, -0.3891, -1.1067, -0.9509, -0.8927, -0.3660, -0.8818,\n",
       "         -1.7618, -0.1342,  0.0671, -0.9041, -0.1342,  2.2919,  1.1436,  1.0098,\n",
       "         -0.1342, -0.9733, -0.1342,  0.2695, -0.1342,  1.2352,  1.4951,  0.2362],\n",
       "        [-1.3757,  0.2000,  1.5567, -1.3586, -0.7301,  0.2000,  1.3809, -0.8781,\n",
       "         -0.2983,  1.1188,  0.4605, -1.5460, -0.9713, -0.5875,  0.3198, -0.8718,\n",
       "         -1.8892, -0.1836,  0.2000, -0.3080,  0.2000,  0.2613,  1.5671,  0.2823,\n",
       "          0.3913,  0.1099, -0.1295,  0.0532, -0.6390,  0.2000,  3.0646,  0.2000],\n",
       "        [-2.3651,  3.1394,  1.3438, -0.0214,  0.0747, -0.5069,  0.6966,  0.1462,\n",
       "         -0.0918,  0.4294,  0.1013, -0.0062, -0.2641, -0.7251,  0.1013,  1.0910,\n",
       "         -1.9658, -0.2576, -0.3396, -0.4211, -0.0115,  0.9347,  0.7575,  1.0155,\n",
       "         -0.8790, -1.1343, -1.2375,  0.2566, -0.9899,  0.2953,  0.7326,  0.1013],\n",
       "        [-1.8360,  2.5858,  1.6713, -0.3571, -1.3029,  0.0994,  0.0994,  0.1456,\n",
       "          0.4786,  0.5127, -0.1116,  0.1531, -0.7965, -0.4472, -0.1861,  0.8275,\n",
       "         -1.9642,  0.0724,  0.3152, -0.4360,  0.2957,  1.2944,  0.2056,  0.4258,\n",
       "          0.0994, -1.2179, -0.9593, -0.8535, -1.7123,  0.6363,  1.0271,  1.2350],\n",
       "        [-2.7257,  1.6885, -0.0618,  0.2389, -0.7005, -0.6386,  1.8471, -0.1423,\n",
       "         -0.5301,  1.0742, -0.3084, -0.0618, -0.0618,  0.2946,  0.4352, -0.0618,\n",
       "         -2.0474, -1.0221, -0.0618, -0.6213,  1.1378,  0.9597,  1.3172,  0.3575,\n",
       "         -1.1318, -1.3196, -0.0618, -0.0618, -0.0618,  0.2848,  1.5664,  0.4806],\n",
       "        [-0.1632,  1.9702,  0.8750, -0.1632, -0.1632, -0.1632,  1.1784, -0.1632,\n",
       "          0.3831,  0.4973,  1.3312,  0.3846, -0.1077, -0.2225,  0.6209, -0.1505,\n",
       "         -2.4672, -0.7635, -0.7412, -2.2048,  1.0826,  1.0769,  1.2760,  0.9134,\n",
       "         -0.8111, -0.1632, -1.6348,  0.0808, -1.6654, -0.0146,  0.1643, -0.0722],\n",
       "        [-2.8222,  2.5095,  0.6120, -0.0213,  0.1133, -0.8247,  1.0187, -0.7839,\n",
       "          0.1133,  0.5451,  0.1133, -0.1385, -0.0043,  0.2667,  0.1133, -0.2480,\n",
       "         -1.5117, -0.5838, -0.3074, -0.9602,  1.3630,  1.1922,  1.4054,  1.0193,\n",
       "         -0.4165, -0.6837, -0.9343, -0.4274, -1.3086,  0.7330,  0.8009,  0.0575],\n",
       "        [-1.8140,  1.7847,  0.4732, -0.8548, -1.1640,  0.2038,  1.7027, -0.1466,\n",
       "          0.2175,  0.4192, -0.7242,  0.2038,  0.4641, -0.2430,  0.6100,  0.2038,\n",
       "         -1.2910, -1.3387, -1.1105,  0.0397,  0.2038,  1.0705,  1.2372,  0.8922,\n",
       "          0.2038, -1.1503, -0.3732, -0.5266, -2.2153,  0.8931,  1.7012,  0.4282]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import Encoder\n",
    "\n",
    "encoder_test = Encoder(32, 32, 8, 0.2)\n",
    "encoder_test.forward(emb_test.forward(test_set[1][0]), test_set[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.3623, -7.1583, -7.7241,  ..., -7.4518, -8.0045, -6.4266],\n",
       "        [-7.1923, -7.0146, -7.9684,  ..., -6.5024, -7.6257, -6.6102],\n",
       "        [-7.8528, -8.0000, -7.1180,  ..., -6.2702, -7.6824, -7.6966],\n",
       "        ...,\n",
       "        [-7.7168, -7.1282, -7.0677,  ..., -6.5226, -7.2996, -7.2195],\n",
       "        [-7.3509, -8.0282, -6.5848,  ..., -6.2151, -7.2776, -7.3208],\n",
       "        [-7.4697, -7.6191, -7.3346,  ..., -6.4897, -7.3715, -6.8816]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_builder import BERT\n",
    "\n",
    "bert_test = BERT(vocab_size=len(vocabulary), max_length=20, dim_inp=32, dim_out=32, attention_heads=8, num_encoders=2, dropout_prob=0.2)\n",
    "bert_test.forward(test_set[1][0], test_set[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset\n",
    "- behver vocabulary\n",
    "- saknar: getitem och prepare dataset som kallar p construct_masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data importerad\n",
      "hr r lugnt\n",
      "[['[CLS]', '[PAD]', 'USA', 'aadA1', 'sul1', 'tet(A)'], ['[CLS]', '[PAD]', 'Sweden', '[MASK]', '[MASK]', 'tet(A)', '[MASK]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT'], ['[CLS]', '[PAD]', 'Sweden', 'glpT_E448K=POINT', 'cyaA_S352T=POINT', 'uhpT_E350Q=POINT'], ['[CLS]', '1979', 'USA', 'glpT_E448K=POINT'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', '[MASK]', '[MASK]'], ['[CLS]', '[PAD]', 'USA', '[MASK]', 'pmrB_E123D=POINT', '[MASK]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT'], ['[CLS]', '[PAD]', 'Sweden', 'parC_A56T=POINT', 'glpT_E448K=POINT', '[MASK]', '[MASK]'], ['[CLS]', '[PAD]', 'Sweden', 'parE_I355T=POINT'], ['[CLS]', '[PAD]', 'Sweden', 'glpT_E448K=POINT'], ['[CLS]', '[PAD]', 'Sweden', \"aph(3'')-Ib\", 'tet(B)', 'sul2', '[MASK]'], ['[CLS]', '[PAD]', 'Indonesia', 'glpT_E448K=POINT'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_E123D=POINT'], ['[CLS]', '[PAD]', 'USA', \"aph(3'')-Ib\", 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', 'aph(6)-Id', 'tet(B)'], ['[CLS]', '1979', 'USA', '[MASK]', '1979', 'pmrB_Y358N=POINT', 'aph(6)-Id', 'aph(6)-Ic'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_E123D=POINT'], ['[CLS]', '[PAD]', 'USA', '[MASK]', 'pmrB_Y358N=POINT'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT']]\n",
      "[[-1, -1, -1, -1, -1, -1], [-1, -1, -1, 216, 217, -1, 218], [-1, -1, -1, 219, -1], [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1], [-1, -1, -1, 219, 223, 224], [-1, -1, -1, 219, -1, 222], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, 225, -1, 215, 220], [-1, -1, -1, -1], [-1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, 218], [-1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, 216, 219, -1, -1, 227], [-1, -1, -1, -1, -1], [-1, -1, -1, 219, -1], [-1, -1, -1, -1, -1]]\n",
      "[['[CLS]', '[PAD]', 'USA', 'aadA1', 'sul1', 'tet(A)', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', '[MASK]', '[MASK]', 'tet(A)', '[MASK]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', 'glpT_E448K=POINT', 'cyaA_S352T=POINT', 'uhpT_E350Q=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '1979', 'USA', 'glpT_E448K=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', '[MASK]', '[MASK]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', '[MASK]', 'pmrB_E123D=POINT', '[MASK]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', 'parC_A56T=POINT', 'glpT_E448K=POINT', '[MASK]', '[MASK]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', 'parE_I355T=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', 'glpT_E448K=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Sweden', \"aph(3'')-Ib\", 'tet(B)', 'sul2', '[MASK]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'Indonesia', 'glpT_E448K=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_E123D=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', \"aph(3'')-Ib\", 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', 'aph(6)-Id', 'tet(B)', '[PAD]', '[PAD]'], ['[CLS]', '1979', 'USA', '[MASK]', '1979', 'pmrB_Y358N=POINT', 'aph(6)-Id', 'aph(6)-Ic', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_E123D=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', '[MASK]', 'pmrB_Y358N=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', '[PAD]', 'USA', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
      "range(0, 347995)\n",
      "hit men inte lngre \n",
      "                          masked_indices  \\\n",
      "0  [0, 1, 59, 213, 214, 215, 1, 1, 1, 1]   \n",
      "1      [0, 1, 60, 2, 2, 215, 2, 1, 1, 1]   \n",
      "2    [0, 1, 59, 219, 220, 1, 1, 1, 1, 1]   \n",
      "3  [0, 1, 60, 219, 221, 222, 1, 1, 1, 1]   \n",
      "4      [0, 4, 59, 219, 1, 1, 1, 1, 1, 1]   \n",
      "\n",
      "                                       indices  \n",
      "0     [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
      "1  [-1, -1, -1, 216, 217, -1, 218, -1, -1, -1]  \n",
      "2    [-1, -1, -1, 219, -1, -1, -1, -1, -1, -1]  \n",
      "3     [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n",
      "4     [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]  \n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import vocab \n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "from data_preprocessing import data_loader\n",
    "\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year)\n",
    "NCBI.fillna('[PAD]', inplace=True)\n",
    "print('data importerad')\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MASK_PERCENTAGE = 0.15\n",
    "\n",
    "\n",
    "# data, vocabulary, max sequence length, mask probability, include sequences, some random state\n",
    "class NCBIDataset(Dataset):\n",
    "\n",
    "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
    "    TARGET_COLUMN = 'indices'\n",
    "    NSP_TARGET_COLUMN = 'is_next'\n",
    "    TOKEN_MASK_COLUMN = 'token_mask'\n",
    "\n",
    "    def __init__(self,\n",
    "                 data: pd.DataFrame,\n",
    "                 vocab: vocab,\n",
    "                 max_seq_len: int,\n",
    "                 mask_prob: float,\n",
    "                 random_state: int = 23,\n",
    "                 ):\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        CLS = '[CLS]'\n",
    "        PAD = '[PAD]'\n",
    "        MASK = '[MASK]'\n",
    "        UNK = '[UNK]'\n",
    "\n",
    "        self.data = data.reset_index(drop=True) \n",
    "        self.num_samples = self.data.shape[0]\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.CLS = CLS \n",
    "        self.PAD = PAD\n",
    "        self.MASK = MASK\n",
    "        self.UNK = UNK\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        input = torch.Tensor(item[self.MASKED_INDICES_COLUMN],device=device).long()\n",
    "        token_mask  = torch.tensor(item[self.TARGET_COLUMN], device=device).long()\n",
    "        attention_mask = (input == self.vocab[self.PAD]).unsqueeze(0)\n",
    "\n",
    "        return input, token_mask , attention_mask\n",
    "\n",
    "    def _construct_masking(self):\n",
    "        sequences = deepcopy(self.data['genes'].tolist())\n",
    "        masked_sequences = list()\n",
    "        target_indices_list = list()\n",
    "        seq_starts = [[self.CLS, self.data['year'].iloc[i], self.data['location'].iloc[i]] for i in range(self.data.shape[0])]\n",
    "\n",
    "        for i, geno_seq in enumerate(sequences):\n",
    "            seq_len = len(geno_seq)\n",
    "            masking_index = np.random.rand(seq_len) < self.mask_prob   \n",
    "            target_indices = np.array([-1]*seq_len)\n",
    "            indices = masking_index.nonzero()[0]\n",
    "            target_indices[indices] = self.vocab.lookup_indices([geno_seq[i] for i in indices])\n",
    "            for i in indices:\n",
    "                r = np.random.rand()\n",
    "                if r < 0.8:\n",
    "                    geno_seq[i] = self.MASK\n",
    "                elif r > 0.9:\n",
    "                    geno_seq[i] = self.vocab.lookup_token(np.random.randint(self.vocab_size))\n",
    "            geno_seq = seq_starts[i] + geno_seq\n",
    "            target_indices = [-1]*3 + target_indices.tolist() \n",
    "            masked_sequences.append(geno_seq)\n",
    "            target_indices_list.append(target_indices)\n",
    "        print('hr r lugnt')\n",
    "        print(masked_sequences[:20])\n",
    "        print(target_indices_list[:20])\n",
    "        masked_sequences = [seq + [self.PAD]*(self.max_seq_len - len(seq)) for seq in masked_sequences]\n",
    "        print(masked_sequences[:20])\n",
    "        print(range(len(target_indices_list)))\n",
    "        for i in range(len(target_indices_list)):\n",
    "            indices = target_indices_list[i]\n",
    "            padding = [-1] * (self.max_seq_len - len(indices))\n",
    "            target_indices_list[i] = indices + padding\n",
    "        print('hit men inte lngre ')\n",
    "        return masked_sequences, target_indices_list \n",
    "        \n",
    "    def prepare_dataset(self):\n",
    "        masked_sequences, target_indices = self._construct_masking()\n",
    "        indices_masked = [self.vocab.lookup_indices(masked_seq) for masked_seq in masked_sequences]\n",
    "\n",
    "        rows = zip(indices_masked, target_indices)\n",
    "        self.data = pd.DataFrame(rows, columns=self.columns)\n",
    "        print(self.data.head())\n",
    "\n",
    "def make_vocabulary(dataset: pd.DataFrame):\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    token_list = Counter()\n",
    "    data = dataset.copy()\n",
    "\n",
    "    location_tokens = list(dict.fromkeys(list(chain(list(data['location'])))))\n",
    "    year_tokens = list(dict.fromkeys(list(chain(list(data['year'])))))\n",
    "    genes_tokens = list(dict.fromkeys(list(chain(*data['genes']))))\n",
    "   \n",
    "    token_list.update(map(str, year_tokens))\n",
    "    token_list.update(map(str, location_tokens))\n",
    "    token_list.update(map(str, genes_tokens))\n",
    "    vocabulary = vocab(token_list,specials = [CLS, PAD, MASK, UNK])\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "max_length = 10\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data importerad\n",
      "hr r lugnt\n",
      "[['[CLS]', nan, 'USA', 'sul1', 'tet(A)', 'aadA1'], ['[CLS]', nan, 'Sweden', '[MASK]', '[MASK]', 'aph(6)-Id', '[MASK]'], ['[CLS]', nan, 'USA', 'pmrB_Y358N=POINT', 'glpT_E448K=POINT'], ['[CLS]', nan, 'Sweden', 'uhpT_E350Q=POINT', 'cyaA_S352T=POINT', 'glpT_E448K=POINT'], ['[CLS]', '1979', 'USA', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', 'pmrB_E123D=POINT', '[MASK]', '[MASK]'], ['[CLS]', nan, 'USA', '[MASK]', 'pmrB_E123D=POINT', '[MASK]'], ['[CLS]', nan, 'USA', 'pmrB_Y358N=POINT', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', 'pmrB_Y358N=POINT', 'glpT_E448K=POINT'], ['[CLS]', nan, 'Sweden', 'pmrB_Y358N=POINT', 'tet(A)', '[MASK]', '[MASK]'], ['[CLS]', nan, 'Sweden', 'parE_I355T=POINT'], ['[CLS]', nan, 'Sweden', 'glpT_E448K=POINT'], ['[CLS]', nan, 'Sweden', 'sul2', \"aph(3'')-Ib\", 'aph(6)-Id', '[MASK]'], ['[CLS]', nan, 'Indonesia', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', 'pmrB_E123D=POINT', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', 'aph(6)-Id', 'glpT_E448K=POINT', 'pmrB_Y358N=POINT', \"aph(3'')-Ib\", 'tet(B)'], ['[CLS]', '1979', 'USA', '[MASK]', 'nan', 'pmrB_Y358N=POINT', \"aph(3'')-Ib\", \"aph(3')-IIa\"], ['[CLS]', nan, 'USA', 'pmrB_E123D=POINT', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', '[MASK]', 'glpT_E448K=POINT'], ['[CLS]', nan, 'USA', 'pmrB_Y358N=POINT', 'glpT_E448K=POINT']]\n",
      "[[-1, -1, -1, -1, -1, -1], [-1, -1, -1, 217, 218, -1, 215], [-1, -1, -1, 220, -1], [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1], [-1, -1, -1, 224, 225, 221], [-1, -1, -1, 222, -1, 221], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, 220, -1, 221, 226], [-1, -1, -1, -1], [-1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, 228], [-1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, 219, 221, -1, -1, 228], [-1, -1, -1, -1, -1], [-1, -1, -1, 220, -1], [-1, -1, -1, -1, -1]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import vocab \n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "from data_preprocessing import data_loader\n",
    "from create_dataset import NCBIDataset\n",
    "\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "\n",
    "NCBI = data_loader(include_pheno,threshold_year)\n",
    "print('data importerad')\n",
    "\n",
    "def make_vocabulary(dataset: pd.DataFrame):\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    token_list = Counter()\n",
    "    data = dataset.copy()\n",
    "\n",
    "    location_tokens = list(dict.fromkeys(list(chain(list(data['location'])))))\n",
    "    year_tokens = list(dict.fromkeys(list(chain(list(data['year'])))))\n",
    "    genes_tokens = list(dict.fromkeys(list(chain(*data['genes']))))\n",
    "   \n",
    "    token_list.update(map(str, year_tokens))\n",
    "    token_list.update(map(str, location_tokens))\n",
    "    token_list.update(map(str, genes_tokens))\n",
    "    vocabulary = vocab(token_list,specials = [CLS, PAD, MASK, UNK])\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "max_length = 10\n",
    "mask_prob = 0.30\n",
    "vocabulary = make_vocabulary(NCBI)\n",
    "\n",
    "test_set = NCBIDataset(NCBI, vocabulary, max_length, mask_prob)\n",
    "test_set.prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import typing\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer \n",
    "\n",
    "print(list(chain(*NCBI['genes']))[:20])\n",
    "\n",
    "mylist = list(dict.fromkeys(list(chain(*NCBI['genes']))))\n",
    "print(len(mylist))\n",
    "print(mylist[:20])\n",
    "\n",
    "test = list(chain(list(NCBI['year'])))\n",
    "print(test[:40])\n",
    "test = list(dict.fromkeys(list(chain(list(NCBI['year'])))))\n",
    "print(len(test))\n",
    "print(test[:40])\n",
    "\n",
    "test = list(chain(list(NCBI['location'])))\n",
    "print(test[:20])\n",
    "test = list(dict.fromkeys(list(chain(list(NCBI['location'])))))\n",
    "print(len(test))\n",
    "print(test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'nan': 2, '1979': 1, '2013': 1, '1996': 1, '1975': 1, '2009': 1, '1974': 1, '1998': 1, '1983': 1, '2008': 1, '1995': 1, '1989': 1, '2003': 1, '1997': 1, '1994': 1, '2001': 1, '1980': 1, '1970': 1, '1971': 1, '2007': 1, '2004': 1, '2006': 1, '1982': 1, '1991': 1, '1990': 1, '2002': 1, '1988': 1, '1987': 1, '2011': 1, '1986': 1, '1985': 1, '2010': 1, '2005': 1, '2012': 1, '1984': 1, '2000': 1, '2014': 1, '2015': 1, '1999': 1, '1992': 1, '1973': 1, '1981': 1, '1993': 1, '2016': 1, '1972': 1, '1976': 1, '1977': 1, '2017': 1, '1978': 1, '2018': 1, '2019': 1, '2020': 1, '2021': 1, '2022': 1, '2023': 1, '2024': 1, 'USA': 1, 'Sweden': 1, 'Indonesia': 1, 'Canada': 1, 'Papua New Guinea': 1, 'Japan': 1, 'China': 1, 'Germany': 1, 'India': 1, 'Bangladesh': 1, 'Brazil': 1, 'UK': 1, 'Denmark': 1, 'South Korea': 1, 'France': 1, 'Norway': 1, 'Malaysia': 1, 'Slovenia': 1, 'Thailand': 1, 'Italy': 1, 'Israel': 1, 'Austria': 1, 'Belgium': 1, 'Guinea-Bissau': 1, 'Australia': 1, 'Hungary': 1, 'Lebanon': 1, 'Spain': 1, 'Tanzania': 1, 'Hong Kong': 1, 'Kenya': 1, 'Guatemala': 1, 'Netherlands': 1, 'Singapore': 1, 'Colombia': 1, 'Chile': 1, 'Argentina': 1, 'Russia': 1, 'Haiti': 1, 'Egypt': 1, 'Madagascar': 1, 'Pakistan': 1, 'Sri Lanka': 1, 'Cuba': 1, 'Viet Nam': 1, 'Nigeria': 1, 'Burkina Faso': 1, 'Mexico': 1, 'Morocco': 1, 'French Guiana': 1, 'Senegal': 1, 'Iran': 1, 'Dominican Republic': 1, 'Uzbekistan': 1, 'Peru': 1, 'Nepal': 1, 'Portugal': 1, 'Gambia': 1, 'Mali': 1, 'Mozambique': 1, 'South Africa': 1, 'DRC': 1, 'Bolivia': 1, 'Czech Republic': 1, 'Serbia': 1, 'Finland': 1, 'Tonga': 1, 'UAE': 1, 'Ireland': 1, 'Turkey': 1, 'Macedonia': 1, 'Poland': 1, 'Switzerland': 1, 'Saudi Arabia': 1, 'Zambia': 1, 'Brunei': 1, 'Estonia': 1, 'Slovakia': 1, 'Jordan': 1, 'Guam': 1, 'Bulgaria': 1, 'Zaire': 1, 'Venezuela': 1, 'Paraguay': 1, 'Costa Rica': 1, 'Uruguay': 1, 'Uganda': 1, 'New Zealand': 1, 'Taiwan': 1, 'Ghana': 1, 'Laos': 1, 'Tunisia': 1, 'Georgia': 1, 'Somalia': 1, 'Puerto Rico': 1, 'West Bank': 1, 'Burundi': 1, 'Greece': 1, 'Qatar': 1, 'Romania': 1, 'Togo': 1, 'Ecuador': 1, 'Guinea': 1, 'Niger': 1, 'Kosovo': 1, 'Afghanistan': 1, 'Ukraine': 1, 'Croatia': 1, 'Cameroon': 1, 'Myanmar': 1, 'Pacific Ocean': 1, 'Malawi': 1, 'Sudan': 1, 'Latvia': 1, 'Lithuania': 1, 'Algeria': 1, 'Oman': 1, 'Cambodia': 1, 'Philippines': 1, 'Rwanda': 1, 'Kuwait': 1, 'Mongolia': 1, 'Antarctica': 1, 'Gabon': 1, 'Syria': 1, 'Korea': 1, 'Mauritius': 1, 'Belarus': 1, 'Ethiopia': 1, 'Guyana': 1, 'Reunion': 1, 'New Caledonia': 1, 'Guadeloupe': 1, 'Cyprus': 1, 'Luxembourg': 1, 'Atlantic Ocean': 1, 'Benin': 1, 'Kazakhstan': 1, 'Bahrain': 1, 'Angola': 1, 'Botswana': 1, 'Armenia': 1, 'Albania': 1, 'Greenland': 1, 'Jamaica': 1, 'Iraq': 1, 'South Sudan': 1, 'Djibouti': 1, \"Cote d'Ivoire\": 1, 'Gaza Strip': 1, 'Iceland': 1, 'Honduras': 1, 'Svalbard': 1, 'Libya': 1, 'sul1': 1, 'tet(A)': 1, 'aadA1': 1, \"aph(3'')-Ib\": 1, 'aph(6)-Id': 1, 'sul2': 1, 'glpT_E448K=POINT': 1, 'pmrB_Y358N=POINT': 1, 'cyaA_S352T=POINT': 1, 'uhpT_E350Q=POINT': 1, 'parE_D475E=POINT': 1, 'pmrB_E123D=POINT': 1, 'parC_A56T=POINT': 1, 'parE_I355T=POINT': 1, 'tet(B)': 1, 'tet(C)': 1, 'catA1': 1, \"aph(3')-Ia\": 1, 'parC_S57T=POINT': 1, 'aac(3)-VIa': 1, 'blaTEM-1': 1, 'aac(3)-IId': 1, 'gyrA_S83L=POINT': 1, 'blaCMY-2': 1, 'aac(3)-IVa': 1, 'aph(4)-Ia': 1, 'parE_I529L=POINT': 1, 'ptsI_V25I=POINT': 1, 'catB3': 1, 'dfrA1': 1, 'aadA5': 1, 'blaSHV-2A': 1, 'ampC_C-42T=POINT': 1, 'cmlA5': 1, \"ant(2'')-Ia\": 1, 'aadA2': 1, 'dfrA12': 1, 'dfrA14': 1, 'dfrA16': 1, 'blaCARB-2': 1, 'ere(A)': 1, 'tet(M)': 1, 'blaPSE': 1, 'nfsA_Q44STOP=POINT': 1, 'blaHER-3': 1, 'fosA7.5': 1, 'blaTEM': 1, 'nfsA_Q113STOP=POINT': 1, 'marR_S3N=POINT': 1, 'pmrB_E121K=POINT': 1, 'parC_E84G=POINT': 1, 'parC_S80I=POINT': 1, 'catA2': 1, 'gyrA_D87N=POINT': 1, 'mph(A)': 1, 'dfrA17': 1, 'sat2': 1, 'ompF_Q88STOP=POINT': 1, 'parE_S458T=POINT': 1, 'dfrA36': 1, 'floR': 1, 'gyrA_D87Y=POINT': 1, 'aadA22': 1, 'parE_S458A=POINT': 1, 'parC_A108T=POINT': 1, 'blaTEM-10': 1, 'blaOXA-1': 1, 'nfsA_G154E=POINT': 1, 'dfrA5': 1, 'blaCTX-M-15': 1, \"aac(6')-Ib-cr5\": 1, 'parC_E84V=POINT': 1, 'mph(B)': 1, 'ampC_T-32A=POINT': 1, 'gyrA_S83A=POINT': 1, 'dfrA7': 1, 'parC_E84A=POINT': 1, 'soxS_A12S=POINT': 1, \"aac(6')-Ib'\": 1, 'blaOXA-9': 1, 'dfrB1': 1, 'blaTEM-116': 1, 'cmlA1': 1, 'sul3': 1, 'aadA13': 1, 'oqxB': 1, 'oqxA': 1, 'bleO': 1, '16S_A964G=POINT': 1, 'pmrB_V161G=POINT': 1, '23S_T754A=POINT': 1, 'ble': 1, \"aph(3')-IIa\": 1, 'aph(6)-Ic': 1, '16S_A1408G=POINT': 1, '16S_C1192T=POINT': 1, 'folP_P64S=POINT': 1, 'aadA7': 1, '16S_A794G=POINT': 1, '16S_A1055G=POINT': 1, 'parC_S80R=POINT': 1, 'cmlA6': 1, 'blaCTX-M-14': 1, 'blaSHV-1': 1, 'tet(D)': 1, 'pmrB_A159V=POINT': 1, 'parE_L416F=POINT': 1, 'rpoB_H526L=POINT': 1, 'dfrA15': 1, 'blaTEM-181': 1, 'dfrA8': 1, 'lnu(F)': 1, 'gyrA_D87G=POINT': 1, 'blaCMY-23': 1, 'qnrS1': 1, 'dfrB4': 1, 'tufA_Q125R=POINT': 1, 'parC_E84K=POINT': 1, 'rmtB1': 1, 'nfsA_W159STOP=POINT': 1, 'pmrB_T92P=POINT': 1, 'blaI': 1, 'blaPC1': 1, 'ompC_Q171STOP=POINT': 1, 'blaTEM-40': 1, 'aadA6': 1, 'aac(3)-IIe': 1, 'folP_P64A=POINT': 1, 'nfsA_Q67STOP=POINT': 1, 'blaTEM-30': 1, '23S_T2609C=POINT': 1, 'blaKPC-3': 1, 'fabI_F203L=POINT': 1, 'blaSHV-12': 1, \"aac(6')-Ib3\": 1, 'nfsA_G126R=POINT': 1, 'emrR_L113P=POINT': 1, 'blaCTX-M-55': 1, 'marR_A70T=POINT': 1, 'rpoB_Q148L=POINT': 1, 'ftsI_I336IKYRI=POINT': 1, 'blaCTX-M-1': 1, 'msr(E)': 1, 'mph(E)': 1, 'lnu(G)': 1, 'blaTEM-52': 1, 'dfrA25': 1, 'qnrB2': 1, \"aac(2')-IIa\": 1, 'erm(B)': 1, 'parC_A108V=POINT': 1, 'aadA15': 1, 'nfsA_G131D=POINT': 1, 'dfrA4': 1, 'rpoB_V146F=POINT': 1, 'soxR_G121D=POINT': 1, 'blaKPC-2': 1, \"aac(6')-IIc\": 1, 'arr': 1, 'aac(3)-IIg': 1, 'blaSHV': 1, 'acrB_R717L=POINT': 1, 'arr-3': 1, 'blaCTX-M-3': 1, 'qnrA3': 1, 'ompC_Q82STOP=POINT': 1, 'blaCTX-M-27': 1, 'blaNDM-1': 1, \"aac(6')-Ib\": 1, 'blaTEM-33': 1, 'dfrA21': 1, 'aadA4': 1, 'catB11': 1, 'blaFOX-5': 1, 'dfrA19': 1, 'ampC_C-11T=POINT': 1, 'blaTEM-105': 1, 'blaTEM-135': 1, 'folP_F28L=POINT': 1, 'dfrA32': 1, 'qnrB19': 1, 'rmtE1': 1, 'dfrA34': 1, 'nfsA_S33R=POINT': 1, 'blaOXA-181': 1, 'dfrA27': 1, 'qnrB6': 1, 'aadA16': 1, 'blaCTX-M-104': 1, 'blaTEM-19': 1, 'ampC_G-15GG=POINT': 1, 'blaTEM-12': 1, 'nfsB_W94STOP=POINT': 1, 'parE_L445H=POINT': 1, 'gyrA_S83W=POINT': 1, 'gyrA_D87H=POINT': 1, 'blaCMY': 1, 'qnrA1': 1, \"aac(6')-Ib4\": 1, 'fosA4': 1, 'mcr-1.1': 1, 'tet(X5)': 1, 'blaTEM-176': 1, 'mcr-3.1': 1, 'ftsI_N337NYRIN=POINT': 1, 'blaNDM-5': 1, 'blaCMY-42': 1, 'blaNDM': 1, \"aac(6')-Ian\": 1, 'aadA10': 1, 'mef(B)': 1, 'aadA25': 1, 'qnrS2': 1, 'fosA3': 1, 'armA': 1, 'fosA7': 1, 'floR2': 1, 'qnrE4': 1, 'tet(G)': 1, 'tet(Y)': 1, 'aadA12': 1, 'estT': 1, 'dfrA23': 1, 'aadA8': 1, 'aadA31': 1, 'qepA1': 1, 'uhpA_G97D=POINT': 1, 'blaOXA-2': 1, 'fosA': 1, 'blaOXA': 1, 'blaTEM-148': 1, 'rmtB3': 1, 'rpoB_Q513L=POINT': 1, 'rpoB_D516G=POINT': 1, 'blaIMP-4': 1, \"ant(3'')-Ia\": 1, 'blaTEM-32': 1, 'dfrA3b': 1, 'nfsA_R203C=POINT': 1, 'rpoB_S531F=POINT': 1, '23S_G2032A=POINT': 1, 'catB8': 1, 'blaOXA-10': 1, 'blaCMY-58': 1, 'nfsA_R133S=POINT': 1, 'qepA4': 1, 'blaCMY-4': 1, 'nfsA_H11Y=POINT': 1, \"aac(6')-Ib-cr\": 1, 'blaCTX-M-32': 1, 'tet(31)': 1, 'blaCTX-M': 1, 'blaHER': 1, 'blaLAP': 1, 'rmtC': 1, 'blaCMY-6': 1, 'blaOXA-48': 1, \"aph(3')-VI\": 1, 'blaVIM-29': 1, \"aac(6')-Il\": 1, 'arr-2': 1, 'blaCTX-M-24': 1, 'blaIMP-26': 1, 'blaNDM-7': 1, 'parE_E460D=POINT': 1, 'blaVIM-4': 1, 'blaTEM-169': 1, 'mcr-10': 1, 'blaTEM-190': 1, 'ompR_G63V=POINT': 1, 'ampC_T-14TGT=POINT': 1, 'marR_R77L=POINT': 1, 'qnrB1': 1, 'blaTEM-156': 1, 'blaCTX-M-123': 1, 'blaTEM-84': 1, 'qepA7': 1, 'dfrA26': 1, 'qnrS11': 1, 'oqxB29': 1, 'blaCTX-M-65': 1, 'nfsA_E223STOP=POINT': 1, \"aph(3')-VIa\": 1, 'blaIMP-14': 1, 'aacA34': 1, 'soxR_R20H=POINT': 1, \"aph(3')-VIb\": 1, 'qnrB10': 1, 'blaOXA-163': 1, \"aac(6')-Ib-cr7\": 1, 'parE_I464F=POINT': 1, 'fosA8': 1, 'blaTEM-57': 1, 'blaLAP-2': 1, 'basR_G53E=POINT': 1, 'blaTEM-106': 1, 'blaTEM-15': 1, 'blaTEM-17': 1, 'blaTEM-20': 1, 'blaSHV-2': 1, 'rmtB': 1, 'blaEC-15': 1, 'blaTEM-35': 1, 'qnrS13': 1, 'qnrB4': 1, 'blaDHA-1': 1, 'gyrA_S83V=POINT': 1, 'blaTEM-210': 1, 'qnrB7': 1, 'blaTEM-31': 1, 'rpoB_Q513P=POINT': 1, 'blaCTX-M-115': 1, 'basR_L105P=POINT': 1, 'parC_G78C=POINT': 1, \"aac(6')-Ib11\": 1, 'blaTEM-208': 1, 'dfrA29': 1, 'blaOXA-244': 1, 'blaIMP-1': 1, 'ftsI_I336IPYRI=POINT': 1, 'blaTEM-37': 1, 'mcr-1': 1, 'blaCTX-M-166': 1, 'blaKPC-4': 1, 'blaSHV-7': 1, 'aac(3)-Ib': 1, 'blaIMP-27': 1, 'qepA': 1, 'qnrD1': 1, 'blaOXA-4': 1, \"aph(3')-XV\": 1, 'blaSHV-5': 1, 'parE_E460K=POINT': 1, 'blaSCO-1': 1, 'blaSCO': 1, 'hugA': 1, 'pmrB_T156M=POINT': 1, 'blaTEM-3': 1, 'blaOXA-50': 1, 'blaCMY-16': 1, 'aac(3)-Id': 1, 'npmB2': 1, 'blaCMY-7': 1, 'erm(T)': 1, 'qnrS': 1, 'pmrB_E166K=POINT': 1, 'fosA7.2': 1, 'ere(B)': 1, 'blaTEM-209': 1, 'oqxA2': 1, 'oqxB2': 1, 'blaOXA-129': 1, 'erm(C)': 1, 'mph(C)': 1, 'ampC_C-42A=POINT': 1, '16S_G527T=POINT': 1, 'qnrS12': 1, 'erm(42)': 1, 'blaNDM-9': 1, 'blaCTX-M-125': 1, 'blaCMY-140': 1, 'blaCTX-M-267': 1, 'aadA21': 1, 'blaTEMp_C32T=POINT': 1, 'blaCTX-M-8': 1, 'blaCTX-M-9': 1, 'rpoB_R529C=POINT': 1, 'blaTEM-39': 1, 'qnrVC4': 1, 'basR_G53R=POINT': 1, 'parC_S80W=POINT': 1, 'blaTEM-154': 1, 'acrR_R45C=POINT': 1, 'nfsA_R15C=POINT': 1, 'dfrA10': 1, 'aacA16': 1, 'qepA8': 1, 'fosA5': 1, 'blaCTX-M-182': 1, 'mcr-3.4': 1, 'tet(X4)': 1, 'qnrS4': 1, 'blaGES-5': 1, 'blaTEM-215': 1, 'mcr-3.5': 1, 'blaVEB-1': 1, \"aac(6')-Ia\": 1, 'blaCTX-M-2': 1, 'lnu(A)': 1, 'qnrB': 1, 'mcr-10.1': 1, 'fosA2': 1, \"aac(6')-33\": 1, 'blaTEM-171': 1, 'tet(X3)': 1, 'folP_P64L=POINT': 1, 'nfsB_F84S=POINT': 1, 'blaCTX-M-179': 1, 'blaCMY-136': 1, 'blaCTX-M-153': 1, 'nfsA_E75STOP=POINT': 1, 'blaTEM-54': 1, 'tufA_A376V=POINT': 1, 'cmlA4': 1, 'fosA10': 1, 'mcr-1.4': 1, 'mcr-1.7': 1, 'fabI_G93S=POINT': 1, 'fabI_G93V=POINT': 1, 'mcr-1.5': 1, 'blaNDM-4': 1, 'gyrB_D426N=POINT': 1, 'blaKPC-18': 1, 'aac(3)-Ia': 1, 'blaCMY-111': 1, 'blaNDM-6': 1, 'mcr-1.26': 1, 'lnu(C)': 1, 'aadD1': 1, 'blaZ': 1, 'mcr-3.2': 1, 'blaOXA-232': 1, 'mcr-2.3': 1, 'blaTEM-103': 1, 'mcr-3.24': 1, 'aadA3': 1, 'aad9': 1, 'blaKPC': 1, 'blaROB-11': 1, 'parE_P439S=POINT': 1, 'blaTEM-166': 1, 'nfsA_R203L=POINT': 1, 'qepA6': 1, \"aph(3')-Ib\": 1, 'blaTEM-207': 1, 'mcr-5.1': 1, 'catA3': 1, 'gyrA_Q106H=POINT': 1, 'catB2': 1, 'pmrB_RPISLR6del=POINT': 1, 'fosL': 1, 'catB6': 1, 'blaVIM-1': 1, 'blaACC-1': 1, 'qnrVC1': 1, 'blaPER-2': 1, 'dfrA46': 1, 'ble-Sh': 1, 'blaNDM-21': 1, 'blaLAP-1': 1, 'dfrF': 1, 'rmtD1': 1, 'dfrA22': 1, 'tufA_G317D=POINT': 1, 'mcr-3': 1, 'blaCTX-M-64': 1, 'marR_R94S=POINT': 1, 'blaLAT': 1, 'blaCMY-44': 1, 'lon_P403L=POINT': 1, 'acrB_R620C=POINT': 1, 'mef(C)': 1, 'mph(G)': 1, 'blaSFO-1': 1, 'blaCTX-M-132': 1, 'cfr': 1, 'mcr-1.18': 1, 'tet(H)': 1, 'blaVEB-5': 1, 'tet(E)': 1, 'rpoB_H526Y=POINT': 1, 'blaTEM-4': 1, 'blaTEM-5': 1, 'blaTEM-6': 1, 'aac(3)-Ic': 1, 'blaTEM-7': 1, 'blaIMP-6': 1, 'blaTEM-8': 1, 'rpoB_L533P=POINT': 1, 'blaTEM-2': 1, 'blaCMY-24': 1, 'blaTEM-9': 1, 'sat4': 1, 'parC_G78D=POINT': 1, 'qepA9': 1, 'npmB1': 1, 'blaVEB-17': 1, \"aac(6')-IIa\": 1, 'qepA10': 1, 'mcr-1.2': 1, 'pmrB_L14Q=POINT': 1, 'blaCTX-M-134': 1, 'blaCTX-M-174': 1, 'blaCMY-69': 1, 'blaCMY-33': 1, 'pmrB_C84Y=POINT': 1, 'pmrB_L10P=POINT': 1, 'basR_R81S=POINT': 1, 'basR_G53W=POINT': 1, 'pmrB_P94L=POINT': 1, 'basR_G15R=POINT': 1, 'blaCMY-59': 1, 'basR_G53S=POINT': 1, 'pmrB_C84R=POINT': 1, 'pmrB_A159P=POINT': 1, 'basR_R81L=POINT': 1, 'pmrB_E121Q=POINT': 1, 'pmrB_L10R=POINT': 1, 'pmrB_L14R=POINT': 1, 'basR_A80V=POINT': 1, 'basR_G53C=POINT': 1, 'basR_G53A=POINT': 1, 'blaTEM-34': 1, 'marR_R94H=POINT': 1, 'erm(F)': 1, 'blaTEM-214': 1, 'gyrA_D87V=POINT': 1, 'nfsB_G192S=POINT': 1, 'blaCTX-M-199': 1, 'mcr-3.19': 1, 'blaFRI': 1, 'blaCTX-M-33': 1, \"aac(6')-Ib-cr10\": 1, 'qnrE1': 1, 'qnrVC': 1, 'blaCMY-166': 1, 'blaSHV-11': 1, 'cmlA': 1, 'ompF_G141D=POINT': 1, 'catB': 1, 'rmtF1': 1, 'qepA2': 1, 'blaTEM-71': 1, 'blaTEM-79': 1, 'qnrB17': 1, 'blaGES-7': 1, 'erm(52)': 1, 'blaSHV-210': 1, 'marR_R77C=POINT': 1, 'blaDHA-7': 1, 'erm(G)': 1, 'blaCTX-M-101': 1, 'blaCTX-M-22': 1, 'blaCTX-M-61': 1, 'blaCTX-M-28': 1, 'blaOXA-21': 1, 'blaACC': 1, 'blaOXA-204': 1, 'blaSHV-108': 1, 'qnrB91': 1, 'marR_R73C=POINT': 1, 'blaOXA-427': 1, 'marR_V45E=POINT': 1, 'rmtE2': 1, 'blaNDM-24': 1, 'blaOXA-17': 1, 'blaGES-6': 1, 'blaTEM-191': 1, 'blaCMY-146': 1, 'blaCMY-148': 1, 'marR_L78M=POINT': 1, 'blaSHV-44': 1, 'dfrA30': 1, 'blaTEM-63': 1, 'blaTEM-242': 1, 'blaOXA-58': 1, 'mcr-1.9': 1, 'blaNDM-19': 1, 'blaNDM-13': 1, 'dfrA3': 1, 'blaTEM-238': 1, 'nfsB_G192D=POINT': 1, 'blaNDM-16b': 1, 'blaCTX-M-98': 1, 'qnrB77': 1, 'blaCMY-141': 1, '16S_G926T=POINT': 1, 'blaTEM-198': 1, 'mcr-5': 1, '23S_G2057A=POINT': 1, 'nfsA_K141STOP=POINT': 1, 'blaCTX-M-136': 1, 'blaCTX-M-5': 1, 'blaCTX-M-206': 1, 'blaCMY-32': 1, 'blaTEM-29': 1, 'blaCTX-M-88': 1, 'rpoB_I572L=POINT': 1, 'gyrA_G81D=POINT': 1, 'mcr-3.20': 1, 'blaDHA': 1, 'rpoB_L511Q=POINT': 1, 'blaIMP': 1, 'parE_D476N=POINT': 1, 'blaNDM-27': 1, 'dfrG': 1, 'catA': 1, \"aac(6')-Ie/aph(2'')-Ia\": 1, 'basR_S39I=POINT': 1, \"aph(3')-IIIa\": 1, 'pmrB_G206D=POINT': 1, 'mcr-4.2': 1, 'blaOXA-23': 1, 'blaACT': 1, 'erm(53)': 1, 'blaCTX-M-71': 1, 'blaCTX-M-189': 1, 'blaCMY-43': 1, 'blaCTX-M-157': 1, 'pmrB_P94S=POINT': 1, 'qnrD': 1, 'blaNDM-20': 1, 'qnrB52': 1, 'blaCTX-M-73': 1, 'blaCTX-M-90': 1, 'basR_G53V=POINT': 1, 'gyrA_G81C=POINT': 1, 'tet(X2)': 1, 'aadS': 1, 'aacA37': 1, 'blaPDC': 1, 'blaOXA-488': 1, 'blaCTX-M-130': 1, '23S_C2611T=POINT': 1, 'pmrB_P94Q=POINT': 1, 'tet(K)': 1, 'mcr-4.6': 1, \"aac(6')-Ib-cr4\": 1, 'blaMOX-9': 1, 'estX/sat2': 1, 'blaIMP-11': 1, 'blaCMY-133': 1, 'blaCTX-M-186': 1, 'blaCMY-145': 1, 'tet(J)': 1, 'dfrE': 1, 'rpoB_P564L=POINT': 1, 'blaCTX-M-190': 1, \"aac(6')-Ib-cr9\": 1, 'tet(Q)': 1, '16S_T1406A=POINT': 1, 'blaCTX-M-79': 1, 'aadE': 1, 'rpoB_T563P=POINT': 1, 'blaVEB-25': 1, 'dfrA33': 1, 'blaVIM-2': 1, 'blaVEB-9': 1, 'blaOXA-20': 1, 'blaCTX-M-218': 1, 'fosL1': 1, 'mcr-1.27': 1, 'blaCTX-M-216': 1, 'aphA16': 1, 'qnrA': 1, 'dfrA9': 1, 'pmrB_P94A=POINT': 1, 'emrR_L64R=POINT': 1, 'blaCMY-132': 1, 'blaCMY-62': 1, 'tet(32)': 1, 'blaCMY-154': 1, 'blaTEM-28': 1, 'blaCTX-M-232': 1, 'blaTEM-219': 1, 'fosL2': 1, 'tet(39)': 1, 'rmtG': 1, 'blaCTX-M-12': 1, 'tufA_R334C=POINT': 1, 'tufA_Y161N=POINT': 1, 'blaTEM-122': 1, 'blaCTX-M-165': 1, '16S_G926A=POINT': 1, 'ompF_L15STOP=POINT': 1, 'blaVIM': 1, 'aadA9': 1, 'mcr-1.28': 1, 'blaCMY-162': 1, 'msr(A)': 1, 'blaR1': 1, 'tet(38)': 1, 'parE_I444F=POINT': 1, 'blaCTX-M-240': 1, 'blaTEMp_G162T=POINT': 1, 'gyrA_D82G=POINT': 1, 'blaTEM-183': 1, 'rpoB_I572F=POINT': 1, 'gyrA_A84P=POINT': 1, 'blaCTX-M-215': 1, 'blaCMY-60': 1, 'mcr-3.39': 1, 'toprJ': 1, 'tmexC': 1, 'tmexD': 1, 'tmexC3': 1, 'toprJ1': 1, 'mcr-3.29': 1, 'mcr-1.32': 1, 'tet(O)': 1, 'blaTEM-237': 1, 'blaPER-7': 1, 'blaCTX-M-191': 1, 'blaCTX-M-127': 1, 'blaCMY-131': 1, 'blaTEM-36': 1, 'cirA_R86S=POINT': 1, 'blaCMY-143': 1, 'blaCTX-M-231': 1, 'blaCTX-M-39': 1, 'blaARL': 1, 'fosB6': 1, 'blaOXA-484': 1, 'blaOXA-162': 1, 'mcr-4': 1, 'mcr-4.5': 1, 'mcr-4.1': 1, 'blaCMY-153': 1, 'mcr-5.3': 1, 'cmlA10': 1, 'dfrB3': 1, 'blaSHV-30': 1, 'catB9': 1, 'rmtF': 1, 'blaCTX-M-62': 1, 'blaCTX-M-44': 1, 'blaVEB-3': 1, 'parC_A85T=POINT': 1, 'blaEC-13': 1, 'blaSHV-31': 1, 'blaADC-265': 1, 'blaOXA-832': 1, 'tetB(58)': 1, 'tetA(58)': 1, 'vgbC': 1, 'blaOXA-283': 1, 'blaOXA-1181': 1, 'mcr-2.1': 1, 'blaTEM-168': 1, 'blaCTX-M-235': 1, 'tufA_R231V=POINT': 1, 'blaFONA': 1, 'blaKPC-49': 1, 'blaCMH': 1, 'cepA': 1, \"aph(7'')-Ia\": 1, 'blaTEM-231': 1, 'blaTEM-141': 1, 'blaCTX-M-196': 1, 'blaCTX-M-105': 1, 'blaCTX-M-19': 1, 'aadA11': 1, 'vanH-A': 1, 'vanS-A': 1, 'vga(A)': 1, 'blaGES': 1, 'blaGES-20': 1, 'blaCTX-M-42': 1, 'blaOXA-347': 1, 'blaTEM-132': 1, 'fosE': 1, \"aac(6')-31\": 1, \"ant(3'')-IIa\": 1, 'fosA7.7': 1, 'blaCMY-75': 1, 'blaCTX-M-56': 1, 'sat3': 1, \"aph(3')-Id\": 1, 'erm(49)': 1, 'blaTEM-83': 1, 'dfrB2': 1, 'aac(3)-If': 1, 'blaTEM-150': 1, 'blaCTX-M-82': 1, 'blaMOX': 1, 'blaCMY-27': 1, 'blaCTX-M-121': 1, 'lsa(A)': 1, 'tet(L)': 1, 'blaCTX-M-137': 1, 'blaPER-1': 1, 'blaNDM-3': 1, 'lsa(C)': 1, 'msr(D)': 1, 'tetB(60)': 1, 'mef(A)': 1, 'blaCTX-M-178': 1, 'blaCTX-M-255': 1, 'blaCTX-M-40': 1, 'qnrB38': 1, 'mcr-1.20': 1, 'blaL1': 1, 'blaCTX-M-140': 1, 'blaSHV-120': 1, \"aac(6')-Iai\": 1, 'tet(W)': 1, 'blaTEM-76': 1, 'ant(9)-Ia': 1, 'erm(A)': 1, 'qnrD2': 1, 'spw': 1, 'ant(6)-Ia': 1, 'lnu(B)': 1, 'str': 1, 'lsa(E)': 1, 'cmx': 1, 'blaGES-11': 1, 'blaNDM-36': 1, 'blaNDM-37': 1, 'blaACC-1d': 1, 'blaADC-2': 1, 'blaCTX-M-223': 1, 'blaCMY-10': 1, 'blaOXA-392': 1, 'blaCTX-M-243': 1, 'blaTEM-77': 1, 'mcr-2.8': 1, 'blaCMY-173': 1, 'dfrA35': 1, 'blaIMI-2': 1, \"aac(6')-Iq\": 1, 'mcr-1.11': 1, 'tmexD1': 1, 'tmexC1': 1, 'blaNDM-15': 1, 'blaIMP-64': 1, 'blaTEM-251': 1, 'mcr-3.21': 1, 'blaNDM-33': 1, 'catB1': 1, 'blaIMP-38': 1, 'fosC2': 1, 'blaNDM-29': 1, 'blaTEM-234': 1, 'aacA43': 1, 'blaIMP-18': 1, 'blaKPC-45': 1, 'tmexD3': 1, 'mcr-1.12': 1, 'mcr-1.31': 1, 'mcr-1.34': 1, 'blaIMP-8': 1, 'fabI_G93A=POINT': 1, 'blaACC-1a': 1, 'qnrB9': 1, 'blaNDM-39': 1, 'blaCTX-M-58': 1, 'blaFOX-14': 1, 'blaTEM-185': 1, 'blaNDM-12': 1, 'blaSED': 1, 'blaCTX-M-193': 1, 'blaSHV-161': 1, 'blaSHV-154': 1, 'marR_V96E=POINT': 1, 'rmtB4': 1, 'blaKPC-21': 1, 'blaCMY-156': 1, 'blaOXA-245': 1, 'cblA': 1, 'blaCMY-172': 1, 'mcr-1.8': 1, 'blaOXA-1213': 1, 'blaVIM-86': 1, 'blaCMY-13': 1, 'blaPER-3': 1, 'blaCTX-M-252': 1, \"aac(6')-29\": 1, 'blaKPC-6': 1, 'gyrA_A196E=POINT': 1, 'mcr-1.33': 1, 'msr(C)': 1, \"aac(6')-I\": 1, 'rmtB2': 1, 'marR_E31STOP=POINT': 1, 'blaCTX-M-143': 1, 'blaNDM-48': 1, 'blaGES-1': 1, 'blaGES-2': 1, 'qnrA6': 1, 'blaSHV-26': 1, 'blaCTX-M-169': 1, 'blaIMP-59': 1, 'blaVIM-23': 1, 'blaTEM-143': 1, 'aacA56': 1, 'blaOXA-1041': 1, 'blaCTX-M-63': 1, 'cfxA': 1, 'nimE': 1, 'satA': 1, 'oqxB32': 1, 'vmlR': 1, 'blaCMY-25': 1, 'blaOXA-1042': 1, 'blaTEM-80': 1, 'blaTEM-81': 1, 'blaCTX-M-150': 1, 'blaCMY-121': 1, 'qnrE2': 1, 'blaCTX-M-170': 1, 'blaCTX-M-195': 1, 'blaMIR-22': 1, 'blaCTX-M-96': 1, 'blaCTX-M-214': 1, 'blaTEM-70': 1, 'blaKPC-31': 1, 'blaSHV-49': 1, 'blaOXA-926': 1, 'blaCARB-12': 1, 'tufA_L121Q=POINT': 1, 'mcr-4.3': 1, 'gar': 1, 'ampC_C-42G=POINT': 1, 'blaOXA-101': 1, 'blaCMY-8b': 1, 'blaCTX-M-25': 1, 'blaCTX-M-202': 1, 'blaCTX-M-35': 1, 'parC_E84R=POINT': 1, \"aac(6')\": 1, 'marR_R73S=POINT': 1, 'blaDHA-6': 1, 'vanH-B': 1, 'vanX-B': 1, 'vanB': 1, 'vanR-B': 1, 'vanY-B': 1, 'vanS-B': 1, 'vanW-B': 1, 'dfrA45': 1, 'blaVEB-8': 1, 'tetB(P)': 1, 'blaCFE': 1, 'blaSRT-2': 1, 'blaNDM-55': 1, 'blaCMY-178': 1, 'blaCTX-M-102': 1, 'blaOXA-1205': 1, 'blaTEM-240': 1, 'blaNDM-35': 1, 'blaTEM-26': 1, 'blaOXA-1207': 1, 'blaOXA-24': 1, 'gyrA_A84V=POINT': 1, \"aph(3')-IIb\": 1, 'blaOXA-851': 1, 'crpP': 1, 'catB7': 1, 'blaACT-15': 1, 'blaCMY-101': 1, 'blaNDM-57': 1, 'qnrB65': 1, 'blaIMP-66': 1, 'blaCTX-M-67': 1, 'ftsI_A498T=POINT': 1, 'blaTEM-236': 1, 'blaDHA-27': 1, 'blaCTX-M-200': 1, 'blaCTX-M-269': 1, 'blaCTX-M-30': 1, 'blaCTX-M-52': 1, 'blaKLUC-3': 1, 'blaOXA-932': 1, 'blaCTX-M-38': 1, 'blaNDM-56': 1, '16S_C1066T=POINT': 1, 'blaVEB': 1, 'rpoB_I572N=POINT': 1, 'vga(A)-LC': 1, 'blaFOX': 1, 'parC_S80Y=POINT': 1, 'blaACC-1c': 1, 'blaCTX-M-192': 1, 'blaCMY-185': 1, 'blaCTX-M-220': 1, 'fosI': 1, \"ant(4')-IIb\": 1, 'blaCTX-M-238': 1, 'blaSHV-8': 1, 'blaSHV-102': 1, 'blaCTX-M-36': 1, 'blaOXA-72': 1, 'blaEC-18': 1, 'blaSHV-230': 1, 'blaTEM-244': 1, 'blaTEMp_C141G=POINT': 1, 'blaVIM-24': 1, 'rmtH': 1, 'tetA(P)': 1, 'blaNDM-26': 1, 'mcr-8.1': 1, 'blaCMY-82': 1, \"aph(3')-I(H957)\": 1, 'blaOXY-1-1': 1})\n",
      "Vocab()\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import typing\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer \n",
    "\n",
    "def make_vocabulary(dataset: pd.DataFrame):\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    SEP = '[SEP]'\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    token_list = Counter()\n",
    "    data = dataset.copy()\n",
    "\n",
    "    location_tokens = list(dict.fromkeys(list(chain(list(data['location'])))))\n",
    "    year_tokens = list(dict.fromkeys(list(chain(list(data['year'])))))\n",
    "    genes_tokens = list(dict.fromkeys(list(chain(*data['genes']))))\n",
    "   \n",
    "    token_list.update(map(str, year_tokens))\n",
    "    token_list.update(map(str, location_tokens))\n",
    "    token_list.update(map(str, genes_tokens))\n",
    "    print(token_list)\n",
    "    vocabulary = vocab(token_list,specials = [CLS, PAD, MASK, SEP, UNK])\n",
    "    print(vocabulary)\n",
    "    return vocabulary\n",
    "vocabulary = make_vocabulary(NCBI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = make_vocabulary(NCBI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
