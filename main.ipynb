{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'make_vocabulary' from 'build_vocabulary' (c:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\build_vocabulary.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_results\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_preprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_loader\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbuild_vocabulary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_vocabulary\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcreate_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NCBIDataset\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert_builder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERT\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'make_vocabulary' from 'build_vocabulary' (c:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\build_vocabulary.py)"
     ]
    }
   ],
   "source": [
    "#Relevant packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "import os\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "from misc import get_split_indices\n",
    "from misc import export_results\n",
    "from data_preprocessing import data_loader\n",
    "from build_vocabulary import make_vocabulary\n",
    "from create_dataset import NCBIDataset\n",
    "from bert_builder import BERT\n",
    "from trainer import BertTrainer\n",
    "\n",
    "#Data directory\n",
    "#Lokalt\n",
    "local_data_dir = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\data'\n",
    "#saga\n",
    "saga_data_dir = \"/home/aeerik/data/raw/\"\n",
    "\n",
    "#save directory\n",
    "save_directory = 'c:\\\\Users\\\\erika\\\\Desktop\\\\Exjobb\\\\savefiles'\n",
    "\n",
    "#System information\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Hyperparameters\n",
    "include_pheno = True\n",
    "threshold_year = 1970\n",
    "data_path = local_data_dir #Ã„NDRA DENNA\n",
    "max_length = [88,51,37]\n",
    "mask_prob = 0.15\n",
    "embedding_dim = 32\n",
    "drop_prob = 0.2\n",
    "limit_data = True\n",
    "reduced_samples = 1000 #Ta bort denna senare\n",
    "\n",
    "enc_dim_inp = 32 \n",
    "enc_dim_out = 32 \n",
    "attention_heads = 8 \n",
    "\n",
    "num_encoders = 4\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "stop_patience = 10\n",
    "export_model = True\n",
    "\n",
    "####################################################\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Using CPU\")  \n",
    "    \n",
    "print(f\"\\n Retrieving data from: {data_path}\")\n",
    "print(\"Loading data...\")\n",
    "NCBI = data_loader(include_pheno,threshold_year,data_path)\n",
    "print(f\"Data correctly loaded, {len(NCBI)} samples found\")\n",
    "\n",
    "print(\"Creating vocabulary...\")\n",
    "vocabulary = make_vocabulary(NCBI, include_pheno)\n",
    "print(f\"Vocabulary created with number of elements:\",len(vocabulary))\n",
    "\n",
    "if limit_data:\n",
    "    print(f\"Reducing samples to {reduced_samples}\")\n",
    "    NCBI = NCBI.head(reduced_samples)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary, max_length, mask_prob, include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary, max_length, mask_prob,include_pheno)\n",
    "print(f\"Datasets has been created with {len(train_set)} samples in the training set and {len(val_set)} samples in the validation set\")\n",
    "\n",
    "print(f\"Creating model...\")\n",
    "model = BERT(len(vocabulary), max_length, enc_dim_inp, enc_dim_out, attention_heads, num_encoders, drop_prob)\n",
    "print(f\"Model successfully loaded\")\n",
    "print(f\"---------------------------------------------------------\")\n",
    "print(f\"Starting training...\")\n",
    "trainer = BertTrainer(model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, save_directory)\n",
    "results = trainer()\n",
    "print(f\"---------------------------------------------------------\")\n",
    "if export_model:\n",
    "    print(f\"Exporting model...\")\n",
    "    export_model_label = str(today)+\"model.pkl\"\n",
    "    trainer._save_model(save_directory+\"/\"+export_model_label)\n",
    "print(\"Exporting results...\")\n",
    "export_results_label = str(today)+\"run.pkl\"\n",
    "export_results(results, save_directory+\"/\"+export_results_label)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
