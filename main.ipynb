{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\n",
      "Fine tuning mode\n",
      "Using CPU\n",
      "\n",
      " Retrieving data from: c:\\Users\\erika\\Desktop\\Exjobb\\data\n",
      "Loading data...\n",
      "Data correctly loaded, 6485 samples found\n",
      "Creating vocabulary...\n",
      "Vocabulary created with number of elements: 1227\n",
      "Number of antibiotics: 44\n",
      "Reducing samples to 1000\n",
      "Datasets has been created with 800 samples in the training set and 200 samples in the validation set\n",
      "Creating model...\n",
      "Parallell networks are trainable\n",
      "Model successfully loaded\n",
      "---------------------------------------------------------\n",
      "Starting training...\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([81])) must be the same as input size (torch.Size([44]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m    125\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m BertTrainer_pt(model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, wandb_project, wandb_run_name)\n\u001b[1;32m--> 127\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer()\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m export_model:\n",
      "File \u001b[1;32mc:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\trainer.py:250\u001b[0m, in \u001b[0;36mBertTrainer_ft.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    249\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 250\u001b[0m avg_epoch_loss_geno, avg_epoch_loss_pheno \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_losses_geno\u001b[38;5;241m.\u001b[39mappend(avg_epoch_loss_geno) \n\u001b[0;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_losses_ab\u001b[38;5;241m.\u001b[39mappend(avg_epoch_loss_pheno)  \n",
      "File \u001b[1;32mc:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\trainer.py:341\u001b[0m, in \u001b[0;36mBertTrainer_ft.train\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    339\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m row\n\u001b[0;32m    340\u001b[0m     target \u001b[38;5;241m=\u001b[39m result_list[i]\n\u001b[1;32m--> 341\u001b[0m     ab_loss \u001b[38;5;241m=\u001b[39m custom_loss(prediction, target\u001b[38;5;241m.\u001b[39mfloat()) \n\u001b[0;32m    342\u001b[0m     pheno_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ab_loss\n\u001b[0;32m    343\u001b[0m pheno_loss\u001b[38;5;241m.\u001b[39mbackward() \n",
      "File \u001b[1;32mc:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\loss_functions.py:6\u001b[0m, in \u001b[0;36mcustom_loss\u001b[1;34m(logits, targets, pad_index)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_loss\u001b[39m(logits, targets, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m----> 6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(logits, targets, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (targets \u001b[38;5;241m!=\u001b[39m pad_index)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m      9\u001b[0m     masked_loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m mask\n",
      "File \u001b[1;32mc:\\Users\\erika\\anaconda3\\envs\\exjobb\\Lib\\site-packages\\torch\\nn\\functional.py:3197\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m-> 3197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([81])) must be the same as input size (torch.Size([44]))"
     ]
    }
   ],
   "source": [
    "#Relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "from misc import get_split_indices\n",
    "from misc import export_results\n",
    "from data_preprocessing import data_loader, data_original\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from create_dataset import NCBIDataset\n",
    "from bert_builder import BERT\n",
    "from trainer import BertTrainer_ft\n",
    "from trainer import BertTrainer_pt\n",
    "from misc import get_paths\n",
    "from misc import model_loader\n",
    "\n",
    "####################################################\n",
    "#Data directories\n",
    "base_dir = Path(os.path.abspath(''))\n",
    "os.chdir(base_dir)\n",
    "data_dir, ab_dir, save_directory = get_paths()\n",
    "\n",
    "#Run settings\n",
    "limit_data = True #Reduces number of used samples in taining\n",
    "wandb_mode = False #Uses wandb for logging\n",
    "mode_ft = True  #True for fine tuning, False for pretraining\n",
    "load_model = False #True to load a model from a file\n",
    "export_model = False\n",
    "\n",
    "#Hyperparameters\n",
    "threshold_year = 1970\n",
    "max_length = [51,44]\n",
    "mask_prob = 0.15\n",
    "drop_prob = 0.2\n",
    "reduced_samples = 1000 \n",
    "\n",
    "dim_emb = 256\n",
    "dim_hidden = 256\n",
    "attention_heads = 4 \n",
    "\n",
    "num_encoders = 3\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "lr = 0.00001\n",
    "stop_patience = 5\n",
    "\n",
    "# WandB settingsS\n",
    "wandb_project = \"F1Test\"\n",
    "wandb_run_name = \"FirstRun\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "####################################################\n",
    "\n",
    "#set mode for run, True for fine tuning, False for pretraining \n",
    "if mode_ft:\n",
    "    print(f\"Fine tuning mode\")\n",
    "    include_pheno = True\n",
    "    # export_model = False\n",
    "else:\n",
    "    print(f\"Pretraining mode\")\n",
    "    include_pheno = False\n",
    "    # export_model = True\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Using CPU\")  \n",
    "    \n",
    "print(f\"\\n Retrieving data from: {data_dir}\")\n",
    "print(\"Loading data...\")\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_dir,ab_dir)\n",
    "NCBI_geno_only = data_original(threshold_year,data_dir, ab_dir)\n",
    "print(f\"Data correctly loaded, {len(NCBI)} samples found\")\n",
    "print(\"Creating vocabulary...\")\n",
    "vocabulary_geno = vocab_geno(NCBI_geno_only)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "print(f\"Vocabulary created with number of elements:\",len(vocabulary_geno))\n",
    "if include_pheno:\n",
    "    print(f\"Number of antibiotics:\",len(vocabulary_pheno))\n",
    "\n",
    "if limit_data:\n",
    "    print(f\"Reducing samples to {reduced_samples}\")\n",
    "    NCBI = NCBI.head(reduced_samples)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "print(f\"Datasets has been created with {len(train_set)} samples in the training set and {len(val_set)} samples in the validation set\")\n",
    "\n",
    "print(f\"Creating model...\")\n",
    "if load_model:\n",
    "    savepath = \"\"\n",
    "    model = model_loader(savepath, vocabulary_geno, vocabulary_pheno, dim_emb, dim_hidden, num_encoders, drop_prob, device).to(device)\n",
    "    if mode_ft:\n",
    "        model.finetune_unfreezeing()\n",
    "    else:\n",
    "        model.pretrain_freezing()\n",
    "else:\n",
    "    if mode_ft:\n",
    "        model = BERT(vocab_size=len(vocabulary_geno), dim_embedding = dim_emb, dim_hidden=dim_hidden, attention_heads=8, num_encoders=num_encoders, dropout_prob=drop_prob, num_ab=len(vocabulary_pheno), device=device).to(device)\n",
    "        model.finetune_unfreezeing()\n",
    "    else:\n",
    "        model = BERT(vocab_size=len(vocabulary_geno), dim_embedding = dim_emb, dim_hidden=dim_hidden, attention_heads=8, num_encoders=num_encoders, dropout_prob=drop_prob, num_ab=len(vocabulary_pheno), device=device).to(device)\n",
    "        model.pretrain_freezing()\n",
    "print(f\"Model successfully loaded\")\n",
    "print(f\"---------------------------------------------------------\")\n",
    "print(f\"Starting training...\")\n",
    "if mode_ft:\n",
    "    trainer = BertTrainer_ft(model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, wandb_project, wandb_run_name)\n",
    "else: \n",
    "    trainer = BertTrainer_pt(model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, wandb_project, wandb_run_name)\n",
    "\n",
    "results = trainer()\n",
    "print(f\"---------------------------------------------------------\")\n",
    "if export_model:\n",
    "    print(f\"Exporting model...\")\n",
    "    export_model_label = str(today)+\"model\"+\"Enc\"+str(num_encoders)+\"Emb\"+str(dim_emb)+\"Mask\"+str(mask_prob)+\".pt\"\n",
    "    trainer._save_model(save_directory+\"/\"+export_model_label)\n",
    "print(\"Exporting results...\")\n",
    "export_results_label = str(today)+\"run\"+\"Mode\"+str(mode_ft)+\".pkl\"\n",
    "export_results(results, save_directory+\"/\"+export_results_label)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
