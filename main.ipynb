{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\n",
      "Fine tuning mode\n",
      "Using CPU\n",
      "\n",
      " Retrieving data from: c:\\Users\\erika\\Desktop\\Exjobb\\data\n",
      "Loading data...\n",
      "Data correctly loaded, 6491 samples found\n",
      "Creating vocabulary...\n",
      "Vocabulary created with number of elements: 1225\n",
      "Number of antibiotics: 81\n",
      "Datasets has been created with 5192 samples in the training set and 1299 samples in the validation set\n",
      "Creating model...\n",
      "Parallell networks are trainable\n",
      "Model successfully loaded\n",
      "---------------------------------------------------------\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeerik\u001b[0m (\u001b[33mstrompfel\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\wandb\\run-20240304_143707-dljxnx8n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/strompfel/Model_complexity/runs/dljxnx8n' target=\"_blank\">3Encoders</a></strong> to <a href='https://wandb.ai/strompfel/Model_complexity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/strompfel/Model_complexity' target=\"_blank\">https://wandb.ai/strompfel/Model_complexity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/strompfel/Model_complexity/runs/dljxnx8n' target=\"_blank\">https://wandb.ai/strompfel/Model_complexity/runs/dljxnx8n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch completed in 2.8 min\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m    125\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m BertTrainer_pt(model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, wandb_project, wandb_run_name)\n\u001b[1;32m--> 127\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m export_model:\n",
      "File \u001b[1;32mc:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\trainer.py:255\u001b[0m, in \u001b[0;36mBertTrainer_ft.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m#Validation\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating on validation set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 255\u001b[0m val_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39mtime\u001b[38;5;241m.\u001b[39mgmtime(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_losses_geno\u001b[38;5;241m.\u001b[39mappend(val_results[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\trainer.py:388\u001b[0m, in \u001b[0;36mBertTrainer_ft.evaluate\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    385\u001b[0m     current_abs \u001b[38;5;241m=\u001b[39m current_abs\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint16)\n\u001b[0;32m    386\u001b[0m     list_AB_predictions\u001b[38;5;241m.\u001b[39mappend(current_abs)\n\u001b[1;32m--> 388\u001b[0m     processed_tensor \u001b[38;5;241m=\u001b[39m [row[row \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m SR_class]\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(processed_tensor):\n\u001b[0;32m    390\u001b[0m     total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (row \u001b[38;5;241m==\u001b[39m list_AB_predictions[i])\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\\trainer.py:388\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    385\u001b[0m     current_abs \u001b[38;5;241m=\u001b[39m current_abs\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint16)\n\u001b[0;32m    386\u001b[0m     list_AB_predictions\u001b[38;5;241m.\u001b[39mappend(current_abs)\n\u001b[1;32m--> 388\u001b[0m     processed_tensor \u001b[38;5;241m=\u001b[39m [\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m SR_class]\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(processed_tensor):\n\u001b[0;32m    390\u001b[0m     total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (row \u001b[38;5;241m==\u001b[39m list_AB_predictions[i])\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "from misc import get_split_indices\n",
    "from misc import export_results\n",
    "from data_preprocessing import data_loader, data_original\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from create_dataset import NCBIDataset\n",
    "from bert_builder import BERT\n",
    "from trainer import BertTrainer_ft\n",
    "from trainer import BertTrainer_pt\n",
    "from misc import get_paths\n",
    "from misc import model_loader\n",
    "\n",
    "####################################################\n",
    "#Data directories\n",
    "base_dir = Path(os.path.abspath(''))\n",
    "os.chdir(base_dir)\n",
    "data_dir, ab_dir, save_directory = get_paths()\n",
    "\n",
    "#Run settings\n",
    "limit_data = True #Reduces number of used samples in taining\n",
    "wandb_mode = False #Uses wandb for logging\n",
    "mode_ft = False  #True for fine tuning, False for pretraining\n",
    "load_model = False #True to load a model from a file\n",
    "export_model = False\n",
    "\n",
    "#Hyperparameters\n",
    "threshold_year = 1970\n",
    "max_length = [51,81]\n",
    "mask_prob = 0.15\n",
    "drop_prob = 0.2\n",
    "reduced_samples = 1000 \n",
    "\n",
    "dim_emb = 256\n",
    "dim_hidden = 256\n",
    "attention_heads = 4 \n",
    "\n",
    "num_encoders = 3\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "lr = 0.00001\n",
    "stop_patience = 5\n",
    "\n",
    "# WandB settingsS\n",
    "wandb_project = \"Model_complexity\"\n",
    "wandb_run_name = \"3Encoders\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "####################################################\n",
    "\n",
    "#set mode for run, True for fine tuning, False for pretraining \n",
    "if mode_ft:\n",
    "    print(f\"Fine tuning mode\")\n",
    "    include_pheno = True\n",
    "    # export_model = False\n",
    "else:\n",
    "    print(f\"Pretraining mode\")\n",
    "    include_pheno = False\n",
    "    # export_model = True\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Using CPU\")  \n",
    "    \n",
    "print(f\"\\n Retrieving data from: {data_dir}\")\n",
    "print(\"Loading data...\")\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_dir,ab_dir)\n",
    "NCBI_geno_only = data_original(threshold_year,data_dir, ab_dir)\n",
    "print(f\"Data correctly loaded, {len(NCBI)} samples found\")\n",
    "print(\"Creating vocabulary...\")\n",
    "vocabulary_geno = vocab_geno(NCBI_geno_only)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "print(f\"Vocabulary created with number of elements:\",len(vocabulary_geno))\n",
    "if include_pheno:\n",
    "    print(f\"Number of antibiotics:\",len(vocabulary_pheno))\n",
    "\n",
    "if limit_data:\n",
    "    print(f\"Reducing samples to {reduced_samples}\")\n",
    "    NCBI = NCBI.head(reduced_samples)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "print(f\"Datasets has been created with {len(train_set)} samples in the training set and {len(val_set)} samples in the validation set\")\n",
    "\n",
    "print(f\"Creating model...\")\n",
    "if load_model:\n",
    "    savepath = \"\"\n",
    "    model = model_loader(savepath, vocabulary_geno, vocabulary_pheno, dim_emb, dim_hidden, num_encoders, drop_prob, device).to(device)\n",
    "    if mode_ft:\n",
    "        model.finetune_unfreezeing()\n",
    "    else:\n",
    "        model.pretrain_freezing()\n",
    "else:\n",
    "    if mode_ft:\n",
    "        model = BERT(vocab_size=len(vocabulary_geno), dim_embedding = dim_emb, dim_hidden=dim_hidden, attention_heads=8, num_encoders=num_encoders, dropout_prob=drop_prob, num_ab=len(vocabulary_pheno), device=device).to(device)\n",
    "        model.finetune_unfreezeing()\n",
    "    else:\n",
    "        model = BERT(vocab_size=len(vocabulary_geno), dim_embedding = dim_emb, dim_hidden=dim_hidden, attention_heads=8, num_encoders=num_encoders, dropout_prob=drop_prob, num_ab=len(vocabulary_pheno), device=device).to(device)\n",
    "        model.pretrain_freezing()\n",
    "print(f\"Model successfully loaded\")\n",
    "print(f\"---------------------------------------------------------\")\n",
    "print(f\"Starting training...\")\n",
    "if mode_ft:\n",
    "    trainer = BertTrainer_ft(model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, wandb_project, wandb_run_name)\n",
    "else: \n",
    "    trainer = BertTrainer_pt(model, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, wandb_project, wandb_run_name)\n",
    "\n",
    "results = trainer()\n",
    "print(f\"---------------------------------------------------------\")\n",
    "if export_model:\n",
    "    print(f\"Exporting model...\")\n",
    "    export_model_label = str(today)+\"model\"+\"Mode\"+str(mode_ft)+\".pt\"\n",
    "    trainer._save_model(save_directory+\"/\"+export_model_label)\n",
    "print(\"Exporting results...\")\n",
    "export_results_label = str(today)+\"run\"+\"Mode\"+str(mode_ft)+\".pkl\"\n",
    "export_results(results, save_directory+\"/\"+export_results_label)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
