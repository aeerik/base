{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erika\\Desktop\\Exjobb\\repo\\base\n",
      "Fine tuning mode\n",
      "Using CPU\n",
      "\n",
      " Retrieving data from: c:\\Users\\erika\\Desktop\\Exjobb\\data\n",
      "Loading data...\n",
      "Data correctly loaded, 6486 samples found\n",
      "Creating vocabulary...\n",
      "Vocabulary created with number of elements: 1231\n",
      "Number of antibiotics: 44\n",
      "Reducing samples to 1000\n",
      "Datasets has been created with 800 samples in the training set and 200 samples in the validation set\n",
      "Creating model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BERT.__init__() missing 1 required positional argument: 'cls_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 114\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode_ft:\n\u001b[1;32m--> 114\u001b[0m         model \u001b[38;5;241m=\u001b[39m BERT(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocabulary_geno), dim_embedding \u001b[38;5;241m=\u001b[39m dim_emb, dim_hidden\u001b[38;5;241m=\u001b[39mdim_hidden, attention_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, num_encoders\u001b[38;5;241m=\u001b[39mnum_encoders, dropout_prob\u001b[38;5;241m=\u001b[39mdrop_prob, num_ab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocabulary_pheno), device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    115\u001b[0m         model\u001b[38;5;241m.\u001b[39mfinetune_unfreezeing()\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: BERT.__init__() missing 1 required positional argument: 'cls_mode'"
     ]
    }
   ],
   "source": [
    "#Relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "from misc import get_split_indices\n",
    "from misc import export_results\n",
    "from data_preprocessing import data_loader, data_original\n",
    "from build_vocabulary import vocab_geno\n",
    "from build_vocabulary import vocab_pheno\n",
    "from create_dataset import NCBIDataset\n",
    "from bert_builder import BERT\n",
    "from trainer import BertTrainer_ft\n",
    "from trainer import BertTrainer_pt\n",
    "from misc import get_paths\n",
    "from misc import model_loader\n",
    "\n",
    "####################################################\n",
    "#Data directories\n",
    "base_dir = Path(os.path.abspath(''))\n",
    "os.chdir(base_dir)\n",
    "data_dir, ab_dir, save_directory = get_paths()\n",
    "\n",
    "#Run settings\n",
    "limit_data = True #Reduces number of used samples in taining\n",
    "wandb_mode = False #Uses wandb for logging\n",
    "mode_ft = True  #True for fine tuning, False for pretraining\n",
    "load_model = False #True to load a model from a file\n",
    "export_model = False\n",
    "\n",
    "#Hyperparameters\n",
    "threshold_year = 1970\n",
    "max_length = [51,44]\n",
    "mask_prob = 0.15\n",
    "drop_prob = 0.2\n",
    "reduced_samples = 1000 \n",
    "\n",
    "dim_emb = 32\n",
    "dim_hidden = 32\n",
    "attention_heads = 4 \n",
    "\n",
    "num_encoders = 1\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "lr = 0.00001\n",
    "stop_patience = 5\n",
    "\n",
    "# WandB settingsS\n",
    "wandb_project = \"F1Test\"\n",
    "wandb_run_name = \"FirstRun\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "####################################################\n",
    "\n",
    "#set mode for run, True for fine tuning, False for pretraining \n",
    "if mode_ft:\n",
    "    print(f\"Fine tuning mode\")\n",
    "    include_pheno = True\n",
    "    # export_model = False\n",
    "else:\n",
    "    print(f\"Pretraining mode\")\n",
    "    include_pheno = False\n",
    "    # export_model = True\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Using CPU\")  \n",
    "    \n",
    "print(f\"\\n Retrieving data from: {data_dir}\")\n",
    "print(\"Loading data...\")\n",
    "NCBI,ab_df = data_loader(include_pheno,threshold_year,data_dir,ab_dir)\n",
    "NCBI_geno_only = data_original(threshold_year,data_dir, ab_dir)\n",
    "print(f\"Data correctly loaded, {len(NCBI)} samples found\")\n",
    "print(\"Creating vocabulary...\")\n",
    "vocabulary_geno = vocab_geno(NCBI_geno_only)\n",
    "vocabulary_pheno = vocab_pheno(ab_df)\n",
    "\n",
    "print(f\"Vocabulary created with number of elements:\",len(vocabulary_geno))\n",
    "if include_pheno:\n",
    "    print(f\"Number of antibiotics:\",len(vocabulary_pheno))\n",
    "\n",
    "if limit_data:\n",
    "    print(f\"Reducing samples to {reduced_samples}\")\n",
    "    NCBI = NCBI.head(reduced_samples)\n",
    "\n",
    "train_indices, val_indices = get_split_indices(len(NCBI), 0.2)\n",
    "train_set = NCBIDataset(NCBI.iloc[train_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "val_set = NCBIDataset(NCBI.iloc[val_indices], vocabulary_geno, vocabulary_pheno, max_length, mask_prob,include_pheno)\n",
    "print(f\"Datasets has been created with {len(train_set)} samples in the training set and {len(val_set)} samples in the validation set\")\n",
    "\n",
    "print(f\"Creating model...\")\n",
    "if load_model:\n",
    "    savepath = \"\"\n",
    "    model = model_loader(savepath, vocabulary_geno, vocabulary_pheno, dim_emb, dim_hidden, num_encoders, drop_prob, device).to(device)\n",
    "    if mode_ft:\n",
    "        model.finetune_unfreezeing()\n",
    "    else:\n",
    "        model.pretrain_freezing()\n",
    "else:\n",
    "    if mode_ft:\n",
    "        model = BERT(vocab_size=len(vocabulary_geno), dim_embedding = dim_emb, dim_hidden=dim_hidden, attention_heads=8, num_encoders=num_encoders, dropout_prob=drop_prob, num_ab=len(vocabulary_pheno), device=device).to(device)\n",
    "        model.finetune_unfreezeing()\n",
    "    else:\n",
    "        model = BERT(vocab_size=len(vocabulary_geno), dim_embedding = dim_emb, dim_hidden=dim_hidden, attention_heads=8, num_encoders=num_encoders, dropout_prob=drop_prob, num_ab=len(vocabulary_pheno), device=device).to(device)\n",
    "        model.pretrain_freezing()\n",
    "print(f\"Model successfully loaded\")\n",
    "print(f\"---------------------------------------------------------\")\n",
    "print(f\"Starting training...\")\n",
    "if mode_ft:\n",
    "    trainer = BertTrainer_ft(model, max_length, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, wandb_project, wandb_run_name)\n",
    "else: \n",
    "    trainer = BertTrainer_pt(model, max_length, train_set, val_set, epochs, batch_size, lr, device, stop_patience, wandb_mode, wandb_project, wandb_run_name)\n",
    "\n",
    "results = trainer()\n",
    "print(f\"---------------------------------------------------------\")\n",
    "if export_model:\n",
    "    print(f\"Exporting model...\")\n",
    "    export_model_label = str(today)+\"model\"+\"Enc\"+str(num_encoders)+\"Emb\"+str(dim_emb)+\"Mask\"+str(mask_prob)+\".pt\"\n",
    "    trainer._save_model(save_directory+\"/\"+export_model_label)\n",
    "print(\"Exporting results...\")\n",
    "export_results_label = str(today)+\"run\"+\"Mode\"+str(mode_ft)+\".pkl\"\n",
    "export_results(results, save_directory+\"/\"+export_results_label)\n",
    "\n",
    "print(f\"F1 printing:\")\n",
    "print(\"Sensitivity:\")\n",
    "print(results['sensitivity'])\n",
    "print(\"Specificity:\")\n",
    "print(results['specificity'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 9,\n",
       " 'geno_train_losses': [7.271409788249452,\n",
       "  7.284043529887258,\n",
       "  7.25816234541528,\n",
       "  7.288112746344672,\n",
       "  7.288263930214776,\n",
       "  7.3023728941693715,\n",
       "  7.299101617601183,\n",
       "  7.293038903931041,\n",
       "  7.305252967057405,\n",
       "  7.291548319804815],\n",
       " 'ab_train_losses': [14.484630408846302,\n",
       "  11.164636409577028,\n",
       "  10.05129909110658,\n",
       "  9.436865510028085,\n",
       "  8.973425621603742,\n",
       "  8.58019385367264,\n",
       "  8.366228719552359,\n",
       "  8.233555542466082,\n",
       "  8.06537053025799,\n",
       "  7.912138007305287],\n",
       " 'geno_val_losses': [7.426679766178131,\n",
       "  7.398818016052246,\n",
       "  7.3964073181152346,\n",
       "  7.423852169513703,\n",
       "  7.4360913157463076,\n",
       "  7.436180436611176,\n",
       "  7.426770806312561,\n",
       "  7.424198937416077,\n",
       "  7.426472389698029,\n",
       "  7.449613845348358],\n",
       " 'ab_val_losses': [11.010989463329315,\n",
       "  9.360369741916656,\n",
       "  8.566507148742676,\n",
       "  8.09139711856842,\n",
       "  7.776236140727997,\n",
       "  7.527003049850464,\n",
       "  7.371677589416504,\n",
       "  7.28359887599945,\n",
       "  7.092921400070191,\n",
       "  7.0048829317092896],\n",
       " 'val_accs': [0.8893286649413914,\n",
       "  0.9106916324148779,\n",
       "  0.9243923478966865,\n",
       "  0.9334246714365454,\n",
       "  0.9370274521743542,\n",
       "  0.9400213122240828,\n",
       "  0.9418988176789973,\n",
       "  0.942000304460344,\n",
       "  0.9455015984168063,\n",
       "  0.9465164662302735],\n",
       " 'sensitivity': [0.0,\n",
       "  0.5135135135135135,\n",
       "  0.8961748633879781,\n",
       "  0.9285714285714286,\n",
       "  0.0,\n",
       "  0.7931034482758621,\n",
       "  0.7853658536585366,\n",
       "  0.6293103448275862,\n",
       "  0.9078947368421053,\n",
       "  0.7692307692307693,\n",
       "  0.808695652173913,\n",
       "  1.0,\n",
       "  0.8762886597938144,\n",
       "  0,\n",
       "  0.7692307692307693,\n",
       "  0.5714285714285714,\n",
       "  0.5,\n",
       "  0.8820754716981132,\n",
       "  1.0,\n",
       "  0.5686274509803921,\n",
       "  0.8991228070175439,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.13333333333333333,\n",
       "  0.782312925170068,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9766081871345029,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.6666666666666666,\n",
       "  0.5416666666666666,\n",
       "  0.5,\n",
       "  0.3333333333333333,\n",
       "  0.0,\n",
       "  0,\n",
       "  0.8414634146341463,\n",
       "  0.9555555555555556,\n",
       "  0.8807947019867549,\n",
       "  0.9175050301810865,\n",
       "  0.6511627906976745,\n",
       "  0.6296296296296297,\n",
       "  0.7782805429864253,\n",
       "  0],\n",
       " 'specificity': [1.0,\n",
       "  0.9978609625668449,\n",
       "  0.9759398496240601,\n",
       "  0.5,\n",
       "  1.0,\n",
       "  0.9679012345679012,\n",
       "  0.9037267080745341,\n",
       "  0.9160447761194029,\n",
       "  0.84375,\n",
       "  0.9655172413793104,\n",
       "  0.9372197309417041,\n",
       "  0.3333333333333333,\n",
       "  0.9298969072164949,\n",
       "  0.7777777777777778,\n",
       "  1.0,\n",
       "  0.8,\n",
       "  0.26666666666666666,\n",
       "  0.9637139807897546,\n",
       "  0.5714285714285714,\n",
       "  0.9915397631133672,\n",
       "  0.9879032258064516,\n",
       "  0,\n",
       "  0.43478260869565216,\n",
       "  1.0,\n",
       "  0.9875598086124402,\n",
       "  1.0,\n",
       "  0.8,\n",
       "  0.9766233766233766,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  0.9333333333333333,\n",
       "  0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.8975609756097561,\n",
       "  0.7307692307692307,\n",
       "  0.9715639810426541,\n",
       "  0.9540740740740741,\n",
       "  0.9897119341563786,\n",
       "  0.8863636363636364,\n",
       "  0.9802197802197802,\n",
       "  0.6]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
